{
  "original_claim": "does google release veo3.1?",
  "timestamp": "2025-10-19T13:35:57.416075",
  "workflow_version": "2.2_final",
  "execution_log": [
    {
      "timestamp": "2025-10-19T13:35:34.675031",
      "step": "classification",
      "agent": "classifier_agent",
      "input_preview": "does google release veo3.1?",
      "output_preview": "{'domain': 'Science', 'claim_type': 'Factual', 'complexity': 'Simple', 'urgency': 'Medium', 'rationale': \"The claim is about a potential release of a new version of Google's software, which falls unde",
      "full_output": {
        "domain": "Science",
        "claim_type": "Factual",
        "complexity": "Simple",
        "urgency": "Medium",
        "rationale": "The claim is about a potential release of a new version of Google's software, which falls under the domain of science (technology). It is a factual question that can be verified. The claim is simple as it involves a single inquiry. The urgency is medium because it pertains to a potential software release, which is important but not an immediate emergency."
      }
    },
    {
      "timestamp": "2025-10-19T13:35:36.639687",
      "step": "decomposition",
      "agent": "decomposer_agent",
      "input_preview": "{'domain': 'Science', 'claim_type': 'Factual', 'complexity': 'Simple', 'urgency': 'Medium', 'rationale': \"The claim is about a potential release of a new version of Google's software, which falls unde",
      "output_preview": "{'original_claim': 'does google release veo3.1?', 'atomic_claims': [{'id': 'claim_1', 'statement': 'Google has released a software version named veo3.1.', 'dependencies': [], 'type': 'fact', 'entities",
      "full_output": {
        "original_claim": "does google release veo3.1?",
        "atomic_claims": [
          {
            "id": "claim_1",
            "statement": "Google has released a software version named veo3.1.",
            "dependencies": [],
            "type": "fact",
            "entities": [
              "Google",
              "veo3.1"
            ],
            "temporal": "2025-10-19",
            "quantitative": "",
            "priority": "high"
          }
        ],
        "dependency_graph": {
          "foundational": [
            "claim_1"
          ]
        },
        "total_claims": 1
      }
    },
    {
      "timestamp": "2025-10-19T13:35:48.277170",
      "step": "question_generation",
      "agent": "question_agent",
      "input_preview": "{'original_claim': 'does google release veo3.1?', 'atomic_claims': [{'id': 'claim_1', 'statement': 'Google has released a software version named veo3.1.', 'dependencies': [], 'type': 'fact', 'entities",
      "output_preview": "{'current_date_used': '2025-10-19', 'queries': [{'id': 'q1', 'query': 'Google official announcement veo3.1 release 2025', 'claim_id': 'claim_1', 'query_type': 'source_verification', 'priority': 'high'",
      "full_output": {
        "current_date_used": "2025-10-19",
        "queries": [
          {
            "id": "q1",
            "query": "Google official announcement veo3.1 release 2025",
            "claim_id": "claim_1",
            "query_type": "source_verification",
            "priority": "high"
          },
          {
            "id": "q2",
            "query": "Google veo3.1 release date 2025",
            "claim_id": "claim_1",
            "query_type": "direct_fact",
            "priority": "high"
          },
          {
            "id": "q3",
            "query": "veo3.1 software version Google 2025 official",
            "claim_id": "claim_1",
            "query_type": "source_verification",
            "priority": "high"
          },
          {
            "id": "q4",
            "query": "Google veo3.1 release news 2025",
            "claim_id": "claim_1",
            "query_type": "source_verification",
            "priority": "high"
          },
          {
            "id": "q5",
            "query": "veo3.1 Google software version 2025",
            "claim_id": "claim_1",
            "query_type": "direct_fact",
            "priority": "high"
          },
          {
            "id": "q6",
            "query": "Google veo3.1 release statement 2025",
            "claim_id": "claim_1",
            "query_type": "source_verification",
            "priority": "high"
          },
          {
            "id": "q7",
            "query": "veo3.1 Google software version 2025 official data",
            "claim_id": "claim_1",
            "query_type": "statistical",
            "priority": "high"
          },
          {
            "id": "q8",
            "query": "Google veo3.1 release 2025 debunked",
            "claim_id": "claim_1",
            "query_type": "contradiction",
            "priority": "high"
          },
          {
            "id": "q9",
            "query": "veo3.1 Google software version 2025 false",
            "claim_id": "claim_1",
            "query_type": "contradiction",
            "priority": "high"
          },
          {
            "id": "q10",
            "query": "Google veo3.1 release 2025 misleading",
            "claim_id": "claim_1",
            "query_type": "contradiction",
            "priority": "high"
          }
        ],
        "total_queries": 10,
        "strategy_rationale": "The queries are designed to verify the foundational claim 'Google has released a software version named veo3.1.' by using specific entities, dates, and authoritative source keywords. The queries vary in type to cover direct facts, source verification, and contradiction checks, ensuring a comprehensive fact-checking approach."
      }
    },
    {
      "timestamp": "2025-10-19T13:35:51.532127",
      "step": "search_execution",
      "agent": "perplexity_api",
      "input_preview": "[{'id': 'q1', 'query': 'Google official announcement veo3.1 release 2025', 'claim_id': 'claim_1', 'query_type': 'source_verification', 'priority': 'high'}, {'id': 'q2', 'query': 'Google veo3.1 release",
      "output_preview": "[{'query_id': 'q2', 'query': 'Google veo3.1 release date 2025', 'claim_id': 'claim_1', 'query_type': 'direct_fact', 'priority': 'high', 'results': [{'position': 1, 'title': 'Veo (text-to-video model) ",
      "full_output": [
        {
          "query_id": "q2",
          "query": "Google veo3.1 release date 2025",
          "claim_id": "claim_1",
          "query_type": "direct_fact",
          "priority": "high",
          "results": [
            {
              "position": 1,
              "title": "Veo (text-to-video model) - Wikipedia",
              "url": "https://en.wikipedia.org/wiki/Veo_(text-to-video_model)",
              "snippet": "**Veo**, or **Google Veo**, is a text-to-video model developed by Google DeepMind and announced in May 2024. As a generative AI model, it creates videos based on user prompts. Veo 3, released in May 2025, can also generate accompanying audio.\n\n## Development\n\nIn May 2024, a multimodal video generation model called Veo was announced at Google I/O 2024. Google claimed that it could generate 1080p videos over a minute long. In December 2024, Google released Veo 2, available via VideoFX. It supports 4K resolution video generation and has an improved understanding of physics. In April 2025, Google announced that Veo 2 became available for advanced users on the Gemini app.\n\nIn May 2025, Google released Veo 3, which not only generates videos but also creates synchronized audio — including dialogue, sound effects, and ambient noise — to match the visuals. Google also announced **Flow**, a video-creation tool powered by Veo and Imagen. Google DeepMind CEO Demis Hassabis described the release as the moment when AI video generation left the era of the silent film.... ## Capabilities and limitations\n\nGoogle Veo can be bought by several subscription/membership tiers, and/or by using Google \"AI credits\". The software itself can be run by two different consoles called Google Gemini and Google Flow, with Gemini being geared towards shorter, quicker, and faster projects, using the Gemini AI chat model, or through Google Flow, which is essentially a movie editor, as well, allowing users to create longer projects, and continuity using the same characters and actors. Users can create a maximum length of eight seconds per clip.\n\nGoogle Veo, has a relatively simple interface and dashboard, however writing prompts, for those who have little to no experience in transcribing or filmmaking may face issues with the software misunderstanding what the user intended by their prompt (no matter how detailed it was). So although Veo does have a friendly and simple setup, prompts, which are the forefront of the software, need to be not only short and to the point, but they also must be very specific, if the user wants the right vision for their project. Google Veo, when it comes to human models, is able to generate several ethnicity and body types. The software is also capable of generating stand up comedy routines, and Music videos. It can as well generate animals, cartoons, and animation. Prompts must accurately describe places, people, and things in each scene, in addition knowledge of film and camera lingo such as panning, zooming, and terms for camera angles, are also important.... Google Veo however, has strict guidelines and blockades to their software. Before a clip is generated, the algorithm computer software reviews it, and if it is anything deemed inappropriate, too graphically sexual, illegal, showcasing graphic abuse/assault/fighting (unless the prompt specifies that it is a fictitious martial arts scene etc.) gross behaviors, antisemitism, racist, homophobic, anything depicting reigning regimes, rioting, blood, gore, or warfare, (unless in some cases the prompt specifies that it is fictitious period drama, the clip may still be generated), the clip will not be generated. In addition, Google Veo cannot and will not generate character actors that look identical to celebrities or real-life individuals. Users have primarily complained that, regardless of how descriptive and detailed their prompts are, Google Veo often misunderstands the input, resulting in completely different outputs. Common issues include the emulation of incorrect subtitles and captions, the generation of complex scenes that are incomplete due to the maximum eight-second length, the production of garbled and nonsensical speech, and character models that appear deformed in both appearance and movement. Users have also reported that their prompts and generated content are falsely flagged as violating guidelines, along with a variety of other issues and complaints. However, trial and error may have to be used with Veo for optimal results.... ## Reactions\n\nA reporter for *Gizmodo* reacted to the release of Veo 3 by observing that users were directing the model to generate low-quality content, such as man on the street interviews or haul videos of people unboxing products. Another media commentator reported that the tool tended to repeat the same joke in response to different prompts.\n\nCommentators speculated that Google had trained the service on YouTube videos or Reddit posts. Google itself had not stated the source of its training content.\n\nIn July 2025, Media Matters for America reported that racist and antisemitic videos generated using Veo 3 were being uploaded to TikTok. Ryan Whitwam of *Ars Technica* commented, \"In a perfect world, Veo 3 would refuse to create these videos, but vagueness in the prompt and the AI's inability to understand the subtleties of racist tropes (i.e., the use of monkeys instead of humans in some videos) make it easy to skirt the rules.\"\n\n## See also\n- Sora (text-to-video model)\n- VideoPoet – Text-to-video model by Google\n- Dream Machine (text-to-video model)\n\n## References\n\n## External links\n- Official website\n- *Introducing Veo 3.1 and advanced capabilities in Flow*\n\nCategories: - 2024 software\n- Applications of artificial intelligence\n- Film and video technology\n- Google DeepMind\n- Text-to-video generation\n- Video processing\n- Generative artificial intelligence\n- 2024 in artificial intelligence",
              "domain": "en.wikipedia.org"
            },
            {
              "position": 2,
              "title": "Introducing Veo 3.1 and advanced capabilities in Flow - Google Blog",
              "url": "https://blog.google/technology/ai/veo-updates-flow/",
              "snippet": "# Introducing Veo 3.1 and advanced capabilities in Flow\n\nFive months ago, we introduced Flow, our AI filmmaking tool powered by Veo, and have been inspired by the creativity it has sparked with over 275 million videos generated in Flow\n\n1\n\n. We're always listening to your feedback, and we've heard that you want more artistic control within Flow, with increased support for audio across all features.\n\nToday, we’re introducing new and enhanced creative capabilities to edit your clips, giving you more granular control over your final scene. For the first time, we’re also bringing audio to existing capabilities like “Ingredients to Video,” “Frames to Video” and “Extend.”\n\nWe’re also introducing Veo 3.1, which brings richer audio, more narrative control, and enhanced realism that captures true-to-life textures. Veo 3.1 is state-of-the-art and builds on Veo 3, with stronger prompt adherence and improved audiovisual quality when turning images into videos.\n\n## Refine your narrative with audio and more control\n\nWith Veo 3.1, we’re bringing audio to existing capabilities to help you craft the perfect scene. These features are experimental and actively improving, and we’re excited to see what you create as we iterate based on your feedback.\n\nNow, with rich, generated audio, you can:\n\n**Craft the look of your scene.**With \"Ingredients to Video,\" you can use multiple reference images to control the characters, objects and style. Flow uses your ingredients to create a final scene that looks just as you envisioned. **Control the shot from start to finish.**Provide a starting and ending image with “Frames to Video,” and Flow will generate a seamless video that bridges the two, perfect for artful and epic transitions. **Create longer, seamless shots.**With \"Extend,\" you can create longer videos, even lasting for a minute or more, that connect to and continue the action from your original clip. Each video is generated based on the final second of your previous clip, making it most useful for creating a longer establishing shot.... ## Edit your ingredients and videos with more precision\n\nGreat ideas can strike at any point in the creative process. For moments when the first take isn't the final one, we're introducing new editing capabilities directly within Flow to help you reimagine and perfect your scenes.\n\n**Add new elements to any scene.**With “Insert,” introduce anything you can imagine, from realistic details to fantastical creatures. Flow now handles complex details like shadows and scene lighting, making the addition look natural. **Remove unwanted objects or characters seamlessly.**Soon, you’ll be able to take anything out of a scene, and Flow will reconstruct the background and surroundings, making it look as though the object was never there.\n\n## Start creating in Flow today\n\nWith more precise editing capabilities, audio across all existing features and higher-quality outputs powered by Veo 3.1, we're opening up new possibilities for richer, more powerful video storytelling right inside Flow.\n\nThe Veo 3.1 model is also available via the Gemini API for developers, Vertex AI for enterprise customers, and the Gemini app. New capabilities are available in both Gemini API\n\n2\n\nand Vertex AI\n\n3",
              "domain": "blog.google"
            },
            {
              "position": 3,
              "title": "Release notes | Gemini API | Google AI for Developers",
              "url": "https://ai.google.dev/gemini-api/docs/changelog",
              "snippet": "This page documents updates to the Gemini API.\n\n## October 17, 2025\n\n**Grounding with Google Maps**is now generally available. For more information, see Grounding with Google Maps documentation.\n\n## October 15, 2025\n\nReleased Veo 3.1 and 3.1 Fast models in public preview, with new features including:\n\n- Extending Veo-created videos.\n\n- Referencing up to three images to generate a video.\n\n- Providing first and last frame images to generate videos from.\n\nThis launch also added more options for Veo 3 output video durations: 4, 6, and 8 seconds.\n\nDeprecation for\n\n`veo-3.0-generate-preview`and\n\n`veo-3.0-fast-generate-preview`coming October 22, 2025.\n\n## October 7, 2025\n\n- Launched Gemini 2.5 Computer Use Preview\n\n## October 2, 2025\n\n- Launched Gemini 2.5 Flash Image GA: Image Generation with Gemini\n\n## September 29, 2025\n\n- The following Gemini 1.5 models are now deprecated:\n\n`gemini-1.5-pro`\n\n`gemini-1.5-flash-8b`\n\n`gemini-1.5-flash`... ## September 9, 2025\n\n- Launched Veo 3 and Veo 3 Fast GA, with lower pricing and new options for aspect ratios, resolution, and seeding. Read the Veo documentation for more information.\n\n## August 26, 2025\n\n- Launched Gemini 2.5 Image Preview, our latest native image generation model.\n\n## August 18, 2025\n\n- Released URL context tool to general\n\navailability (GA), a tool for providing URLs as additional context to\n\nprompts. Support for using URL context with the\n\n`gemini-2.0-flash`model (available during experimental release) will be discontinued in one week.\n\n## August 14, 2025\n\n- Released Imagen 4 Ultra, Standard and Fast models as generally available (GA). To learn more, see the Imagen page.\n\n## August 7, 2025\n\n`allow_adult`setting in Image to Video generation are now available in restricted regions. See the Veo page for details.\n\n## July 31, 2025\n\n- Launched image-to-video generation for the Veo 3 Preview model.\n\n- Released Veo 3 Fast Preview model.\n\n- To learn more about Veo 3, visit the Veo page.... ## July 22, 2025\n\n- Released\n\n`gemini-2.5-flash-lite`, our fast, low-cost, high-performance Gemini 2.5 model. To learn more, see Gemini 2.5 Flash-Lite.\n\n## July 17, 2025\n\nLaunched\n\n`veo-3.0-generate-preview`, the latest update to Veo introducing video with audio generation. To learn more about Veo 3, visit the Veo page.\n\nIncreased rate limits for Imagen 4 Standard and Ultra. Visit the Rate limits page for more details.\n\n## July 14, 2025\n\n- Released\n\n`gemini-embedding-001`, the stable version of our text embedding model. To learn more, see embeddings. The\n\n`gemini-embedding-exp-03-07`model will be deprecated on August 14, 2025.\n\n## July 7, 2025\n\n- Launched Gemini API Batch Mode. Batch up requests and send them to process asynchronously. To learn more, see Batch Mode.\n\n## June 26, 2025\n\nThe preview models\n\n`gemini-2.5-pro-preview-05-06`and\n\n`gemini-2.5-pro-preview-03-25`are now redirecting to the latest stable version\n\n`gemini-2.5-pro`.\n\n`gemini-2.5-pro-exp-03-25`is deprecated.... ## May 7, 2025\n\n- Released\n\n`gemini-2.0-flash-preview-image-generation`, a preview model for generating and editing images. To learn more, see Image generation and Gemini 2.0 Flash Preview Image Generation.\n\n## May 6, 2025\n\n- Released\n\n`gemini-2.5-pro-preview-05-06`, a new version of our most powerful model, with improvements on code and function calling.\n\n`gemini-2.5-pro-preview-03-25`will automatically point to the new version of the model.\n\n## April 17, 2025\n\n- Released\n\n`gemini-2.5-flash-preview-04-17`, a Gemini preview model optimized for price-performance and adaptive thinking. To learn more, see Gemini 2.5 Flash Preview and Thinking.\n\n## April 16, 2025\n\n- Launched context caching for Gemini 2.0 Flash.... ## April 9, 2025\n\n**Model updates:**\n\n- Released\n\n`veo-2.0-generate-001`, a generally available (GA) text- and image-to-video model, capable of generating detailed and artistically nuanced videos. To learn more, see the Veo docs.\n\nReleased\n\n`gemini-2.0-flash-live-001`, a public preview version of the Live API model with billing enabled.\n\n**Enhanced Session Management and Reliability** **Session Resumption:**Keep sessions alive across temporary network disruptions. The API now supports server-side session state storage (for up to 24 hours) and provides handles (session_resumption) to reconnect and resume where you left off. **Longer Sessions via Context Compression:**Enable extended interactions beyond previous time limits. Configure context window compression with a sliding window mechanism to automatically manage context length, preventing abrupt terminations due to context limits. **Graceful Disconnect Notification:**Receive a\n\n`GoAway`server message indicating when a connection is about to close, allowing for graceful handling before termination.\n\n\n\n**More Control over Interaction Dynamics** **Configurable Voice Activity Detection (VAD):**Choose sensitivity levels or disable automatic VAD entirely and use new client events (\n\n`activityStart`,\n\n`activityEnd`) for manual turn control.\n\n**Configurable Interruption Handling:**Decide whether user input should interrupt the model's response. **Configurable Turn Coverage:**Choose whether the API processes all audio and video input continuously or only captures it when the end-user is detected speaking. **Configurable Media Resolution:**Optimize for quality or token usage by selecting the resolution for input media. **Richer Output and Features** **Expanded Voice & Language Options:**Choose from two new voices and 30 new languages for audio output. The output language is now configurable within\n\n`speechConfig`.\n\n**Text Streaming:**Receive text responses incrementally as they are generated, enabling faster display to the user. **Token Usage Reporting:**Gain insights into usage with detailed token counts provided in the\n\n`usageMetadata`field of server messages, broken down by modality and prompt or response phases.... ## April 4, 2025\n\n- Released\n\n`gemini-2.5-pro-preview-03-25`, a public preview Gemini 2.5 Pro version with billing enabled. You can continue to use\n\n`gemini-2.5-pro-exp-03-25`on the free tier.\n\n## March 25, 2025\n\n- Released\n\n`gemini-2.5-pro-exp-03-25`, a public experimental Gemini model with thinking mode always on by default. To learn more, see Gemini 2.5 Pro Experimental.\n\n## March 12, 2025\n\n**Model updates:**\n\n- Launched an experimental Gemini 2.0 Flash model capable of image generation and editing.\n\n- Released\n\n`gemma-3-27b-it`, available on AI Studio and through the Gemini API, as part of the Gemma 3 launch.\n\n**API updates:**\n\n- Added support for YouTube URLs as a media source.\n\n- Added support for including an inline video of less than 20MB.\n\n## March 11, 2025\n\n**SDK updates:**\n\n- Released the Google Gen AI SDK for TypeScript and JavaScript to public preview.\n\n## March 7, 2025\n\n**Model updates:**\n\n- Released\n\n`gemini-embedding-exp-03-07`, an experimental Gemini-based embeddings model in public preview.... ## February 28, 2025\n\n**API updates:**\n\n- Support for Search as a tool\n\nadded to\n\n`gemini-2.0-pro-exp-02-05`, an experimental model based on Gemini 2.0 Pro.\n\n## February 25, 2025\n\n**Model updates:**\n\n- Released\n\n`gemini-2.0-flash-lite`, a generally available (GA) version of Gemini 2.0 Flash-Lite, which is optimized for speed, scale, and cost efficiency.\n\n## February 19, 2025\n\n**AI Studio updates:**\n\n- Support for additional regions (Kosovo, Greenland and Faroe Islands).\n\n**API updates:**\n\n- Support for additional regions (Kosovo, Greenland and Faroe Islands).\n\n## February 18, 2025\n\n**Model updates:**\n\n- Gemini 1.0 Pro is no longer supported. For the list of supported models, see Gemini models.\n\n## February 11, 2025\n\n**API updates:**\n\n- Updates on the OpenAI libraries compatibility.\n\n## February 6, 2025\n\n**Model updates:**\n\n- Released\n\n`imagen-3.0-generate-002`, a generally available (GA) version of Imagen 3 in the Gemini API.\n\n**SDK updates:**\n\n- Released the Google Gen AI SDK for Java for public preview.",
              "domain": "ai.google.dev"
            },
            {
              "position": 4,
              "title": "Build with Veo 3, now available in the Gemini API",
              "url": "https://developers.googleblog.com/en/veo-3-now-available-gemini-api/",
              "snippet": "First unveiled at Google I/O 2025, people around the world have already generated tens of millions of high-quality videos with Veo 3 (along with some new fun and interesting video trends). It is our first video model to incorporate high-fidelity video outputs and native audio, first with text-to-video and soon with image-to-video.\n\nDevelopers are already experimenting with Veo 3, discovering how the model can help them brainstorm content, rapidly iterate, and be more efficient.\n\nVeo 3 is designed to handle a range of video generation tasks, from cinematic narratives to dynamic character animations. With Veo 3, you can create more immersive experiences by not only generating stunning visuals, but also audio like dialogue and sound effects.\n\nLet’s take a look at some examples.\n\nExplore these examples and more with Veo 3 in Google AI Studio, available as an SDK template and interactive Starter App to remix, copy and extend. The Starter App and its sample code offer a convenient way for Paid Tier users to rapidly prototype with Veo 3 and more on the Gemini API, directly from Google AI Studio.\n\nClick the Key button in the top right of the AI Studio Build interface to select a Google Cloud Project with billing enabled to use the Paid Tier in AI Studio apps. See the FAQs for more.... Veo 3 will be priced at $0.75 per second for video and audio output. Additionally, Veo 3 Fast will be available soon, offering a faster and more cost-effective option for video creation.\n\nHere’s a basic Python example to create a video:\n\n```\n\nimport time\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.0-generate-preview\",\n\nprompt=\"a close-up shot of a golden retriever playing in a field of sunflowers\",\n\nconfig=types.GenerateVideosConfig(\n\nnegative_prompt=\"barking, woofing\",\n\n),\n\n\n\n# Waiting for the video(s) to be generated\n\nwhile not operation.done:\n\ntime.sleep(20)\n\noperation = client.operations.get(operation)\n\ngenerated_video = operation.result.generated_videos[0]\n\nclient.files.download(file=generated_video.video)\n\ngenerated_video.video.save(\"veo3_video.mp4\")\n\n```\n\nAll videos generated by Veo 3 models will continue to include a digital SynthID watermark. To get started, check out the documentation, cookbook, and a Veo 3 starter app in Google AI Studio:\n\nIn addition to being available via the Gemini API in Google AI Studio, Veo 3 is also available to Google AI subscribers in the Gemini app and Flow, and to enterprise customers via Vertex AI.... 1: Fluffy Characters Stop Motion: Inside a brightly colored, cozy kitchen made of felt and yarn. Professor Nibbles, a plump, fluffy hamster with oversized glasses, nervously stirs a bubbling pot on a miniature stove, muttering, \"Just a little more... 'essence of savory,' as the recipe calls for.\" The camera is a mid-shot, capturing his frantic stirring. Suddenly, the pot emits a loud \"POP!\" followed by a comical \"whoosh\" sound, and a geyser of iridescent green slime erupts, covering the entire kitchen. Professor Nibbles shrieks, \"Oh, dear! Not again!\" and scurries away, leaving a trail of tiny, panicked squeaks.\n\n2: The sequence begins with an extreme close-up of a single gear, slowly turning and reflecting harsh sunlight. The camera gradually pulls back in a continuous movement, revealing this is but one component of a colossal, mechanical heart half-buried in a desolate, rust-colored desert. A sweeping aerial shot establishes its enormous scale and isolation in the barren landscape. The camera descends to capture pipes hissing steam and the rhythmic thumping that echoes across the empty plains. A subtle shake effect synchronizes with each massive heartbeat. A lateral tracking shot discovers tiny, robed figures scurrying across the metallic surface. The camera follows one such figure in a detailed tracking shot as they perform meticulous maintenance, polishing brass valves and tightening immense bolts. A complex movement circles the entire structure, capturing different maintenance teams working in precarious positions across its rusted exterior. The final shot begins tight on the meticulous work of one tiny figure before executing a dramatic pull-out that reveals the true scale of the heart and the minuscule size of its caretakers, tending to the vital organ of an unseen, sleeping giant that extends beyond the frame.",
              "domain": "developers.googleblog.com"
            },
            {
              "position": 5,
              "title": "Meet Flow: AI-powered filmmaking with Veo 3",
              "url": "https://blog.google/technology/ai/google-flow-veo-ai-filmmaking-tool/",
              "snippet": "# Meet Flow: AI-powered filmmaking with Veo 3\n\nToday we’re introducing Flow, our new AI filmmaking tool.\n\nIt’s built by and for creatives, and it’s the only AI filmmaking tool custom-designed for Google’s most advanced models — Veo, Imagen and Gemini. Flow can help storytellers explore their ideas without bounds and create cinematic clips and scenes for their stories. It’s early days, and we’re excited to shape the future of Flow with creatives and filmmakers.\n\n### What’s possible with Flow\n\nFlow is inspired by what it feels like when time slows down and creation is effortless, iterative and full of possibility. It’s custom-designed for Veo, Google’s state-of-the-art generative video model, with exceptional prompt adherence and stunning cinematic outputs that excel at physics and realism. Behind the scenes, Gemini models make prompting intuitive, so you can describe your vision in everyday language. You can bring your own assets to create characters, or use Flow to make your own ingredients with Imagen’s text-to-image capabilities.\n\nOnce you’ve created a subject or a scene, you can integrate those same ingredients into different clips and scenes with consistency. Or you can use a scene image to start a new shot.\n\nCreate your ingredients\n\nUse those ingredients to create a clip\n\nReference ingredients in plain language... ### Key features to unlock your storytelling\n\nFlow also comes with a range of features for professionals or those just getting started:\n\n**Camera Controls:**Master your shot with direct control over camera motion, angles and perspectives. **Scenebuilder:**Seamlessly edit and extend your existing shots — revealing more of the action or transitioning to what happens next with continuous motion and consistent characters. **Asset Management:**Easily manage and organize all of your ingredients and prompts. **Flow TV** **:**Spark your creativity with an ever-growing showcase of clips, channels, and content generated with Veo. You can see the exact prompts and techniques used for clips you like, providing a practical way to learn and adapt new styles.\n\nSeamless transitions\n\nCamera controls\n\nCinematic quality\n\n### Get started with Flow\n\nFlow is the evolution of VideoFX, a Google Labs experiment that launched last year. Starting today, Flow is available to subscribers of our Google AI Pro and Google AI Ultra plans in the U.S., with more countries coming soon.\n\nGoogle AI Pro gives you the key Flow features and 100 generations per month, and Google AI Ultra gives you the highest usage limits and early access to Veo 3 with native audio generation, bringing environmental sounds and character dialogue directly into video creation.... ### How we’re collaborating with filmmakers\n\nAs with any groundbreaking technology, we’re still understanding the full potential of AI in filmmaking. We see the emergence of these tools as an enabler, helping a new wave of filmmakers more easily tell their stories. By offering filmmakers early access to Flow, we were able to better understand how our technology could best support and integrate into their creative workflows — and we’ve woven their insights into Flow. Here are some filmmakers we partnered with and the short films they developed using Flow along with other tools and techniques.\n\n**Dave Clark**\n\nDave is an award-winning filmmaker focused on embracing new technology as part of his filmmaking. He used AI to develop two of his most recent short films, “Battalion” and “NinjaPunk.” His newest short film “Freelancers” uses Google’s AI and other tools to tell the story of two estranged adopted brothers on similar quests.\n\n**Henry Daubrez**\n\nHenry has been using tech tools in his art for the last 18 years. Earlier this year he unveiled “Kitsune” using Veo 2 — a moving short film about “love between two souls separated by everything except their shared feelings of loneliness.” Now, Henry is bringing the story of his own creative journey to life in “Electric Pink.”\n\n**Junie Lau**\n\nJunie Lau is a film director and multidisciplinary creative deeply passionate about innovation, viewing AI as a vital collaborator in expanding the boundaries of creative expression. Her work delves into artistic narratives within the hyper-modern era, including themes of virtual identity, digital humanities and digital ontology. Currently, Junie is working on a film titled “Dear Stranger,” which explores the boundless and infinite nature of universal love between a grandmother and grandchild across countless parallel worlds.\n\nAI is ushering in a new chapter of creativity and filmmaking, and while it’s still early, we see so much potential for tools like Flow to unlock new voices and creations.\n\nFor more insights on Flow and how AI helps storytellers take more risks in the creative storytelling process, watch Dave Clark, Junie Lau, and Henry Daubrez in \"Behind the Lens: AI, Creativity, and the Future of Filmmaking Tools.\"",
              "domain": "blog.google"
            },
            {
              "position": 6,
              "title": "Introducing Veo 3.1 and new creative capabilities in the Gemini API",
              "url": "https://developers.googleblog.com/en/introducing-veo-3-1-and-new-creative-capabilities-in-the-gemini-api/",
              "snippet": "**Today, we are releasing Veo 3.1 and Veo 3.1 Fast in paid preview in the** **Gemini API** **.** This updated model offers several improvements, as well as improved outputs when generating video from images. These new models are available via the Gemini API in Google AI Studio and Vertex AI. Veo 3.1 is also available in the Gemini app and Flow.\n\nVeo 3.1 and Veo 3.1 Fast empower developers to create more engaging content through significant upgrades. The models now generate\n\n**richer native audio**, from natural conversations to synchronized sound effects, and offer **greater narrative control** with an improved understanding of cinematic styles. **Enhanced image-to-video** capabilities ensure better prompt adherence while delivering superior audio and visual quality and maintaining character consistency across multiple scenes.\n\nIn addition to the model update, we are introducing new model capabilities, including using reference images to guide video generation, extend existing Veo videos, and generate transitions between a first and last frame.\n\nYou can now guide the generation process by providing up to 3 reference images of a character, object, or scene. This is helpful for maintaining character consistency across multiple shots or applying a specific style to your video.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nconfig=types.GenerateVideosConfig(\n\nreference_images=[reference_image1, reference_image2],\n\n),\n\n\n\n```... Your story is no longer limited by the original generation. With Scene extension, you can create longer videos, even lasting for a minute or more, by generating new clips that connect to your previous video. Each new video is generated based on the final second of the previous clip. This maintains visual continuity, making the feature ideal for extending shots with background audio.\n\n```\n\nfrom google import genai\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nvideo=video_to_extend\n\n\n\n```\n\nCreate smooth, natural scenes that bridge two different images. By providing a starting and an ending image, you can direct Veo 3.1 to generate the transition between them, complete with accompanying audio.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nimage=first_frame,\n\nconfig=types.GenerateVideosConfig(\n\nlast_frame=last_frame,\n\n),\n\n\n\n```\n\n**What people are building with Veo 3.1**\n\nPromise Studios, a GenAI movie studio, uses Veo 3.1 within its MUSE Platform to enhance generative storyboarding and previsualization for director-driven storytelling at production quality.... Latitude is experimenting with Veo 3.1 in its generative narrative engine to instantly bring user-created stories to life.\n\nVeo 3.1 and these new capabilities are now available in paid preview via the Gemini API.\n\nVeo 3.1 is the same price as Veo 3. We are incredibly excited to see what you will build with these new capabilities!",
              "domain": "developers.googleblog.com"
            },
            {
              "position": 7,
              "title": "Veo 3 Fast available for everyone on Vertex AI | Google Cloud Blog",
              "url": "https://cloud.google.com/blog/products/ai-machine-learning/veo-3-fast-available-for-everyone-on-vertex-ai",
              "snippet": "# Veo 3 and Veo 3 Fast are now generally available on Vertex AI\n\n##### Jason Gelman\n\nDirector, Product Management, Vertex AI... ##### Try Gemini 2.5\n\nOur most intelligent model is now available on Vertex AITry now\n\nVeo 3 has seen massive global adoption with over 70 million videos created since May, and we've seen tremendous momentum with our enterprise customers as well. Since its preview launch on Vertex AI in June, enterprise customers have already generated over 6 million videos, showcasing the incredible demand for professional-grade, scalable AI video creation.\n\nToday, we’re building on this momentum with some exciting updates to Veo on Vertex AI.\n\n\n\n**Veo 3,**our most advanced video generation model, is now generally available to everyone on Vertex AI.\n\n\n\n**Veo 3 Fast**, a model designed for speed and rapid iteration, is now generally available for everyone on Vertex AI. It's a faster way to turn text to video, from narrated product demos to short films.\n\n\n\nComing to public preview on Vertex AI in August, Veo 3 and Veo 3 Fast will also offer\n\n**image-to-video capabilities**to make it possible for you to bring static visuals and images to life. All you have to do is provide the source image along with a text prompt that describes what kind of video you want to create.\n\n**How businesses are building with Veo 3 on Vertex AI**... Google Cloud customers around the world are using Veo 3 and Veo 3 Fast on Vertex AI to create professional-quality video content with unparalleled efficiency and creative freedom. Let’s look at some examples.\n\n**Canva**\n\n“Enabling anyone to bring their ideas to life – especially their most creative ones – has been core to Canva's mission ever since we set out to empower the world to design. By democratising access to a powerful technology like Google’s Veo 3 inside Canva AI, your big ideas can now be brought to life in the highest quality video and sound, all from within your existing Canva subscription. In true Canva fashion, we’ve built this with an intuitive interface and simple editing tools in place, all backed by Canva Shield.” –\n\n**Cameron Adams, co-founder and Chief Product Officer, Canva **\n\nBut the momentum extends beyond design. The team at\n\n**BarkleyOKRP**, a leading ad agency, is using Veo 3 to speed up video production timelines.\n\n“The rapid advancements from Veo 2 to Veo 3 within such a short time frame on this project have been nothing short of remarkable. Our team undertook the task of re-creating numerous music videos initially produced with Veo 2 once Veo 3 was released, primarily due to the significantly improved synchronization between voice and mouth movements. The continuous daily progress we are witnessing is truly extraordinary.” –... **Julie Ray Barr, Senior Vice President Client Experience, BarkleyOKRP**\n\nAt global investing platform\n\n**eToro**, the team is making marketing iterations a breeze with Veo 3.\n\n“At eToro, innovation is in our DNA. As a global investing platform serving clients in 75 countries, local storytelling isn’t optional - it’s essential. With Veo 3, we produced 15 fully AI‑generated versions of our ad, each in the native language of its market, all while capturing real emotion at scale. Ironically, AI didn’t reduce humanity - it amplified it. Veo 3 lets us tell more stories, in more tongues, with more impact.” –\n\n**Shay Chikotay, Head of Creative & Content, eToro**\n\n**Razorfish**, an interactive agency and part of the Publicis Groupe, is using Veo to bring creative to life.\n\n\"For The Morelandos, our campaign with Visit Orlando and Google, we used the full Vertex AI stack—Gemini to mine real reviews, Imagen to bring the characters to life, and Veo to give them motion. Veo let us go from story to near-cinematic video in a fraction of the usual time—which meant more room to explore, iterate, and push the idea further.\" –... **Anthony Yell, Chief Creative Officer, Razorfish**\n\n**Synthesia** **, **a leading synthetic media generation company, is using Veo to contextually adapt visuals to its hyper-realistic AI avatars and voices.\n\n“Veo 3 represents a leap forward in generative AI, and its integration into Synthesia’s platform will redefine how businesses create video content. By combining our hyper-realistic AI avatars and voices with Veo-powered fully contextual visuals that adapt to each unique story, we’re giving enterprise teams the creative power to communicate with unrivalled clarity and impact.” –\n\n**Bill Leaver, Product Manager, Synthesia **\n\n**How enterprises can use Veo 3 Fast for speed and creativity **\n\n**Veo 3 Fast** is a great fit for work that requires rapid iteration and speed. It has an ideal balance between processing time and high-quality visual output, making it especially helpful for:\n\n\n\nQuickly generating and testing variations of ad concepts to respond to market trends.\n\n\n\nEfficiently creating video demonstrations for entire product catalogs from still images.\n\n\n\nDeveloping engaging animated explainers and training modules in less time.\n\n**Veo 3 and Veo 3 Fast on Vertex AI mean even more capabilities for enterprise storytelling**\n\nVeo 3 and Veo 3 Fast are designed to give creators the control and quality needed to move beyond short clips and produce complete, compelling narratives. Here are some of the core features now generally available on Vertex AI.... **Create scenes with native audio:**Veo 3 generates video and audio in a single step. This means you can create scenes with characters that speak with accurate lip-syncing, and sound effects that fit the mood. **Deliver professional quality at enterprise scale:**Veo 3 produces high-definition (1080p) video, suitable for professional marketing campaigns, product demonstrations, and internal communications. You can create content that meets brand standards, saving time and money. **Simplify content localization for global audiences:**Veo 3’s native dialogue generation helps businesses connect with an international audience by producing a video once and localizing the dialogue for dozens of languages. **Image-to-video (coming to public preview on Vertex AI in August):**Veo 3 and Veo 3 Fast can also take a single image, which can be a photo you uploaded or an AI-generated image, and animate it, creating an 8-second video clip. This feature is particularly powerful for content creators, marketers, and businesses looking to animate existing visual assets, create engaging social media content, or generate compelling product demonstrations from high-quality images.\n\n### Enterprise-grade safety and security\n\nVeo 3 and Veo 3 Fast on Vertex AI are built for scalable and responsible enterprise use. We embed digital watermarks into every frame with SynthID, helping combat misinformation and misattribution. Veo 3 and Veo 3 Fast are also covered by our indemnity for generative AI services.... ### Get started with Veo 3 and Veo 3 Fast today\n\nTo get started, go here to learn more about Veo 3 and Veo 3 Fast on Vertex AI, and try it on Vertex AI Media Studio.",
              "domain": "cloud.google.com"
            },
            {
              "position": 8,
              "title": "First look: Google Veo 3 (May/2025)",
              "url": "https://www.youtube.com/watch?v=OrVUHMK58GE",
              "snippet": "## Dr Alan D. Thompson\n##### May 25, 2025 (0:05:24)\nSource: \nhttps://www.reddit.com/r/PowerfulJRE/comments/1kt942i/these_are_all_ai_videos_generated_with_google_veo/\n\nThe Memo: https://lifearchitect.ai/memo/... {ts:0} That's one move with AI that makes haters go crazy every time. Oh, y'all\n{ts:3} gotta give them that. This is wild. It's over. We are cooked on that thread. You get me?\n{ts:9} Oh my god. Yes. Victory royale with a pickaxe. So, this is an AI video about nothing. It's about\n{ts:19} nothing. Who would watch a video about nothing? So, I went to the zoo the other day and\n{ts:27} all they had was one dog. It was a [Music] shiu. We're going to light up the\n{ts:43} [Music] [Applause] [Music]\n{ts:51} sky. I'm not sure I can go on. [Music] The sum of the squares of the two\n{ts:61} shorter sides is equal to the square of the longest side. We can talk. No more silence. Yes, we can talk. We can talk.\n{ts:68} We can talk. We can talk with accents. Oh, I think that would be marvelous. Yes, it is very fun. But yes, it is very\n{ts:75} good. Very fun. I can talk. Yes, we can talk. Yes, we can talk. We can talk. We can talk. Yes, we... {ts:84} can talk. No. Yes, we can talk as cartoons. This is amazing. Imagine all the narrative\n{ts:92} possibilities. We can sing talk. Let's talk. So, what are we going to talk about now? What are we going to\n{ts:104} talk about now that we can talk? I have no idea. What do you want to talk about now that I can talk?\n{ts:112} Yeah. I I don't know if I have something to say.\n{ts:118} We can talk about how magical this is. Is [Music]\n{ts:124} I want to say something important, something deep. The future is still in our hands.\n{ts:132} That's cliche dialogue. Let's not talk. How much wood would a woodchuck chuck if\n{ts:146} a woodchuck could chuck wood? How much wood would a woodchuck chuck if a woodchuck could chuck wood? How much\n{ts:155} wood would a woodchuck chuck if a woodchuck could chuck wood? How much wood would a woodchuck chuck if a\n{ts:165} woodchuck could chuck wood? How much wood would a woodchuck chuck if a woodchuck could chuck wood?... {ts:175} How much wood would a woodchuck chuck if a woodchuck could chuck wood? How much wood would a woodchuck\n{ts:187} chuck? [Applause] Welcome to a non-existent car show.\n{ts:193} Let's see some opinions. I mean, man, the acceleration is crazy. You look far, step on the pedal, and you\n{ts:202} are there. I feel safe with him in an SUV, and it seems to be like the right type of car for him. I think the range\n{ts:210} is only um only going to get better. Sorry. We don't want to drive gas cars anymore.\n{ts:217} Yeah. No more gas cars. You can see uh I'm kind of a kind of a misfit here, but uh don't tell anyone\n{ts:226} I've just bought an electric car. I think it's really great for families and for little babies with all the safety\n{ts:231} features that these SUVs have. But what you're really seeing is that technology is going to be very, very important in\n{ts:239} terms of how we go forward. It was um great to come to the conference because my husband loves cars. I think I... {ts:249} have to buy an EV now. I love my muscle cars, but I try to stay as healthy as I can so\n{ts:257} I can make it to the next car show. When we get in there, I want no [ __ ] You stay on my six at all\n{ts:269} times. [Music] Stay sharp. These [ __ ] are nasty and\n{ts:282} dangerous. Stay alert. [ __ ] What the hell happened here? Where are the\n{ts:291} [Applause] bodies? Heat. Heat. Fire heat.",
              "domain": "www.youtube.com"
            },
            {
              "position": 9,
              "title": "Google releases Veo 3.1, adds it to Flow video editor",
              "url": "https://techcrunch.com/2025/10/15/google-releases-veo-3-1-adds-it-to-flow-video-editor/",
              "snippet": "In Brief\nGoogle launched its new video model Veo 3.1 with improved audio output, granular editing controls, and better output for image to video. It said that Veo 3.1 builds on May's Veo 3 release and generates more realistic clips and adheres to prompts better.\nThe model allows users to add an object to the video and have it blend into the clip's style, Google said. Soon, users will be able to remove an existing object from the video in Flow, too.\nVeo 3 already has edit features such as adding reference images to drive a character, providing the first and last frame to generate a clip using AI, and the ability to extend an existing video based on the last few frames. With Veo 3.1, Google is adding audio to all these features to make the clips more lively.\nThe company is rolling out the model to its video editor Flow, the Gemini App, along with Vertex and Gemini APIs. It said that since Flow's launch in May, users have created more than 275 million videos on the app.",
              "domain": "techcrunch.com"
            },
            {
              "position": 10,
              "title": "Gemini AI video generator powered by Veo 3.1",
              "url": "https://gemini.google/overview/video-generation/",
              "snippet": "# Break the\n\n**silence** with Veo 3.1\n\nCreate high-quality, 8-second videos with Veo 3.1, our latest AI video generator. Simply describe what you have in mind or upload a photo and watch your ideas come to life with native audio generation. Try it with a Google AI Pro plan or get the highest access with the Ultra plan.\n\n**Veo 3.1** speaks for itself\n\n## Dream it. Describe it.\n\n**Done.**\n\n## For Exploring\n\nPlay with diverse styles, bring animated characters to life, and combine objects in ways you never thought possible. See what you can create using text to video with AI.\n\n## For Sharing\n\nCreate funny memes, turn inside jokes into videos, re-imagine special moments, and add a personal touch to make someone smile.\n\n## For Brainstorming\n\nBreak through creative blocks and visualize your ideas in a flash. From product concepts and designs to rapid prototyping and storytelling, Gemini can help.\n\n## Learn more about our\n\n**Veo Models**\n\nCreate videos with sound using our video generation model that maintains high-quality while optimizing for speed.\n\nCreate high-quality, 8-second videos with sound using our state-of-the-art video generation model.... ## Frequently asked questions\n\nYes, you can create and share videos in your mobile Gemini app. To create videos, tap the \"video\" button in your prompt bar. If you don't see it, tap the button with three dots to view more options.\n\nTry Veo 3.1 Fast with a Google AI Pro plan or get the highest access to Veo 3.1 in Google AI Ultra. Country availability here.\n\nFor now, the ability to generate a video from a photo is not available in the European Economic Area, Switzerland, or the United Kingdom.\n\nWe’ve taken several important safety steps to make AI video generation a safe experience. This includes extensive red teaming and evaluation aimed at preventing the generation of content that violates our policies. Additionally, all videos generated with Veo in the Gemini app are marked with a visible watermark and SynthID, a digital watermark embedded in each frame, which indicates the videos are AI-generated.\n\nGemini's outputs are primarily determined by user prompts and like any generative AI tool, there may be instances where it generates content that some individuals find objectionable. We’ll continue to listen to your feedback through the thumbs up/down buttons and make ongoing improvements. For more details, you can read about our approach on our website.\n\nResults for illustrative purposes and may vary. Internet and subscription for certain features required. Available to users 18+. Create responsibly.",
              "domain": "gemini.google"
            }
          ],
          "success": true,
          "error": null
        },
        {
          "query_id": "q1",
          "query": "Google official announcement veo3.1 release 2025",
          "claim_id": "claim_1",
          "query_type": "source_verification",
          "priority": "high",
          "results": [
            {
              "position": 1,
              "title": "Introducing Veo 3.1 and advanced capabilities in Flow - Google Blog",
              "url": "https://blog.google/technology/ai/veo-updates-flow/",
              "snippet": "# Introducing Veo 3.1 and advanced capabilities in Flow\n\nFive months ago, we introduced Flow, our AI filmmaking tool powered by Veo, and have been inspired by the creativity it has sparked with over 275 million videos generated in Flow\n\n1\n\n. We're always listening to your feedback, and we've heard that you want more artistic control within Flow, with increased support for audio across all features.\n\nToday, we’re introducing new and enhanced creative capabilities to edit your clips, giving you more granular control over your final scene. For the first time, we’re also bringing audio to existing capabilities like “Ingredients to Video,” “Frames to Video” and “Extend.”\n\nWe’re also introducing Veo 3.1, which brings richer audio, more narrative control, and enhanced realism that captures true-to-life textures. Veo 3.1 is state-of-the-art and builds on Veo 3, with stronger prompt adherence and improved audiovisual quality when turning images into videos.\n\n## Refine your narrative with audio and more control\n\nWith Veo 3.1, we’re bringing audio to existing capabilities to help you craft the perfect scene. These features are experimental and actively improving, and we’re excited to see what you create as we iterate based on your feedback.\n\nNow, with rich, generated audio, you can:\n\n**Craft the look of your scene.**With \"Ingredients to Video,\" you can use multiple reference images to control the characters, objects and style. Flow uses your ingredients to create a final scene that looks just as you envisioned. **Control the shot from start to finish.**Provide a starting and ending image with “Frames to Video,” and Flow will generate a seamless video that bridges the two, perfect for artful and epic transitions. **Create longer, seamless shots.**With \"Extend,\" you can create longer videos, even lasting for a minute or more, that connect to and continue the action from your original clip. Each video is generated based on the final second of your previous clip, making it most useful for creating a longer establishing shot.... ## Edit your ingredients and videos with more precision\n\nGreat ideas can strike at any point in the creative process. For moments when the first take isn't the final one, we're introducing new editing capabilities directly within Flow to help you reimagine and perfect your scenes.\n\n**Add new elements to any scene.**With “Insert,” introduce anything you can imagine, from realistic details to fantastical creatures. Flow now handles complex details like shadows and scene lighting, making the addition look natural. **Remove unwanted objects or characters seamlessly.**Soon, you’ll be able to take anything out of a scene, and Flow will reconstruct the background and surroundings, making it look as though the object was never there.\n\n## Start creating in Flow today\n\nWith more precise editing capabilities, audio across all existing features and higher-quality outputs powered by Veo 3.1, we're opening up new possibilities for richer, more powerful video storytelling right inside Flow.\n\nThe Veo 3.1 model is also available via the Gemini API for developers, Vertex AI for enterprise customers, and the Gemini app. New capabilities are available in both Gemini API\n\n2\n\nand Vertex AI\n\n3",
              "domain": "blog.google"
            },
            {
              "position": 2,
              "title": "Google announces Veo 3.1 and updates Flow with more controls, tools",
              "url": "https://9to5google.com/2025/10/15/veo-3-1/",
              "snippet": "Google today announced Veo 3.1 as its latest video generation model, with Flow getting a number of updates to take advantage of the latest capabilities.\n\nCompared to Veo 3, which was announced at I/O 2025 in May, this new version offers richer audio and “enhanced realism that captures true-to-life textures.” Veo 3.1 has a deeper understanding of storytelling, cinematic styles, and character interactions to give you more narrative control. The image-to-video capability benefits from improved audio-visual quality and better follows your prompt.\n\nVeo 3.1 and Veo 3.1 Fast are available in the Gemini app, as well as the Gemini API and Vertex AI, to power text-to-video and image-to-video for horizontal (16×9) and vertical (9×16) outputs.\n\nMeanwhile, Google is updating the Flow filmmaking tool to take advantage of Veo 3.1. Audio generation is coming to:\n\n**Ingredients to Video**: This can include images of multiple characters, objects, and styles **Scene extension**: You can Extend an existing clip, with Google basing it on the last second **Frames to Video**: This lets you specify the starting and ending images, with Flow bridging the two\n\nFlow is getting new editing capabilities. You can insert elements like objects, characters, and details, with Google handling shadows, scene lighting, and other complex details to make everything look natural.\n\nComing soon is the ability to remove objects and characters from a scene, with Flow working to reconstruct the background and surroundings to make the edit seamless.\n\n\n\n## Comments",
              "domain": "9to5google.com"
            },
            {
              "position": 3,
              "title": "Google rolls out its new Veo 3 video-generation model ...",
              "url": "https://techcrunch.com/2025/07/03/google-rolls-out-its-new-veo-3-video-generation-model-globally/",
              "snippet": "Google on Thursday said it has begun rolling out its Veo 3 video generation model to Gemini users in more than 159 countries.\n\nVideo generation via the new model is available only to paying subscribers of Google’s AI Pro plan and is capped at three videos per day.\n\nVeo 3, which Google showed off in May, lets users generate videos up to eight seconds long using text prompts.\n\nGoogle’s Josh Woodward has said that the company is working on adding image-to-video generation capabilities to Gemini.",
              "domain": "techcrunch.com"
            },
            {
              "position": 4,
              "title": "Google releases Veo 3.1, adds it to Flow video editor",
              "url": "https://techcrunch.com/2025/10/15/google-releases-veo-3-1-adds-it-to-flow-video-editor/",
              "snippet": "In Brief\nGoogle launched its new video model Veo 3.1 with improved audio output, granular editing controls, and better output for image to video. It said that Veo 3.1 builds on May's Veo 3 release and generates more realistic clips and adheres to prompts better.\nThe model allows users to add an object to the video and have it blend into the clip's style, Google said. Soon, users will be able to remove an existing object from the video in Flow, too.\nVeo 3 already has edit features such as adding reference images to drive a character, providing the first and last frame to generate a clip using AI, and the ability to extend an existing video based on the last few frames. With Veo 3.1, Google is adding audio to all these features to make the clips more lively.\nThe company is rolling out the model to its video editor Flow, the Gemini App, along with Vertex and Gemini APIs. It said that since Flow's launch in May, users have created more than 275 million videos on the app.",
              "domain": "techcrunch.com"
            },
            {
              "position": 5,
              "title": "Build with Veo 3, now available in the Gemini API",
              "url": "https://developers.googleblog.com/en/veo-3-now-available-gemini-api/",
              "snippet": "First unveiled at Google I/O 2025, people around the world have already generated tens of millions of high-quality videos with Veo 3 (along with some new fun and interesting video trends). It is our first video model to incorporate high-fidelity video outputs and native audio, first with text-to-video and soon with image-to-video.\n\nDevelopers are already experimenting with Veo 3, discovering how the model can help them brainstorm content, rapidly iterate, and be more efficient.\n\nVeo 3 is designed to handle a range of video generation tasks, from cinematic narratives to dynamic character animations. With Veo 3, you can create more immersive experiences by not only generating stunning visuals, but also audio like dialogue and sound effects.\n\nLet’s take a look at some examples.\n\nExplore these examples and more with Veo 3 in Google AI Studio, available as an SDK template and interactive Starter App to remix, copy and extend. The Starter App and its sample code offer a convenient way for Paid Tier users to rapidly prototype with Veo 3 and more on the Gemini API, directly from Google AI Studio.\n\nClick the Key button in the top right of the AI Studio Build interface to select a Google Cloud Project with billing enabled to use the Paid Tier in AI Studio apps. See the FAQs for more.... Veo 3 will be priced at $0.75 per second for video and audio output. Additionally, Veo 3 Fast will be available soon, offering a faster and more cost-effective option for video creation.\n\nHere’s a basic Python example to create a video:\n\n```\n\nimport time\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.0-generate-preview\",\n\nprompt=\"a close-up shot of a golden retriever playing in a field of sunflowers\",\n\nconfig=types.GenerateVideosConfig(\n\nnegative_prompt=\"barking, woofing\",\n\n),\n\n\n\n# Waiting for the video(s) to be generated\n\nwhile not operation.done:\n\ntime.sleep(20)\n\noperation = client.operations.get(operation)\n\ngenerated_video = operation.result.generated_videos[0]\n\nclient.files.download(file=generated_video.video)\n\ngenerated_video.video.save(\"veo3_video.mp4\")\n\n```\n\nAll videos generated by Veo 3 models will continue to include a digital SynthID watermark. To get started, check out the documentation, cookbook, and a Veo 3 starter app in Google AI Studio:\n\nIn addition to being available via the Gemini API in Google AI Studio, Veo 3 is also available to Google AI subscribers in the Gemini app and Flow, and to enterprise customers via Vertex AI.... 1: Fluffy Characters Stop Motion: Inside a brightly colored, cozy kitchen made of felt and yarn. Professor Nibbles, a plump, fluffy hamster with oversized glasses, nervously stirs a bubbling pot on a miniature stove, muttering, \"Just a little more... 'essence of savory,' as the recipe calls for.\" The camera is a mid-shot, capturing his frantic stirring. Suddenly, the pot emits a loud \"POP!\" followed by a comical \"whoosh\" sound, and a geyser of iridescent green slime erupts, covering the entire kitchen. Professor Nibbles shrieks, \"Oh, dear! Not again!\" and scurries away, leaving a trail of tiny, panicked squeaks.\n\n2: The sequence begins with an extreme close-up of a single gear, slowly turning and reflecting harsh sunlight. The camera gradually pulls back in a continuous movement, revealing this is but one component of a colossal, mechanical heart half-buried in a desolate, rust-colored desert. A sweeping aerial shot establishes its enormous scale and isolation in the barren landscape. The camera descends to capture pipes hissing steam and the rhythmic thumping that echoes across the empty plains. A subtle shake effect synchronizes with each massive heartbeat. A lateral tracking shot discovers tiny, robed figures scurrying across the metallic surface. The camera follows one such figure in a detailed tracking shot as they perform meticulous maintenance, polishing brass valves and tightening immense bolts. A complex movement circles the entire structure, capturing different maintenance teams working in precarious positions across its rusted exterior. The final shot begins tight on the meticulous work of one tiny figure before executing a dramatic pull-out that reveals the true scale of the heart and the minuscule size of its caretakers, tending to the vital organ of an unseen, sleeping giant that extends beyond the frame.",
              "domain": "developers.googleblog.com"
            },
            {
              "position": 6,
              "title": "Introducing Veo 3.1 and new creative capabilities in the Gemini API",
              "url": "https://developers.googleblog.com/en/introducing-veo-3-1-and-new-creative-capabilities-in-the-gemini-api/",
              "snippet": "**Today, we are releasing Veo 3.1 and Veo 3.1 Fast in paid preview in the** **Gemini API** **.** This updated model offers several improvements, as well as improved outputs when generating video from images. These new models are available via the Gemini API in Google AI Studio and Vertex AI. Veo 3.1 is also available in the Gemini app and Flow.\n\nVeo 3.1 and Veo 3.1 Fast empower developers to create more engaging content through significant upgrades. The models now generate\n\n**richer native audio**, from natural conversations to synchronized sound effects, and offer **greater narrative control** with an improved understanding of cinematic styles. **Enhanced image-to-video** capabilities ensure better prompt adherence while delivering superior audio and visual quality and maintaining character consistency across multiple scenes.\n\nIn addition to the model update, we are introducing new model capabilities, including using reference images to guide video generation, extend existing Veo videos, and generate transitions between a first and last frame.\n\nYou can now guide the generation process by providing up to 3 reference images of a character, object, or scene. This is helpful for maintaining character consistency across multiple shots or applying a specific style to your video.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nconfig=types.GenerateVideosConfig(\n\nreference_images=[reference_image1, reference_image2],\n\n),\n\n\n\n```... Your story is no longer limited by the original generation. With Scene extension, you can create longer videos, even lasting for a minute or more, by generating new clips that connect to your previous video. Each new video is generated based on the final second of the previous clip. This maintains visual continuity, making the feature ideal for extending shots with background audio.\n\n```\n\nfrom google import genai\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nvideo=video_to_extend\n\n\n\n```\n\nCreate smooth, natural scenes that bridge two different images. By providing a starting and an ending image, you can direct Veo 3.1 to generate the transition between them, complete with accompanying audio.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nimage=first_frame,\n\nconfig=types.GenerateVideosConfig(\n\nlast_frame=last_frame,\n\n),\n\n\n\n```\n\n**What people are building with Veo 3.1**\n\nPromise Studios, a GenAI movie studio, uses Veo 3.1 within its MUSE Platform to enhance generative storyboarding and previsualization for director-driven storytelling at production quality.... Latitude is experimenting with Veo 3.1 in its generative narrative engine to instantly bring user-created stories to life.\n\nVeo 3.1 and these new capabilities are now available in paid preview via the Gemini API.\n\nVeo 3.1 is the same price as Veo 3. We are incredibly excited to see what you will build with these new capabilities!",
              "domain": "developers.googleblog.com"
            },
            {
              "position": 7,
              "title": "Google expands Veo 3 to Gemini in more countries and on ...",
              "url": "https://blog.google/products/gemini/veo-3-expansion-mobile/",
              "snippet": "We’re thrilled by the response to Veo 3. The Google AI Ultra plan grants the highest access to Veo 3 and later today we’re launching it in the UK. The Ultra plan is now available in 73 countries, and we’re working hard to bring it to even more.\n\nGoogle AI Pro subscribers in those countries get limited access to Veo 3 in Flow, and a 10-pack of trial video generations to test it out in the Gemini app. Starting today, Pro subscribers will also have access to Veo 3 in the Gemini mobile app.\n\nIt's important that people can access provenance tools for content online. The SynthID watermark is embedded in all content generated by Google's generative AI models. Our SynthID Detector rolled out to early testers last week, and we plan to expand access soon. As an additional step to help people identify AI-generated content, today we're adding a visible watermark to all videos, except for videos generated by Ultra members in Flow, our tool for AI filmmakers.",
              "domain": "blog.google"
            },
            {
              "position": 8,
              "title": "What Is Google Veo 3.1? A Beginner's Guide to AI Video ...",
              "url": "https://skywork.ai/blog/google-veo-3-1-beginners-guide-ai-video-model/",
              "snippet": "If you’ve seen short, cinematic clips made “from a prompt” and wondered how it works, you’re in the right place. This guide will walk you through Google’s latest video generation model, Veo 3.1, in plain English—what it is, why it matters, where you can try it, and a simple first project you can finish in about 10 minutes. Don’t worry if you’ve never touched AI video before. We’ll move step by step and flag common gotchas so you can avoid them.\n\n## What is Google Veo 3.1?\n\nGoogle Veo 3.1 is a text-to-video AI model that turns short written prompts and references into short video clips, with native audio support and more control over storytelling and style. In October 2025, Google announced Veo 3.1 (and a faster variant) in paid preview for developers and creators. According to the Google Developers Blog announcement (Oct 15, 2025), Veo 3.1 is available via the Gemini API in Google AI Studio and Vertex AI, and it adds richer native audio, improved adherence to cinematic styles, and new creative controls.\n\nWhy it matters for beginners: Veo 3.1’s improvements make it easier to describe the mood and motion you want—like “a slow dolly-in on a vintage typewriter with soft rain ambience”—and actually get something close, without wrestling with advanced settings.... ## What’s new compared to earlier Veo versions?\n\nGoogle calls out a few key upgrades in Veo 3.1:\n\n- Richer native audio generation and better narrative control, with improved understanding of cinematic styles (publisher: Google; see the Developers Blog announcement (2025)).\n\n- New creative tools in supporting apps: you can guide generation with multiple reference images, extend existing Veo clips, or bridge between a first and last frame to create transitions. These capabilities are highlighted in the Google Blog on Veo 3.1 and Flow (Oct 2025).\n\n- Native audio emphasis and model positioning are also described on the DeepMind Veo model page, which introduces Veo’s “video, meet audio” concept.\n\nIndependent coverage, like TechCrunch’s report on the Veo 3.1 release (Oct 2025), provides context on rollout and app integration. For hard limits (length, resolution), always defer to Google’s official docs, as those details can change.\n\n## Where can you use Veo 3.1 today?\n\nAs of October 2025:\n\n- Flow (Google’s AI video editor): Google’s announcement notes Veo 3.1 and advanced creative controls in Flow, including “Ingredients to Video” (use multiple images as style/character references), “Frames to Video” (bridge between start/end frames), and “Extend” (lengthen a clip). Details are outlined in the Google Blog on Veo 3.1 and Flow (2025).\n\n- Gemini API (Google AI Studio and Vertex AI): Veo 3.1 and Veo 3.1 Fast are in paid preview via the Gemini API, per the Google Developers Blog announcement (2025).\n\nImportant: Access tiers, regions, and pricing can vary and may change. If you’re not seeing options in your account, check Google’s documentation or support for your account type and region.... ## What can Veo 3.1 generate right now? (Practical limits to know)\n\n- In Vertex AI’s preview for Veo 3.1 base generations, the documented lengths are currently short (choose 4, 6, or 8 seconds). This is specified on the Vertex AI Veo 3.1 preview page.\n\n- In Gemini API extension workflows, you can extend Veo-generated clips. The API docs describe a maximum of up to 141 seconds for input Veo videos in those extension scenarios, with 720p listed in that context. See the Gemini API video documentation (Google).\n\nThese numbers help set expectations: start with short shots, then extend or chain shots together. Avoid assuming 1080p or minute-long base generations unless Google’s official docs explicitly state it for your environment.... ## A 10-minute quickstart: your first Veo 3.1 clip\n\nWe’ll make a simple “coffee shop mood” shot you can adapt for social posts.\n\n- Choose your aspect ratio\n\n- 16:9 (landscape) for YouTube and desktop-first screens\n\n- 9:16 (vertical) for TikTok, Reels, and Shorts Pick one at the start to avoid accidental cropping later.\n\n- Write a clear, concrete prompt Use this structure to get reliable results: subject + action + setting + style + camera + audio.\n\n- Example prompt: “A barista gently places a ceramic cup on a wooden counter; shallow depth of field; warm morning light streaming through windows; slow dolly-in; subtle steam rising; soft cafe ambience, no dialogue, light jazz in the background.”\n\nIf you want a deeper primer on prompt structure and clarity, see these prompt engineering best practices for beginners.\n\n- Generate in your chosen interface\n\n- In Flow: Start a new project, choose Veo 3.1, paste your prompt, set aspect ratio, and generate.\n\n- In Gemini API/Vertex AI: Use the model/version and parameters documented for Veo 3.1 in your environment. Begin with short durations and default settings.\n\n- Review the output like a director\n\n- Does the subject and action match? Is the lighting/mood close?\n\n- Is the camera movement smooth? Is audio what you expected?\n\n- Make one or two changes per iteration—small edits beat big rewrites.\n\n- Refine with controls\n\n- Reference images: If you want consistency (e.g., the same mug or barista style), use up to three reference images to guide the look. This capability is described in Google’s announcements (2025).\n\n- Frames to Video: Provide a starting and ending frame to shape the motion between them.\n\n- Extend: Lengthen your favorite moment to create a longer beat.\n\n- Export Choose the format/aspect ratio you started with. If you need both vertical and horizontal versions, plan to reframe or regenerate with the other aspect ratio rather than cropping aggressively.... ## A practical planning example using Skywork AI (optional, 5 minutes)\n\nSkywork AI can help you prepare the words before you ever hit “Generate.” Disclosure: Skywork AI is our product.\n\nHere’s a neutral, step-by-step way to use it purely for planning:\n\n- Open Skywork and create a new document. Ask for a short video outline: “30-second coffee shop mood piece: 3 shots, warm tone, slow camera.”\n\n- Have it draft a compact shot list with camera moves and audio notes, like:\n\n- Close-up of cup; slow dolly-in; soft steam; light jazz; no dialogue\n\n- Medium barista hands; gentle rack focus; cafe ambience; espresso hiss\n\n- Wide room tone; sunbeams; slow tilt up; footsteps and cups\n\n- Ask for a final prompt assembled from the shot you want to generate first. Copy that prompt into Veo 3.1.\n\nIf you prefer to outline prompts and story beats yourself, this short guide to Skywork’s General Mode for planning and outlining walks through a simple, distraction-free workflow.... ## FAQs for first-time users\n\n\n\nIs Veo 3.1 free? No. Google describes Veo 3.1 and Veo 3.1 Fast as being in paid preview via the Gemini API as of October 2025. See the Google Developers Blog announcement (2025). Pricing varies by product and usage.\n\n\n\nWhere can I try Veo 3.1? Flow (for a visual editor), and the Gemini API via Google AI Studio and Vertex AI (for developers). Google outlines these options in the Veo 3.1 and Flow post (2025) and the Developers Blog announcement (2025).\n\n\n\nHow long can my video be? It depends on the environment and workflow. In Vertex AI’s Veo 3.1 preview, base generations are short (4, 6, or 8 seconds). In Gemini API extension workflows, input Veo videos can be extended up to 141 seconds. See the Vertex AI preview page and the Gemini API video docs.\n\n\n\nWhat resolutions are supported? The Gemini API documentation for extension workflows references 720p in that context. Official materials do not universally guarantee 1080p for base Veo 3.1 generations at this time.\n\n\n\nCan I keep a character or object consistent across shots? Yes. Google notes you can guide Veo 3.1 with up to three reference images for a character, object, or scene. This is described in the Developers Blog announcement (2025).\n\n\n\nCan I add my own audio and sync it? Veo 3.1 focuses on native audio generation controlled via your prompt. The public pages cited here do not detail a full “upload and auto-sync” workflow, so avoid relying on that unless you see it documented for your account.... ## Next steps\n\n- Try a tiny project: one 6–8 second shot with a clear prompt. If you like the result, use “Extend” or stitch a second shot with “Frames to Video.”\n\n- Build a simple storyboard first. If you want help outlining ideas, you can use Skywork AI to draft prompts and shot lists before you generate. Keep it simple: one scene, one action, one camera move.\n\n- When you’re ready to go deeper, explore Google’s official resources: the Veo 3.1 and Flow post (2025), the Developers Blog announcement (2025), the DeepMind Veo overview, the Gemini API video docs, and the Vertex AI Veo 3.1 preview.\n\nYou don’t need special “artistic talent” to start—just one clear sentence and a little patience. Generate, review, tweak, repeat. That’s how everyone learns, and you’ll be surprised how quickly it clicks.",
              "domain": "skywork.ai"
            },
            {
              "position": 9,
              "title": "Veo 3 available for everyone in public preview on Vertex AI - Google Cloud",
              "url": "https://cloud.google.com/blog/products/ai-machine-learning/veo-3-available-for-everyone-in-public-preview-on-vertex-ai",
              "snippet": "# You dream it, Veo creates it: Veo 3 is now available for everyone in public preview on Vertex AI\n\n##### Jason Gelman\n\nDirector, Product Management, Vertex AI\n\n##### Try Gemini 2.5\n\nOur most intelligent model is now available on Vertex AITry now\n\nA great story doesn't just tell you, it shows you. With Veo 3, we’ve leapt forward in combining video and audio generation to take storytelling to the next level.\n\nToday, we’re excited to share that Veo 3 is now available for all Google Cloud customers and partners in public preview on Vertex AI.\n\n**Why this matters: **Veo 3 is your partner for creating near-cinematic quality generative video, moving beyond novelty to narrative-driven creation. It not only brings stunning visual quality, but now adds sound from background sounds to dialogue. With Veo 3 on Vertex AI, you can take advantage of three powerful new capabilities:\n\n\n\n**Fluid, natural videos that synchronize video with audio and dialogue.**Veo 3 can synchronize your audio and visuals in a single pass. The model produces rich soundscapes containing everything from dialogue and ambient noise, to sound effects and background music.\n\n\n\n**Cinematic video that captures creative nuances.**Veo 3 makes it easy to capture creative nuances and detailed scene interactions in your prompt, from the shade of the sky to the precise way the sun hits water in the afternoon light, and produces high-definition video.\n\n\n\n**Realistic movement that simulates real-world physics.**To create believable scenes, Veo 3 simulates real-world physics. This results in realistic water movement, accurate shadows connected with objects and characters, and natural human motion.... ### Businesses are already using Veo to make creating easier\n\nVeo 3 is helping Google Cloud customers create external content – from social media ads to product demos – and internal materials like training videos and presentations. Hear directly from the teams:\n\n“Veo 3 has marked the difference within the gen AI industry, and we’re glad that Freepik users have been some of the first to try the model out. The quality of the video generations combined with the audio integration option is the game changer in our AI Suite. We look forward to continuing this collaboration to bring the best AI tools and features to our users” – Omar Pera, CPO, Freepik\n\n“Creativity is deeply personal, and our goal is to build a platform that adapts to every workflow. By working with Google, we’re combining the best technologies to give creators more control, efficiency, and power than ever before. Our collaboration with Google Cloud represents a strategic evolution that will not only enhance accessibility and efficiency but fundamentally transform how people create. We believe the future of generative video technology will leverage the best technologies to build the most flexible and accessible tools. This is an exciting step toward realizing that vision” – Zeev Farbman, Co-Founder & CEO, Lightricks.\n\n\"Veo 3 is the single greatest leap forward in practically useful AI for advertising since genAI first broke into the mainstream in 2023. By allowing brands to make fully fledged films from a single prompt - including brand, story, video, sound effects, voiceovers and more - Veo3 in one swoop lowers the barriers to entry to gen AI for creative people and elevates gen AI to a top tier brand building tool usable at every stage of the marketing funnel.\" – Will Hanschell, co-founder and CEO, , Pencil\n\n**Bring your vision to life with Veo 3 today**\n\nVeo 3 on Vertex AI is built for scalable enterprise use with crucial guardrails like safety filter controls and SynthID to ensure responsible deployment for any use case. To get started, go here to learn more about Veo 3 on Vertex AI and try it on Vertex AI Media Studio. Get started today!",
              "domain": "cloud.google.com"
            },
            {
              "position": 10,
              "title": "Google's Veo 3.1 is better at generating videos from images",
              "url": "https://www.engadget.com/ai/googles-veo-31-is-better-at-generating-videos-from-images-220829129.html",
              "snippet": "Google has released a new update to its Veo AI video generation model that should make it do a better job of sticking to prompts and converting images into videos. Veo 3.1 is available to try today through Google's Gemini API and is now also powering the company's Flow video editor.\nVeo 3.1 builds on the new capabilities Google introduced with launch of Veo 3 at Google I/O 2025. The new model offers better \"prompt adherence,\" according to Google, and should have an easier time creating videos based on the image \"ingredients\" you upload alongside your written prompt. Veo 3.1 also makes it possible to convert images to video and generate audio at the same time, a capability that wasn't available with Veo 3.\nIn Flow, Veo 3.1 supports at least a new feature that gives you finer control over the videos you generate. With what Google calls \"Frame to Video,\" Flow lets you upload a first and last frame, and then generates the video in-between. Adobe Firefly, which is powered by Veo 3, offers a similar feature, but Flow will be able to pull it off and create audio at the same time. Those added audio skills will also apply to the video editor's ability to extend clips and insert objects into existing footage, too.\nBased on the samples Google's shared, videos generated with Veo 3.1 still have an uncanny quality that seems to vary greatly depending on the prompt and subject. Even if it's missing some of the realism of OpenAI's Sora 2, though, the company's decision to try and make Veo more useful to people who actually work with video rather than a source of social media spam is a welcome move.",
              "domain": "www.engadget.com"
            }
          ],
          "success": true,
          "error": null
        },
        {
          "query_id": "q4",
          "query": "Google veo3.1 release news 2025",
          "claim_id": "claim_1",
          "query_type": "source_verification",
          "priority": "high",
          "results": [
            {
              "position": 1,
              "title": "Introducing Veo 3.1 and advanced capabilities in Flow - Google Blog",
              "url": "https://blog.google/technology/ai/veo-updates-flow/",
              "snippet": "# Introducing Veo 3.1 and advanced capabilities in Flow\n\nFive months ago, we introduced Flow, our AI filmmaking tool powered by Veo, and have been inspired by the creativity it has sparked with over 275 million videos generated in Flow\n\n1\n\n. We're always listening to your feedback, and we've heard that you want more artistic control within Flow, with increased support for audio across all features.\n\nToday, we’re introducing new and enhanced creative capabilities to edit your clips, giving you more granular control over your final scene. For the first time, we’re also bringing audio to existing capabilities like “Ingredients to Video,” “Frames to Video” and “Extend.”\n\nWe’re also introducing Veo 3.1, which brings richer audio, more narrative control, and enhanced realism that captures true-to-life textures. Veo 3.1 is state-of-the-art and builds on Veo 3, with stronger prompt adherence and improved audiovisual quality when turning images into videos.\n\n## Refine your narrative with audio and more control\n\nWith Veo 3.1, we’re bringing audio to existing capabilities to help you craft the perfect scene. These features are experimental and actively improving, and we’re excited to see what you create as we iterate based on your feedback.\n\nNow, with rich, generated audio, you can:\n\n**Craft the look of your scene.**With \"Ingredients to Video,\" you can use multiple reference images to control the characters, objects and style. Flow uses your ingredients to create a final scene that looks just as you envisioned. **Control the shot from start to finish.**Provide a starting and ending image with “Frames to Video,” and Flow will generate a seamless video that bridges the two, perfect for artful and epic transitions. **Create longer, seamless shots.**With \"Extend,\" you can create longer videos, even lasting for a minute or more, that connect to and continue the action from your original clip. Each video is generated based on the final second of your previous clip, making it most useful for creating a longer establishing shot.... ## Edit your ingredients and videos with more precision\n\nGreat ideas can strike at any point in the creative process. For moments when the first take isn't the final one, we're introducing new editing capabilities directly within Flow to help you reimagine and perfect your scenes.\n\n**Add new elements to any scene.**With “Insert,” introduce anything you can imagine, from realistic details to fantastical creatures. Flow now handles complex details like shadows and scene lighting, making the addition look natural. **Remove unwanted objects or characters seamlessly.**Soon, you’ll be able to take anything out of a scene, and Flow will reconstruct the background and surroundings, making it look as though the object was never there.\n\n## Start creating in Flow today\n\nWith more precise editing capabilities, audio across all existing features and higher-quality outputs powered by Veo 3.1, we're opening up new possibilities for richer, more powerful video storytelling right inside Flow.\n\nThe Veo 3.1 model is also available via the Gemini API for developers, Vertex AI for enterprise customers, and the Gemini app. New capabilities are available in both Gemini API\n\n2\n\nand Vertex AI\n\n3",
              "domain": "blog.google"
            },
            {
              "position": 2,
              "title": "Google announces Veo 3.1 and updates Flow with more controls, tools",
              "url": "https://9to5google.com/2025/10/15/veo-3-1/",
              "snippet": "Google today announced Veo 3.1 as its latest video generation model, with Flow getting a number of updates to take advantage of the latest capabilities.\n\nCompared to Veo 3, which was announced at I/O 2025 in May, this new version offers richer audio and “enhanced realism that captures true-to-life textures.” Veo 3.1 has a deeper understanding of storytelling, cinematic styles, and character interactions to give you more narrative control. The image-to-video capability benefits from improved audio-visual quality and better follows your prompt.\n\nVeo 3.1 and Veo 3.1 Fast are available in the Gemini app, as well as the Gemini API and Vertex AI, to power text-to-video and image-to-video for horizontal (16×9) and vertical (9×16) outputs.\n\nMeanwhile, Google is updating the Flow filmmaking tool to take advantage of Veo 3.1. Audio generation is coming to:\n\n**Ingredients to Video**: This can include images of multiple characters, objects, and styles **Scene extension**: You can Extend an existing clip, with Google basing it on the last second **Frames to Video**: This lets you specify the starting and ending images, with Flow bridging the two\n\nFlow is getting new editing capabilities. You can insert elements like objects, characters, and details, with Google handling shadows, scene lighting, and other complex details to make everything look natural.\n\nComing soon is the ability to remove objects and characters from a scene, with Flow working to reconstruct the background and surroundings to make the edit seamless.\n\n\n\n## Comments",
              "domain": "9to5google.com"
            },
            {
              "position": 3,
              "title": "Google releases Veo 3.1, adds it to Flow video editor",
              "url": "https://techcrunch.com/2025/10/15/google-releases-veo-3-1-adds-it-to-flow-video-editor/",
              "snippet": "In Brief\nGoogle launched its new video model Veo 3.1 with improved audio output, granular editing controls, and better output for image to video. It said that Veo 3.1 builds on May's Veo 3 release and generates more realistic clips and adheres to prompts better.\nThe model allows users to add an object to the video and have it blend into the clip's style, Google said. Soon, users will be able to remove an existing object from the video in Flow, too.\nVeo 3 already has edit features such as adding reference images to drive a character, providing the first and last frame to generate a clip using AI, and the ability to extend an existing video based on the last few frames. With Veo 3.1, Google is adding audio to all these features to make the clips more lively.\nThe company is rolling out the model to its video editor Flow, the Gemini App, along with Vertex and Gemini APIs. It said that since Flow's launch in May, users have created more than 275 million videos on the app.",
              "domain": "techcrunch.com"
            },
            {
              "position": 4,
              "title": "Introducing Veo 3.1 and new creative capabilities in the Gemini API",
              "url": "https://developers.googleblog.com/en/introducing-veo-3-1-and-new-creative-capabilities-in-the-gemini-api/",
              "snippet": "**Today, we are releasing Veo 3.1 and Veo 3.1 Fast in paid preview in the** **Gemini API** **.** This updated model offers several improvements, as well as improved outputs when generating video from images. These new models are available via the Gemini API in Google AI Studio and Vertex AI. Veo 3.1 is also available in the Gemini app and Flow.\n\nVeo 3.1 and Veo 3.1 Fast empower developers to create more engaging content through significant upgrades. The models now generate\n\n**richer native audio**, from natural conversations to synchronized sound effects, and offer **greater narrative control** with an improved understanding of cinematic styles. **Enhanced image-to-video** capabilities ensure better prompt adherence while delivering superior audio and visual quality and maintaining character consistency across multiple scenes.\n\nIn addition to the model update, we are introducing new model capabilities, including using reference images to guide video generation, extend existing Veo videos, and generate transitions between a first and last frame.\n\nYou can now guide the generation process by providing up to 3 reference images of a character, object, or scene. This is helpful for maintaining character consistency across multiple shots or applying a specific style to your video.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nconfig=types.GenerateVideosConfig(\n\nreference_images=[reference_image1, reference_image2],\n\n),\n\n\n\n```... Your story is no longer limited by the original generation. With Scene extension, you can create longer videos, even lasting for a minute or more, by generating new clips that connect to your previous video. Each new video is generated based on the final second of the previous clip. This maintains visual continuity, making the feature ideal for extending shots with background audio.\n\n```\n\nfrom google import genai\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nvideo=video_to_extend\n\n\n\n```\n\nCreate smooth, natural scenes that bridge two different images. By providing a starting and an ending image, you can direct Veo 3.1 to generate the transition between them, complete with accompanying audio.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nimage=first_frame,\n\nconfig=types.GenerateVideosConfig(\n\nlast_frame=last_frame,\n\n),\n\n\n\n```\n\n**What people are building with Veo 3.1**\n\nPromise Studios, a GenAI movie studio, uses Veo 3.1 within its MUSE Platform to enhance generative storyboarding and previsualization for director-driven storytelling at production quality.... Latitude is experimenting with Veo 3.1 in its generative narrative engine to instantly bring user-created stories to life.\n\nVeo 3.1 and these new capabilities are now available in paid preview via the Gemini API.\n\nVeo 3.1 is the same price as Veo 3. We are incredibly excited to see what you will build with these new capabilities!",
              "domain": "developers.googleblog.com"
            },
            {
              "position": 5,
              "title": "We've got a surprise Pixel Drop for you.",
              "url": "https://blog.google/products/pixel/pixel-drop-july-2025/",
              "snippet": "Here’s what’s new for Pixel:\n\n**Veo 3 on Pixel:**Pixel 9 Pro owners get a full year of our Google AI Pro subscription at no cost, giving them access to the latest features in the Gemini app. And that includes Veo 3, which you can use to describe your idea and watch it come to life as a high-quality, short video, complete with natural audio. **New Circle to Search capabilities:**Dive deeper and ask follow-up questions about anything you see on your screen with AI Mode in Circle to Search, available in the U.S. and India. We’re also adding in-game help in Circle to Search, so you can find helpful articles and videos timestamped to your exact spot in your mobile game, without switching apps. **Gemini on Pixel Watch:**Get the help you need right on your wrist, with our advanced AI models powered by WearOS.",
              "domain": "blog.google"
            },
            {
              "position": 6,
              "title": "Google's Veo 3.1 is better at generating videos from images",
              "url": "https://www.engadget.com/ai/googles-veo-31-is-better-at-generating-videos-from-images-220829129.html",
              "snippet": "Google has released a new update to its Veo AI video generation model that should make it do a better job of sticking to prompts and converting images into videos. Veo 3.1 is available to try today through Google's Gemini API and is now also powering the company's Flow video editor.\nVeo 3.1 builds on the new capabilities Google introduced with launch of Veo 3 at Google I/O 2025. The new model offers better \"prompt adherence,\" according to Google, and should have an easier time creating videos based on the image \"ingredients\" you upload alongside your written prompt. Veo 3.1 also makes it possible to convert images to video and generate audio at the same time, a capability that wasn't available with Veo 3.\nIn Flow, Veo 3.1 supports at least a new feature that gives you finer control over the videos you generate. With what Google calls \"Frame to Video,\" Flow lets you upload a first and last frame, and then generates the video in-between. Adobe Firefly, which is powered by Veo 3, offers a similar feature, but Flow will be able to pull it off and create audio at the same time. Those added audio skills will also apply to the video editor's ability to extend clips and insert objects into existing footage, too.\nBased on the samples Google's shared, videos generated with Veo 3.1 still have an uncanny quality that seems to vary greatly depending on the prompt and subject. Even if it's missing some of the realism of OpenAI's Sora 2, though, the company's decision to try and make Veo more useful to people who actually work with video rather than a source of social media spam is a welcome move.",
              "domain": "www.engadget.com"
            },
            {
              "position": 7,
              "title": "Google rolls out its new Veo 3 video-generation model ...",
              "url": "https://techcrunch.com/2025/07/03/google-rolls-out-its-new-veo-3-video-generation-model-globally/",
              "snippet": "Google on Thursday said it has begun rolling out its Veo 3 video generation model to Gemini users in more than 159 countries.\n\nVideo generation via the new model is available only to paying subscribers of Google’s AI Pro plan and is capped at three videos per day.\n\nVeo 3, which Google showed off in May, lets users generate videos up to eight seconds long using text prompts.\n\nGoogle’s Josh Woodward has said that the company is working on adding image-to-video generation capabilities to Gemini.",
              "domain": "techcrunch.com"
            },
            {
              "position": 8,
              "title": "Google's Veo 3 Update! July 2025!",
              "url": "https://www.youtube.com/watch?v=qhReJkSRKOc&vl=en",
              "snippet": "## Murray Frost\n##### Jul 09, 2025 (0:04:15)\n✅ Build a Monetized YouTube Channel in 90 days: https://murrayfrost.com/YT-Accelerator\n\nI teach people YouTube from REAL data from over 150 clients and my own channels. Data-driven feedback and strategies. None of this guessing garbage people put on online teaching you how to do YouTube.... {ts:0} So, Google Labs just had an update and it's technically called Flow. It's on\n{ts:4} their labs.google platform and you can see by the beginning of this video that it does still need some work, but you\n{ts:9} haven't been able to do that recently with Google Labs. And now you can do it with Google's Vo3. So, there's been a\n{ts:15} couple updates here that you can see. And the first one here is using images or allowing images to talk with Google's\n{ts:20} VO3, which again still needs some work, but look, it gets maybe 60% of the way there. I think it looks pretty decent.\n{ts:29} I'm obviously not going to use it to try and convince people that it's real, but maybe you can get kind of creative with\n{ts:34} this and get people to make some really funny, strange, or just dumb things that people love. I don't know why people\n{ts:42} just love brain rot these days. Now, they're also adding the option to do this with V V3 on frames to video, which\n{ts:49} is actually kind of cool. And they... 're also allowing you to top up your subscription with the kind of a mid tier\n{ts:56} option cuz previously they had just the the starter which was about 20 bucks or so per month in the US and then it was\n{ts:63} straight up to I think 250 without the discount for the first 3 months and there's just no in between. It's just a\n{ts:70} massive jump. So they added like a kind of a mid tier there. I think a couple too. Well, I'll show you what that looks\n{ts:75} like and you can top up your credits there as well. They've also gone ahead and just added better audio coverage,\n{ts:83} which I haven't really noticed all that much to be honest. Right now, there's not a huge difference as at least a\n{ts:89} noticeable difference in my opinion from me using it. Um, they also do remove audio when miners are involved. Keep\n{ts:96} that in mind. That's why your audio isn't being generated if you have kids in the video or maybe uh even teenagers\n{ts:101} sometimes. Um, but then there's also they've mentioned they're reducing um unwanted subtitles, which is actually... {ts:107} quite nice. They've been removing the the VEO watermark as well, but now they said they've reduced the unwanted\n{ts:114} subtitles. I still get them sometimes. So, I literally in caps specify in the prompt to not include captions because\n{ts:121} otherwise if I don't, sometimes the captions still show up. And the really nice quality of life update they've made\n{ts:127} here is that when you are just starting a new project or revisiting an existing one, it doesn't reset the model that you\n{ts:135} have, or at least if it does, it resets to the VO3 fast beta audio. So, this is where you're generating audio. So now\n{ts:142} you don't accidentally have VO2 selected with no audio every time you either reload a page, start a new project, or\n{ts:150} you leave and come back and it's been reset to just its default VO2. Now it's actually its default is V3 fast beta\n{ts:157} audio. So the 20 credits per generation, the cheaper VO3 option with audio. So now you don't accidentally generate\n{ts:164} videos without audio, which has happened a lot to me and I... 've wasted thousands of credits doing that by accident. So,\n{ts:171} great update right there. Quality of life, which you don't have to waste any more credits. Now, now let's say that\n{ts:176} you don't want to spend the $124 per month, and this is for the first 3 months. Then, it goes to 150, I believe,\n{ts:183} per month. So, I'm probably going to cancel it at that point because that's just really expensive, at least using\n{ts:189} VO3. Now you have the option if you have the uh let's see which was it the pro subscription the $20 a month\n{ts:195} subscription right here Google AI pro you still get a th000 credits per month in uh Google labs but you also have the\n{ts:204} option to top up your credits so for example in here when you're creating your AI videos if you run out of credits\n{ts:211} like here you can just hit get more AI credits and you can choose how much you want to add so you don't have to spend\n{ts:216} $200 or $150 at a time you could spend an extra for 24 bucks that month just to top up your credits. This wasn... 't\n{ts:223} available in the lower plans. It was only available in the the maximum ultra tier. So, I really like the ability to\n{ts:230} do that now. So, you don't have to spend so much all at once, especially if you're not using all your credits at the\n{ts:235} end of each month, but maybe one month you're just out and you need to add more, you can do so and add some\n{ts:240} credits, which is actually quite nice. I've hit this button a little too much recently, but uh hey, I'm getting good\n{ts:247} results. So, those are all the current updates with Google's Veo3 with their labs platform. Hope to see you in the\n{ts:253} next one.",
              "domain": "www.youtube.com"
            },
            {
              "position": 9,
              "title": "Build with Veo 3, now available in the Gemini API",
              "url": "https://developers.googleblog.com/en/veo-3-now-available-gemini-api/",
              "snippet": "First unveiled at Google I/O 2025, people around the world have already generated tens of millions of high-quality videos with Veo 3 (along with some new fun and interesting video trends). It is our first video model to incorporate high-fidelity video outputs and native audio, first with text-to-video and soon with image-to-video.\n\nDevelopers are already experimenting with Veo 3, discovering how the model can help them brainstorm content, rapidly iterate, and be more efficient.\n\nVeo 3 is designed to handle a range of video generation tasks, from cinematic narratives to dynamic character animations. With Veo 3, you can create more immersive experiences by not only generating stunning visuals, but also audio like dialogue and sound effects.\n\nLet’s take a look at some examples.\n\nExplore these examples and more with Veo 3 in Google AI Studio, available as an SDK template and interactive Starter App to remix, copy and extend. The Starter App and its sample code offer a convenient way for Paid Tier users to rapidly prototype with Veo 3 and more on the Gemini API, directly from Google AI Studio.\n\nClick the Key button in the top right of the AI Studio Build interface to select a Google Cloud Project with billing enabled to use the Paid Tier in AI Studio apps. See the FAQs for more.... Veo 3 will be priced at $0.75 per second for video and audio output. Additionally, Veo 3 Fast will be available soon, offering a faster and more cost-effective option for video creation.\n\nHere’s a basic Python example to create a video:\n\n```\n\nimport time\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.0-generate-preview\",\n\nprompt=\"a close-up shot of a golden retriever playing in a field of sunflowers\",\n\nconfig=types.GenerateVideosConfig(\n\nnegative_prompt=\"barking, woofing\",\n\n),\n\n\n\n# Waiting for the video(s) to be generated\n\nwhile not operation.done:\n\ntime.sleep(20)\n\noperation = client.operations.get(operation)\n\ngenerated_video = operation.result.generated_videos[0]\n\nclient.files.download(file=generated_video.video)\n\ngenerated_video.video.save(\"veo3_video.mp4\")\n\n```\n\nAll videos generated by Veo 3 models will continue to include a digital SynthID watermark. To get started, check out the documentation, cookbook, and a Veo 3 starter app in Google AI Studio:\n\nIn addition to being available via the Gemini API in Google AI Studio, Veo 3 is also available to Google AI subscribers in the Gemini app and Flow, and to enterprise customers via Vertex AI.... 1: Fluffy Characters Stop Motion: Inside a brightly colored, cozy kitchen made of felt and yarn. Professor Nibbles, a plump, fluffy hamster with oversized glasses, nervously stirs a bubbling pot on a miniature stove, muttering, \"Just a little more... 'essence of savory,' as the recipe calls for.\" The camera is a mid-shot, capturing his frantic stirring. Suddenly, the pot emits a loud \"POP!\" followed by a comical \"whoosh\" sound, and a geyser of iridescent green slime erupts, covering the entire kitchen. Professor Nibbles shrieks, \"Oh, dear! Not again!\" and scurries away, leaving a trail of tiny, panicked squeaks.\n\n2: The sequence begins with an extreme close-up of a single gear, slowly turning and reflecting harsh sunlight. The camera gradually pulls back in a continuous movement, revealing this is but one component of a colossal, mechanical heart half-buried in a desolate, rust-colored desert. A sweeping aerial shot establishes its enormous scale and isolation in the barren landscape. The camera descends to capture pipes hissing steam and the rhythmic thumping that echoes across the empty plains. A subtle shake effect synchronizes with each massive heartbeat. A lateral tracking shot discovers tiny, robed figures scurrying across the metallic surface. The camera follows one such figure in a detailed tracking shot as they perform meticulous maintenance, polishing brass valves and tightening immense bolts. A complex movement circles the entire structure, capturing different maintenance teams working in precarious positions across its rusted exterior. The final shot begins tight on the meticulous work of one tiny figure before executing a dramatic pull-out that reveals the true scale of the heart and the minuscule size of its caretakers, tending to the vital organ of an unseen, sleeping giant that extends beyond the frame.",
              "domain": "developers.googleblog.com"
            },
            {
              "position": 10,
              "title": "Google Veo 3.1 Just Dropped — How to Use It + What’s New! ✅",
              "url": "https://www.youtube.com/watch?v=y0QZbjd2f_k",
              "snippet": "## Aivoxy\n##### Oct 15, 2025 (0:03:37)\nGoogle Veo 3.1 Just Dropped — How to Use It + What’s New! ✅\n\n🔥 For Prompts:\nhttps://t.me/aivoxxy\n\nveo 3.1\ngoogle veo 3.1\nveo3.1\nveo 3\ngoogle veo 3\nveo 3.1 release date\nveo 3.1 generated videos\ngoogle veo 3.1\n\nGoogle DeepMind just released Veo 3.1, and it’s packed with upgrades that could change the AI video game forever.\nIn this video, we demonstrate how to use Veo 3.1, walk you through the new features, and explain why this update is so exciting.\n\nSome highlights of Veo 3.1 include:\nScene Builder — plan your video shot by shot\nCharacter Consistency — no more random face changes\nNative 1080p output with cinematic presets\nMulti-prompting for multi-shot sequences\nAudio & dialogue generation built-in\n\nWe’ll also compare it with other AI video tools like Sora 2, so you know what’s really new and what’s just hype.\n\n👨‍👩‍👦‍👦 Join this channel to get access to perks:\nhttps://www.youtube.com/channel/UC4VpM2gtTxbXSiyL1ODl9uQ/join\n\n🔥 This AI Bigfoot Vlog Looks 100% Real – Made with Veo 3!\nhttps://youtu.be/_1EH25-1NkQ\n\n🔥 Google Veo 3 Just Changed Video Creation Forever! 😱🚨\nhttps://youtu.be/-_4jOwjQaQI... 🔥 Google AI Mode Just Changed Search FOREVER!\nhttps://youtu.be/JX64HQ8_QCg\n--------------------------------------------------------------------------\n🔥 Grok 3 Now Has Voice! 🔥 4 Mind-Blowing Examples You Need to See!\nhttps://youtu.be/B_pKBwj08Wg\n\n🔥 Grok 3 vs ChatGPT: Unfiltered Voice Chat? Surprising Responses!\nhttps://youtu.be/snOj_6VytiY\n\n🔥 Grok 3 vs ChatGPT vs DeepSeek vs Claude 3.5: Who's the Brainiest? 🤯\nhttps://youtu.be/XKDdUuy-hmE... 🔥 You Think It's Fake - DeepSeek vs ChatGPT\nhttps://youtu.be/SruDuJkw78U\n-------------------------------------------------------------------------\n#veo3.1 #veo3 #aivideo... {ts:0} Come closer. Let me share a secret with you.\n{ts:5} I am your new girlfriend. Google just surprised everyone. VO 3.1 is officially live today. It's faster,\n{ts:22} smarter, and now generates full cinematic clips with sound. And yeah, I've already got access. Let me show you\n{ts:30} how crazy this is. So, here's the thing. VO3.1 isn't available everywhere just yet. Right now, it's rolling out only\n{ts:39} inside Google Flow. So, if you already have a Google AI account, you can access it right from there. I'm already logged\n{ts:46} in and as you can see, the VO3.1 fast model is live. So, yeah, no fake claims here. This is real and I've been talking\n{ts:55} about this since my earlier videos. Now using it is simple. Write your prompt. Select VO3.1 fast. Hit generate. That's\n{ts:65} it. No complex settings, no extra steps. All right, let's talk about what's actually\n{ts:79} new in VO3.1. This update isn't a total overhaul. It... 's more like a smarter, cleaner upgrade to\n{ts:87} V3. Here's what's changed. One, smarter scene creation. You can now combine reference images, locations, characters,\n{ts:96} and objects, and VO automatically builds a cohesive scene around them. No more weird lighting mismatches or random jump\n{ts:104} cuts. Two, built-in sound generation. Every clip now includes matching audio, ambient sounds, dialogue, or effects\n{ts:113} that sync perfectly with the visuals. It makes your videos feel far more cinematic and alive. Three, clip\n{ts:121} extension. You can take a short clip and expand it into a full cinematic sequence. VO keeps the same lighting,\n{ts:129} motion, and audio flow seamlessly. It's perfect for trailers, ads, or storydriven projects. Four, custom start\n{ts:138} and end control. Now you can choose where a shot begins and ends, and VO fills in the middle with smooth,\n{ts:145} realistic transitions that gives you more storytelling control. Five, add or remove elements. Want to remove an\n{ts:153} object or drop in a new one? Just select it. VO automatically adjusts lighting, shadows, and scale to keep everything... {ts:161} realistic. Six, higher realism and physics accuracy. Scenes now behave like real life. Lighting looks natural,\n{ts:170} character movement feels grounded, and the visuals have that cinematic finish. You'll also notice image to video\n{ts:176} generation looks a lot cleaner now. Frames are stable, colors look balanced, and it feels much closer to real\n{ts:183} footage. That said, it still can't generate long videos yet. The maximum I've seen is around 8 seconds. So don't\n{ts:191} expect 1 minute clips for now. So yeah, not a massive reinvention, but a much smarter, smoother version of what VO\n{ts:198} already did well. I'll be testing this more and posting sideby-side comparisons soon. Stay tuned to see how it compares\n{ts:205} to VO3. If this video helped you, hit like, share it with your tech buddies, and make sure you subscribe for more AI\n{ts:212} updates like this. Thanks for watching and I'll see you in the next",
              "domain": "www.youtube.com"
            }
          ],
          "success": true,
          "error": null
        },
        {
          "query_id": "q3",
          "query": "veo3.1 software version Google 2025 official",
          "claim_id": "claim_1",
          "query_type": "source_verification",
          "priority": "high",
          "results": [
            {
              "position": 1,
              "title": "Introducing Veo 3.1 and advanced capabilities in Flow - Google Blog",
              "url": "https://blog.google/technology/ai/veo-updates-flow/",
              "snippet": "# Introducing Veo 3.1 and advanced capabilities in Flow\n\nFive months ago, we introduced Flow, our AI filmmaking tool powered by Veo, and have been inspired by the creativity it has sparked with over 275 million videos generated in Flow\n\n1\n\n. We're always listening to your feedback, and we've heard that you want more artistic control within Flow, with increased support for audio across all features.\n\nToday, we’re introducing new and enhanced creative capabilities to edit your clips, giving you more granular control over your final scene. For the first time, we’re also bringing audio to existing capabilities like “Ingredients to Video,” “Frames to Video” and “Extend.”\n\nWe’re also introducing Veo 3.1, which brings richer audio, more narrative control, and enhanced realism that captures true-to-life textures. Veo 3.1 is state-of-the-art and builds on Veo 3, with stronger prompt adherence and improved audiovisual quality when turning images into videos.\n\n## Refine your narrative with audio and more control\n\nWith Veo 3.1, we’re bringing audio to existing capabilities to help you craft the perfect scene. These features are experimental and actively improving, and we’re excited to see what you create as we iterate based on your feedback.\n\nNow, with rich, generated audio, you can:\n\n**Craft the look of your scene.**With \"Ingredients to Video,\" you can use multiple reference images to control the characters, objects and style. Flow uses your ingredients to create a final scene that looks just as you envisioned. **Control the shot from start to finish.**Provide a starting and ending image with “Frames to Video,” and Flow will generate a seamless video that bridges the two, perfect for artful and epic transitions. **Create longer, seamless shots.**With \"Extend,\" you can create longer videos, even lasting for a minute or more, that connect to and continue the action from your original clip. Each video is generated based on the final second of your previous clip, making it most useful for creating a longer establishing shot.... ## Edit your ingredients and videos with more precision\n\nGreat ideas can strike at any point in the creative process. For moments when the first take isn't the final one, we're introducing new editing capabilities directly within Flow to help you reimagine and perfect your scenes.\n\n**Add new elements to any scene.**With “Insert,” introduce anything you can imagine, from realistic details to fantastical creatures. Flow now handles complex details like shadows and scene lighting, making the addition look natural. **Remove unwanted objects or characters seamlessly.**Soon, you’ll be able to take anything out of a scene, and Flow will reconstruct the background and surroundings, making it look as though the object was never there.\n\n## Start creating in Flow today\n\nWith more precise editing capabilities, audio across all existing features and higher-quality outputs powered by Veo 3.1, we're opening up new possibilities for richer, more powerful video storytelling right inside Flow.\n\nThe Veo 3.1 model is also available via the Gemini API for developers, Vertex AI for enterprise customers, and the Gemini app. New capabilities are available in both Gemini API\n\n2\n\nand Vertex AI\n\n3",
              "domain": "blog.google"
            },
            {
              "position": 2,
              "title": "Build with Veo 3, now available in the Gemini API",
              "url": "https://developers.googleblog.com/en/veo-3-now-available-gemini-api/",
              "snippet": "First unveiled at Google I/O 2025, people around the world have already generated tens of millions of high-quality videos with Veo 3 (along with some new fun and interesting video trends). It is our first video model to incorporate high-fidelity video outputs and native audio, first with text-to-video and soon with image-to-video.\n\nDevelopers are already experimenting with Veo 3, discovering how the model can help them brainstorm content, rapidly iterate, and be more efficient.\n\nVeo 3 is designed to handle a range of video generation tasks, from cinematic narratives to dynamic character animations. With Veo 3, you can create more immersive experiences by not only generating stunning visuals, but also audio like dialogue and sound effects.\n\nLet’s take a look at some examples.\n\nExplore these examples and more with Veo 3 in Google AI Studio, available as an SDK template and interactive Starter App to remix, copy and extend. The Starter App and its sample code offer a convenient way for Paid Tier users to rapidly prototype with Veo 3 and more on the Gemini API, directly from Google AI Studio.\n\nClick the Key button in the top right of the AI Studio Build interface to select a Google Cloud Project with billing enabled to use the Paid Tier in AI Studio apps. See the FAQs for more.... Veo 3 will be priced at $0.75 per second for video and audio output. Additionally, Veo 3 Fast will be available soon, offering a faster and more cost-effective option for video creation.\n\nHere’s a basic Python example to create a video:\n\n```\n\nimport time\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.0-generate-preview\",\n\nprompt=\"a close-up shot of a golden retriever playing in a field of sunflowers\",\n\nconfig=types.GenerateVideosConfig(\n\nnegative_prompt=\"barking, woofing\",\n\n),\n\n\n\n# Waiting for the video(s) to be generated\n\nwhile not operation.done:\n\ntime.sleep(20)\n\noperation = client.operations.get(operation)\n\ngenerated_video = operation.result.generated_videos[0]\n\nclient.files.download(file=generated_video.video)\n\ngenerated_video.video.save(\"veo3_video.mp4\")\n\n```\n\nAll videos generated by Veo 3 models will continue to include a digital SynthID watermark. To get started, check out the documentation, cookbook, and a Veo 3 starter app in Google AI Studio:\n\nIn addition to being available via the Gemini API in Google AI Studio, Veo 3 is also available to Google AI subscribers in the Gemini app and Flow, and to enterprise customers via Vertex AI.... 1: Fluffy Characters Stop Motion: Inside a brightly colored, cozy kitchen made of felt and yarn. Professor Nibbles, a plump, fluffy hamster with oversized glasses, nervously stirs a bubbling pot on a miniature stove, muttering, \"Just a little more... 'essence of savory,' as the recipe calls for.\" The camera is a mid-shot, capturing his frantic stirring. Suddenly, the pot emits a loud \"POP!\" followed by a comical \"whoosh\" sound, and a geyser of iridescent green slime erupts, covering the entire kitchen. Professor Nibbles shrieks, \"Oh, dear! Not again!\" and scurries away, leaving a trail of tiny, panicked squeaks.\n\n2: The sequence begins with an extreme close-up of a single gear, slowly turning and reflecting harsh sunlight. The camera gradually pulls back in a continuous movement, revealing this is but one component of a colossal, mechanical heart half-buried in a desolate, rust-colored desert. A sweeping aerial shot establishes its enormous scale and isolation in the barren landscape. The camera descends to capture pipes hissing steam and the rhythmic thumping that echoes across the empty plains. A subtle shake effect synchronizes with each massive heartbeat. A lateral tracking shot discovers tiny, robed figures scurrying across the metallic surface. The camera follows one such figure in a detailed tracking shot as they perform meticulous maintenance, polishing brass valves and tightening immense bolts. A complex movement circles the entire structure, capturing different maintenance teams working in precarious positions across its rusted exterior. The final shot begins tight on the meticulous work of one tiny figure before executing a dramatic pull-out that reveals the true scale of the heart and the minuscule size of its caretakers, tending to the vital organ of an unseen, sleeping giant that extends beyond the frame.",
              "domain": "developers.googleblog.com"
            },
            {
              "position": 3,
              "title": "Google announces Veo 3.1 and updates Flow with more controls, tools",
              "url": "https://9to5google.com/2025/10/15/veo-3-1/",
              "snippet": "Google today announced Veo 3.1 as its latest video generation model, with Flow getting a number of updates to take advantage of the latest capabilities.\n\nCompared to Veo 3, which was announced at I/O 2025 in May, this new version offers richer audio and “enhanced realism that captures true-to-life textures.” Veo 3.1 has a deeper understanding of storytelling, cinematic styles, and character interactions to give you more narrative control. The image-to-video capability benefits from improved audio-visual quality and better follows your prompt.\n\nVeo 3.1 and Veo 3.1 Fast are available in the Gemini app, as well as the Gemini API and Vertex AI, to power text-to-video and image-to-video for horizontal (16×9) and vertical (9×16) outputs.\n\nMeanwhile, Google is updating the Flow filmmaking tool to take advantage of Veo 3.1. Audio generation is coming to:\n\n**Ingredients to Video**: This can include images of multiple characters, objects, and styles **Scene extension**: You can Extend an existing clip, with Google basing it on the last second **Frames to Video**: This lets you specify the starting and ending images, with Flow bridging the two\n\nFlow is getting new editing capabilities. You can insert elements like objects, characters, and details, with Google handling shadows, scene lighting, and other complex details to make everything look natural.\n\nComing soon is the ability to remove objects and characters from a scene, with Flow working to reconstruct the background and surroundings to make the edit seamless.\n\n\n\n## Comments",
              "domain": "9to5google.com"
            },
            {
              "position": 4,
              "title": "Introducing Veo 3.1 and new creative capabilities in the Gemini API",
              "url": "https://developers.googleblog.com/en/introducing-veo-3-1-and-new-creative-capabilities-in-the-gemini-api/",
              "snippet": "**Today, we are releasing Veo 3.1 and Veo 3.1 Fast in paid preview in the** **Gemini API** **.** This updated model offers several improvements, as well as improved outputs when generating video from images. These new models are available via the Gemini API in Google AI Studio and Vertex AI. Veo 3.1 is also available in the Gemini app and Flow.\n\nVeo 3.1 and Veo 3.1 Fast empower developers to create more engaging content through significant upgrades. The models now generate\n\n**richer native audio**, from natural conversations to synchronized sound effects, and offer **greater narrative control** with an improved understanding of cinematic styles. **Enhanced image-to-video** capabilities ensure better prompt adherence while delivering superior audio and visual quality and maintaining character consistency across multiple scenes.\n\nIn addition to the model update, we are introducing new model capabilities, including using reference images to guide video generation, extend existing Veo videos, and generate transitions between a first and last frame.\n\nYou can now guide the generation process by providing up to 3 reference images of a character, object, or scene. This is helpful for maintaining character consistency across multiple shots or applying a specific style to your video.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nconfig=types.GenerateVideosConfig(\n\nreference_images=[reference_image1, reference_image2],\n\n),\n\n\n\n```... Your story is no longer limited by the original generation. With Scene extension, you can create longer videos, even lasting for a minute or more, by generating new clips that connect to your previous video. Each new video is generated based on the final second of the previous clip. This maintains visual continuity, making the feature ideal for extending shots with background audio.\n\n```\n\nfrom google import genai\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nvideo=video_to_extend\n\n\n\n```\n\nCreate smooth, natural scenes that bridge two different images. By providing a starting and an ending image, you can direct Veo 3.1 to generate the transition between them, complete with accompanying audio.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nimage=first_frame,\n\nconfig=types.GenerateVideosConfig(\n\nlast_frame=last_frame,\n\n),\n\n\n\n```\n\n**What people are building with Veo 3.1**\n\nPromise Studios, a GenAI movie studio, uses Veo 3.1 within its MUSE Platform to enhance generative storyboarding and previsualization for director-driven storytelling at production quality.... Latitude is experimenting with Veo 3.1 in its generative narrative engine to instantly bring user-created stories to life.\n\nVeo 3.1 and these new capabilities are now available in paid preview via the Gemini API.\n\nVeo 3.1 is the same price as Veo 3. We are incredibly excited to see what you will build with these new capabilities!",
              "domain": "developers.googleblog.com"
            },
            {
              "position": 5,
              "title": "Google's Veo 3.1 is better at generating videos from images",
              "url": "https://www.engadget.com/ai/googles-veo-31-is-better-at-generating-videos-from-images-220829129.html",
              "snippet": "Google has released a new update to its Veo AI video generation model that should make it do a better job of sticking to prompts and converting images into videos. Veo 3.1 is available to try today through Google's Gemini API and is now also powering the company's Flow video editor.\nVeo 3.1 builds on the new capabilities Google introduced with launch of Veo 3 at Google I/O 2025. The new model offers better \"prompt adherence,\" according to Google, and should have an easier time creating videos based on the image \"ingredients\" you upload alongside your written prompt. Veo 3.1 also makes it possible to convert images to video and generate audio at the same time, a capability that wasn't available with Veo 3.\nIn Flow, Veo 3.1 supports at least a new feature that gives you finer control over the videos you generate. With what Google calls \"Frame to Video,\" Flow lets you upload a first and last frame, and then generates the video in-between. Adobe Firefly, which is powered by Veo 3, offers a similar feature, but Flow will be able to pull it off and create audio at the same time. Those added audio skills will also apply to the video editor's ability to extend clips and insert objects into existing footage, too.\nBased on the samples Google's shared, videos generated with Veo 3.1 still have an uncanny quality that seems to vary greatly depending on the prompt and subject. Even if it's missing some of the realism of OpenAI's Sora 2, though, the company's decision to try and make Veo more useful to people who actually work with video rather than a source of social media spam is a welcome move.",
              "domain": "www.engadget.com"
            },
            {
              "position": 6,
              "title": "Veo 3 Fast available for everyone on Vertex AI | Google Cloud Blog",
              "url": "https://cloud.google.com/blog/products/ai-machine-learning/veo-3-fast-available-for-everyone-on-vertex-ai",
              "snippet": "# Veo 3 and Veo 3 Fast are now generally available on Vertex AI\n\n##### Jason Gelman\n\nDirector, Product Management, Vertex AI... ##### Try Gemini 2.5\n\nOur most intelligent model is now available on Vertex AITry now\n\nVeo 3 has seen massive global adoption with over 70 million videos created since May, and we've seen tremendous momentum with our enterprise customers as well. Since its preview launch on Vertex AI in June, enterprise customers have already generated over 6 million videos, showcasing the incredible demand for professional-grade, scalable AI video creation.\n\nToday, we’re building on this momentum with some exciting updates to Veo on Vertex AI.\n\n\n\n**Veo 3,**our most advanced video generation model, is now generally available to everyone on Vertex AI.\n\n\n\n**Veo 3 Fast**, a model designed for speed and rapid iteration, is now generally available for everyone on Vertex AI. It's a faster way to turn text to video, from narrated product demos to short films.\n\n\n\nComing to public preview on Vertex AI in August, Veo 3 and Veo 3 Fast will also offer\n\n**image-to-video capabilities**to make it possible for you to bring static visuals and images to life. All you have to do is provide the source image along with a text prompt that describes what kind of video you want to create.\n\n**How businesses are building with Veo 3 on Vertex AI**... Google Cloud customers around the world are using Veo 3 and Veo 3 Fast on Vertex AI to create professional-quality video content with unparalleled efficiency and creative freedom. Let’s look at some examples.\n\n**Canva**\n\n“Enabling anyone to bring their ideas to life – especially their most creative ones – has been core to Canva's mission ever since we set out to empower the world to design. By democratising access to a powerful technology like Google’s Veo 3 inside Canva AI, your big ideas can now be brought to life in the highest quality video and sound, all from within your existing Canva subscription. In true Canva fashion, we’ve built this with an intuitive interface and simple editing tools in place, all backed by Canva Shield.” –\n\n**Cameron Adams, co-founder and Chief Product Officer, Canva **\n\nBut the momentum extends beyond design. The team at\n\n**BarkleyOKRP**, a leading ad agency, is using Veo 3 to speed up video production timelines.\n\n“The rapid advancements from Veo 2 to Veo 3 within such a short time frame on this project have been nothing short of remarkable. Our team undertook the task of re-creating numerous music videos initially produced with Veo 2 once Veo 3 was released, primarily due to the significantly improved synchronization between voice and mouth movements. The continuous daily progress we are witnessing is truly extraordinary.” –... **Julie Ray Barr, Senior Vice President Client Experience, BarkleyOKRP**\n\nAt global investing platform\n\n**eToro**, the team is making marketing iterations a breeze with Veo 3.\n\n“At eToro, innovation is in our DNA. As a global investing platform serving clients in 75 countries, local storytelling isn’t optional - it’s essential. With Veo 3, we produced 15 fully AI‑generated versions of our ad, each in the native language of its market, all while capturing real emotion at scale. Ironically, AI didn’t reduce humanity - it amplified it. Veo 3 lets us tell more stories, in more tongues, with more impact.” –\n\n**Shay Chikotay, Head of Creative & Content, eToro**\n\n**Razorfish**, an interactive agency and part of the Publicis Groupe, is using Veo to bring creative to life.\n\n\"For The Morelandos, our campaign with Visit Orlando and Google, we used the full Vertex AI stack—Gemini to mine real reviews, Imagen to bring the characters to life, and Veo to give them motion. Veo let us go from story to near-cinematic video in a fraction of the usual time—which meant more room to explore, iterate, and push the idea further.\" –... **Anthony Yell, Chief Creative Officer, Razorfish**\n\n**Synthesia** **, **a leading synthetic media generation company, is using Veo to contextually adapt visuals to its hyper-realistic AI avatars and voices.\n\n“Veo 3 represents a leap forward in generative AI, and its integration into Synthesia’s platform will redefine how businesses create video content. By combining our hyper-realistic AI avatars and voices with Veo-powered fully contextual visuals that adapt to each unique story, we’re giving enterprise teams the creative power to communicate with unrivalled clarity and impact.” –\n\n**Bill Leaver, Product Manager, Synthesia **\n\n**How enterprises can use Veo 3 Fast for speed and creativity **\n\n**Veo 3 Fast** is a great fit for work that requires rapid iteration and speed. It has an ideal balance between processing time and high-quality visual output, making it especially helpful for:\n\n\n\nQuickly generating and testing variations of ad concepts to respond to market trends.\n\n\n\nEfficiently creating video demonstrations for entire product catalogs from still images.\n\n\n\nDeveloping engaging animated explainers and training modules in less time.\n\n**Veo 3 and Veo 3 Fast on Vertex AI mean even more capabilities for enterprise storytelling**\n\nVeo 3 and Veo 3 Fast are designed to give creators the control and quality needed to move beyond short clips and produce complete, compelling narratives. Here are some of the core features now generally available on Vertex AI.... **Create scenes with native audio:**Veo 3 generates video and audio in a single step. This means you can create scenes with characters that speak with accurate lip-syncing, and sound effects that fit the mood. **Deliver professional quality at enterprise scale:**Veo 3 produces high-definition (1080p) video, suitable for professional marketing campaigns, product demonstrations, and internal communications. You can create content that meets brand standards, saving time and money. **Simplify content localization for global audiences:**Veo 3’s native dialogue generation helps businesses connect with an international audience by producing a video once and localizing the dialogue for dozens of languages. **Image-to-video (coming to public preview on Vertex AI in August):**Veo 3 and Veo 3 Fast can also take a single image, which can be a photo you uploaded or an AI-generated image, and animate it, creating an 8-second video clip. This feature is particularly powerful for content creators, marketers, and businesses looking to animate existing visual assets, create engaging social media content, or generate compelling product demonstrations from high-quality images.\n\n### Enterprise-grade safety and security\n\nVeo 3 and Veo 3 Fast on Vertex AI are built for scalable and responsible enterprise use. We embed digital watermarks into every frame with SynthID, helping combat misinformation and misattribution. Veo 3 and Veo 3 Fast are also covered by our indemnity for generative AI services.... ### Get started with Veo 3 and Veo 3 Fast today\n\nTo get started, go here to learn more about Veo 3 and Veo 3 Fast on Vertex AI, and try it on Vertex AI Media Studio.",
              "domain": "cloud.google.com"
            },
            {
              "position": 7,
              "title": "Generate videos with Veo 3.1 in Gemini API",
              "url": "https://ai.google.dev/gemini-api/docs/video",
              "snippet": "Veo 3.1 is Google's state-of-the-art model for generating high-fidelity, 8-second 720p or 1080p videos featuring stunning realism and natively generated audio. You can access this model programmatically using the Gemini API. To learn more about the available Veo model variants, see the Model Versions section.\n\nVeo 3.1 excels at a wide range of visual and cinematic styles and introduces several new capabilities:\n\n**Video extension**: Extend videos that were previously generated using Veo. **Frame-specific generation**: Generate a video by specifying the first and last frames. **Image-based direction**: Use up to three reference images to guide the content of your generated video.\n\nFor more information about writing effective text prompts for video generation, see the Veo prompt guide... ### Go\n\n```\n\npackage main\n\nimport (\n\n\"context\"\n\n\"log\"\n\n\"os\"\n\n\"time\"\n\n\"google.golang.org/genai\"\n\n\n\nfunc main() {\n\nctx := context.Background()\n\nclient, err := genai.NewClient(ctx, nil)\n\nif err != nil {\n\nlog.Fatal(err)\n\n\n\nprompt := `A close up of two people staring at a cryptic drawing on a wall, torchlight flickering.\n\nA man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'`\n\noperation, _ := client.Models.GenerateVideos(\n\nctx,\n\n\"veo-3.1-generate-preview\",\n\nprompt,\n\nnil,\n\nnil,\n\n\n\n// Poll the operation status until the video is ready.\n\nfor !operation.Done {\n\nlog.Println(\"Waiting for video generation to complete...\")\n\ntime.Sleep(10 * time.Second)\n\noperation, _ = client.Operations.GetVideosOperation(ctx, operation, nil)\n\n\n\n// Download the generated video.\n\nvideo := operation.Response.GeneratedVideos[0]\n\nclient.Files.Download(ctx, video.Video, nil)\n\nfname := \"dialogue_example.mp4\"\n\n_ = os.WriteFile(fname, video.Video.VideoBytes, 0644)\n\nlog.Printf(\"Generated video saved to %s\\n\", fname)\n\n\n\n```... ### JavaScript\n\n```\n\nimport { GoogleGenAI } from \"@google/genai\";\n\nconst ai = new GoogleGenAI({});\n\nconst prompt = \"Panning wide shot of a calico kitten sleeping in the sunshine\";\n\n// Step 1: Generate an image with Nano Banana.\n\nconst imageResponse = await ai.models.generateContent({\n\nmodel: \"gemini-2.5-flash-image\",\n\nprompt: prompt,\n\n});\n\n// Step 2: Generate video with Veo 3.1 using the image.\n\nlet operation = await ai.models.generateVideos({\n\nmodel: \"veo-3.1-generate-preview\",\n\nprompt: prompt,\n\nimage: {\n\nimageBytes: imageResponse.generatedImages[0].image.imageBytes,\n\nmimeType: \"image/png\",\n\n},\n\n});\n\n// Poll the operation status until the video is ready.\n\nwhile (!operation.done) {\n\nconsole.log(\"Waiting for video generation to complete...\")\n\nawait new Promise((resolve) => setTimeout(resolve, 10000));\n\noperation = await ai.operations.getVideosOperation({\n\noperation: operation,\n\n});\n\n\n\n// Download the video.\n\nai.files.download({\n\nfile: operation.response.generatedVideos[0].video,\n\ndownloadPath: \"veo3_with_image_input.mp4\",\n\n});\n\nconsole.log(`Generated video saved to veo3_with_image_input.mp4`);\n\n```... ### Go\n\n```\n\npackage main\n\nimport (\n\n\"context\"\n\n\"log\"\n\n\"os\"\n\n\"time\"\n\n\"google.golang.org/genai\"\n\n\n\nfunc main() {\n\nctx := context.Background()\n\nclient, err := genai.NewClient(ctx, nil)\n\nif err != nil {\n\nlog.Fatal(err)\n\n\n\nprompt := \"Panning wide shot of a calico kitten sleeping in the sunshine\"\n\n// Step 1: Generate an image with Nano Banana.\n\nimageResponse, err := client.Models.GenerateContent(\n\nctx,\n\n\"gemini-2.5-flash-image\",\n\nprompt,\n\nnil, // GenerateImagesConfig\n\n\n\nif err != nil {\n\nlog.Fatal(err)\n\n\n\n// Step 2: Generate video with Veo 3.1 using the image.\n\noperation, err := client.Models.GenerateVideos(\n\nctx,\n\n\"veo-3.1-generate-preview\",\n\nprompt,\n\nimageResponse.GeneratedImages[0].Image,\n\nnil, // GenerateVideosConfig\n\n\n\nif err != nil {\n\nlog.Fatal(err)\n\n\n\n// Poll the operation status until the video is ready.\n\nfor !operation.Done {\n\nlog.Println(\"Waiting for video generation to complete...\")\n\ntime.Sleep(10 * time.Second)\n\noperation, _ = client.Operations.GetVideosOperation(ctx, operation, nil)\n\n\n\n// Download the video.\n\nvideo := operation.Response.GeneratedVideos[0]\n\nclient.Files.Download(ctx, video.Video, nil)\n\nfname := \"veo3_with_image_input.mp4\"\n\n_ = os.WriteFile(fname, video.Video.VideoBytes, 0644)\n\nlog.Printf(\"Generated video saved to %s\\n\", fname)\n\n\n\n```... ## Extending Veo videos\n\nUse Veo 3.1 to extend videos that you previously generated with Veo by 7 seconds and up to 20 times.\n\nInput video limitations:\n\n- Veo-generated videos only up to 141 seconds long.\n\n- Gemini API only supports video extensions for Veo-generated videos.\n\n- Input videos are expected to have a certain length, aspect ratio, and dimensions:\n\n- Aspect ratio: 9:16 or 16:9\n\n- Resolution: 720p\n\n- Video length: 141 seconds or less\n\nThe output of the extension is a single video combining the user input video and the generated extended video for up to 148 seconds of video.\n\nThis example takes the Veo-generated video\n\n*butterfly_video*, shown here with\n\nits original prompt, and extends it using the\n\n`video` parameter and a new\n\nprompt:\n\n|Prompt|Output: `butterfly_video`|\n|--|--|\n|An origami butterfly flaps its wings and flies out of the french doors into the garden.|\n### Python\n\n```\n\nimport time\n\nfrom google import genai\n\nclient = genai.Client()\n\nprompt = \"Track the butterfly into the garden as it lands on an orange origami flower. A fluffy white puppy runs up and gently pats the flower.\"\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nvideo=butterfly_video,\n\nprompt=prompt,\n\nconfig=types.GenerateVideosConfig(\n\nnumber_of_videos=1,\n\nresolution=\"720p\"\n\n),\n\n\n\n# Poll the operation status until the video is ready.\n\nwhile not operation.done:\n\nprint(\"Waiting for video generation to complete...\")\n\ntime.sleep(10)\n\noperation = client.operations.get(operation)\n\n# Download the video.\n\nvideo = operation.response.generated_videos[0]\n\nclient.files.download(file=video.video)\n\nvideo.video.save(\"veo3.1_extension.mp4\")\n\nprint(\"Generated video saved to veo3.1_extension.mp4\")\n\n```\n\nFor information about writing effective text prompts for video generation, see the Veo prompt guide.... ## Model features\n\n|Feature|Description|Veo 3.1 & Veo 3.1 Fast|Veo 3 & Veo 3 Fast|Veo 2|\n|--|--|--|--|--|\n|Audio|Natively generates audio with video.|Natively generates audio with video.|✔️ Always on|❌ Silent only|\n|Input Modalities|The type of input used for generation.|Text-to-Video, Image-to-Video, Video-to-Video|Text-to-Video, Image-to-Video|Text-to-Video, Image-to-Video|\n|Resolution|The output resolution of the video.|720p & 1080p (8s length only) 720p only when using video extension.|720p & 1080p (16:9 only)|720p|\n|Frame Rate|The output frame rate of the video.|24fps|24fps|24fps|\n|Video Duration|Length of the generated video.|8 seconds, 6 seconds, 4 seconds 8 seconds only when using reference images|8 seconds|5-8 seconds|\n|Videos per Request|Number of videos generated per request.|1|1|1 or 2|\n|Status & Details|Model availability and further details.|Preview|Stable|Stable|... ## Model versions\n\nCheck out the Pricing and Rate limits pages for more Veo model-specific usage details.\n\n### Veo 3.1 Preview\n\n|Property|Description|\n|--|--|\n|Model code||\n|Supported data types|Text, Image Video with audio|\n|Limits|1,024 tokens 1|\n|Latest update|September 2025|\n### Veo 3.1 Fast Preview\n\n|Property|Description|\n|--|--|\n|Model code||\n|Supported data types|Text, Image Video with audio|\n|Limits|1,024 tokens 1|\n|Latest update|September 2025|\n### Veo 3\n\n|Property|Description|\n|--|--|\n|Model code||\n|Supported data types|Text, Image Video with audio|\n|Limits|1,024 tokens 1|\n|Latest update|July 2025|\n### Veo 3 FastVeo 3 Fast allows developers to create videos with sound while maintaining high quality and optimizing for speed and business use cases. It's ideal for backend services that programmatically generate ads, tools for rapid A/B testing of creative concepts, or apps that need to quickly produce social media content.\n\n|Property|Description|\n|--|--|\n|Model code||\n|Supported data types|Text, Image Video with audio|\n|Limits|1,024 tokens 1|\n|Latest update|July 2025|\n### Veo 2\n\n|Property|Description|\n|--|--|\n|Model code||\n|Supported data types|Text, image Video|\n|Limits|N/A Any image resolution and aspect ratio up to 20MB file size Up to 2|\n|Latest update|April 2025|",
              "domain": "ai.google.dev"
            },
            {
              "position": 8,
              "title": "Google releases Veo 3.1, adds it to Flow video editor",
              "url": "https://techcrunch.com/2025/10/15/google-releases-veo-3-1-adds-it-to-flow-video-editor/",
              "snippet": "In Brief\nGoogle launched its new video model Veo 3.1 with improved audio output, granular editing controls, and better output for image to video. It said that Veo 3.1 builds on May's Veo 3 release and generates more realistic clips and adheres to prompts better.\nThe model allows users to add an object to the video and have it blend into the clip's style, Google said. Soon, users will be able to remove an existing object from the video in Flow, too.\nVeo 3 already has edit features such as adding reference images to drive a character, providing the first and last frame to generate a clip using AI, and the ability to extend an existing video based on the last few frames. With Veo 3.1, Google is adding audio to all these features to make the clips more lively.\nThe company is rolling out the model to its video editor Flow, the Gemini App, along with Vertex and Gemini APIs. It said that since Flow's launch in May, users have created more than 275 million videos on the app.",
              "domain": "techcrunch.com"
            },
            {
              "position": 9,
              "title": "Ultimate VEO 3.1 Update EXPLAINED: How To Use Google Veo-3 For Beginners",
              "url": "https://www.youtube.com/watch?v=FMdIIBByNZo",
              "snippet": "## AI Master\n##### Jul 09, 2025 (0:19:09)\n🚀 Become an AI Master – All-in-one AI Video Learning https://aimaster.me/pro\nLearn more about Magic Patterns here: https://magicpatterns.1stcollab.com/iamaimaster_3\n\nIn this video I ditch the press-release fluff and put Google’s brand-new Veo 3.1 through its paces, showing you exactly how to turn nothing but text prompts into crisp 8-second clips with native sound. You’ll watch me bounce between Gemini and the pro-grade Flow interface, learn my Prompt-Director Formula for nailing subject, action, context, motion, style, framing & audio, and see three crowd-pleasing demos in action. I’ll also flag the sneaky differences between Veo 3.1 Fast  and full Veo 3.1 Quality, share quick wins like borrowing Midjourney stills for style reference, and spotlight.\n\nChapters:\n0:00 – Why Veo 3 matters\n0:41 Flow vs Gemini\n2:11 Example #1\n4:23 The VEO3 Prompt Formula\n11:25 Example #2\n15:26 Example #3\n17:38 Wrapping up... {ts:0} Hey guys, I need a photo of AI master. Do you know who is AI master?\n{ts:5} Make Death Star great again. Okay. Oh, today I'm pulling back the curtain on\n{ts:10} Google's new VO3, the next gen AI video model that's got me acting like a kid in a tech toy store. We're talking text to\n{ts:18} video. You type a description and VO3 spits out an 8second video clip with sound. Yes, audio and video from just\n{ts:26} your words. [Music] In this tutorial, I'll show you how to\n{ts:31} harness VO3 to create funny viral style videos like this. It's just a few toasters, right? What's\n{ts:38} the worst that could happen? The big deal here is native audio. Unlike earlier models, VO3 doesn't just\n{ts:46} create silent clips. It also produces sound effects, background noise, even character dialogue to match. Each clip\n{ts:52} is limited to about 8 seconds for now, so we're talking short form content. They left behind a a ball today. It\n{ts:60} bounced higher than I can jump. What manner of magic is that? Now, Google gives us two main ways to... {ts:67} use V3. Flow and Gemini. Think of them as two different cameras that use the same film. Google Flow is like a pro\n{ts:75} filmmaking tool built around V3. It's a dedicated interface currently on Google apps where you can storyboard scenes,\n{ts:82} manage assets, tweak camera moves, the works. Flow was literally customd designed for VO. Lets you do fancy\n{ts:89} things. Upload or generate ingredients like character images via image genen and reuse them in multiple shots for\n{ts:96} consistency. Control camera angles and motion paths manually and even uses scene builder to extend the clips\n{ts:102} seamlessly into the next part of the story. It's basically an AI video studio for filmmakers. Now, Flow isn't open to\n{ts:109} everyone by default. It's available if you have a Google AI Pro or Ultra subscription in certain regions. Pro\n{ts:116} subscribers get most flow features, but only the ultra tier unlocks V3 fully. If you are on Pro, you have something\n{ts:123} called VO3 fast speed optimized version with sound slightly lower quality. I'll talk about that in a sec.\n{ts:132} All right, this idea cracks me up. A grandma skydiving into the Super Bowl. Talk about a stun granny. Let... {ts:276} I always try to include. Subject, who or what is in the scene, action, what the subject is doing. Context, setting,\n{ts:284} where and when is this happening. Motion, camera and movement, how the scene is shot and moves. Style, the\n{ts:291} visual style or genre. framing, composition, how the shot is framed, and constraints, extras, any additional\n{ts:298} details or instructions. And don't forget audio. VO3 will generate sound to match your scene, but only if you tell\n{ts:305} it what you want to hear. So, I often append an audio double colon section in the prompt describing sound effects,\n{ts:312} background noise, or dialogue lines. For example, audio double colon crowd cheering, upbeat rock music playing\n{ts:319} faintly, or character says hello world. You can specify dialogue explicitly, word for word, or implicitly, like a man\n{ts:327} introduces himself and let the AI make up the words. If you ask for dialogue, add no subtitles to your prompt.\n{ts:334} Otherwise, the model might plaster autogenerated subtitles on the video, which look well, not great. We... 'll see\n{ts:340} that in action soon. A boring prompt like a man answers a phone might yield a bland clip. But a richly detailed prompt\n{ts:348} can produce something cinematic. For instance, check this out. Basic prompt. A man answers a rotary phone.\n{ts:356} Hello. And detailed prompt. A shaky dolly zoom goes from a faraway\n{ts:364} blur to a close-up cinematic shot of a desperate man in a weathered green trench coat as he picks up a rotary\n{ts:370} phone mounted on a gritty brick wall. Bathed in the eerie glow of a green neon sign. The zoom reveals the tension on\n{ts:377} his face as he struggles to speak. shallow depth of field keeps focus on his furrowed brow and the phone, while\n{ts:384} the background is a blur of neon colors and shadows, creating a sense of urgency and isolation.\n{ts:392} I I have to Which one would you rather watch? The\n{ts:400} second prompt, nail subject, action, context, motion, that dolly zoom, style, cinematic, neon, noir, vibes, framing,\n{ts:408} close-up, ambience, eerie, neon, glow, all the elements. VO3 will follow the detailed prompt much more faithfully... {ts:416} than a vague one, yielding a dramatic video instead of a random guy on a phone. Bottom line, be specific and\n{ts:423} vivid. Describe what the camera sees and what the mic hears. Now, coming up with such detailed prompt can feel like\n{ts:429} flexing a new muscle. Here's where I cheat a little. I use Chad GBT to help refine prompt. Sometimes I'll start with\n{ts:436} a simple idea, say grandma's skydiving into stadium, and ask Chad GBT to describe the scene in a funny cinematic\n{ts:445} way with sights and sounds. The AI will usually output a nicely embellished description that I can copy or tweak for\n{ts:452} VO. Similarly, I use Smidjourney for visual prototyping. I'll feed a quick version of my idea to Midjourney to get\n{ts:459} a still image that helps me visualize the color palette or style I want. Maybe I like how it looks as a 1980s cartoon\n{ts:467} versus a gritty realistic photo and I can then describe that style in my VO prompt. This extra step isn't required,\n{ts:476} but if you... {ts:509} cheering. stadium announcer muffled wind rushing and the grandma's voice yelling yahoo in excitement no subtitles\n{ts:517} and that covers subject grandma action skydiving context super stadium motion wide follow shot slow-mo style cinematic\n{ts:527} bright realistic and audio crowd wind voice all in one prompt paragraph it's lengthy but should guide V3 perfectly a\n{ts:536} moment of truth let's generate this and see What happens? Yahoo!\n{ts:545} [Applause] Wow, look at that. She's really skydiving into the stadium. The video\n{ts:554} shows this tiny parachute coming down over a huge football field just like we asked. There's a wide shot of the\n{ts:561} stadium with fans standing up. I can see the crowd detail and yes, I hear the cheering roar. The grandma is visible\n{ts:567} hanging from the parachute. Maybe not super close on her face because we chose a wide shot, but enough to tell she's an\n{ts:574} elderly lady in gear. B3 as well as other generators are undoubtedly great, but they all can do\n{ts:582} one thing. Generate complete visual project. Sure, they can do few pictures and a few seconds of video, but what... 't resist add an explosion or two like a car blowing up\n{ts:998} behind them. Audio obviously loud audio double colin roaring T-Rex cowboy whooping car tires screeching an\n{ts:1008} explosion sound and dramatic action music. I will format it as one prompt and run it through Gemini. And here it\n{ts:1016} comes. We've got a T-Rex barreling down what looks like Time Square. A cowboy on his\n{ts:1029} back waving his head. And yes, there is an actual explosion behind them. Looks like a car got tossed. The fact that VO3",
              "domain": "www.youtube.com"
            },
            {
              "position": 10,
              "title": "Gemini AI video generator powered by Veo 3.1",
              "url": "https://gemini.google/overview/video-generation/",
              "snippet": "# Break the\n\n**silence** with Veo 3.1\n\nCreate high-quality, 8-second videos with Veo 3.1, our latest AI video generator. Simply describe what you have in mind or upload a photo and watch your ideas come to life with native audio generation. Try it with a Google AI Pro plan or get the highest access with the Ultra plan.\n\n**Veo 3.1** speaks for itself\n\n## Dream it. Describe it.\n\n**Done.**\n\n## For Exploring\n\nPlay with diverse styles, bring animated characters to life, and combine objects in ways you never thought possible. See what you can create using text to video with AI.\n\n## For Sharing\n\nCreate funny memes, turn inside jokes into videos, re-imagine special moments, and add a personal touch to make someone smile.\n\n## For Brainstorming\n\nBreak through creative blocks and visualize your ideas in a flash. From product concepts and designs to rapid prototyping and storytelling, Gemini can help.\n\n## Learn more about our\n\n**Veo Models**\n\nCreate videos with sound using our video generation model that maintains high-quality while optimizing for speed.\n\nCreate high-quality, 8-second videos with sound using our state-of-the-art video generation model.... ## Frequently asked questions\n\nYes, you can create and share videos in your mobile Gemini app. To create videos, tap the \"video\" button in your prompt bar. If you don't see it, tap the button with three dots to view more options.\n\nTry Veo 3.1 Fast with a Google AI Pro plan or get the highest access to Veo 3.1 in Google AI Ultra. Country availability here.\n\nFor now, the ability to generate a video from a photo is not available in the European Economic Area, Switzerland, or the United Kingdom.\n\nWe’ve taken several important safety steps to make AI video generation a safe experience. This includes extensive red teaming and evaluation aimed at preventing the generation of content that violates our policies. Additionally, all videos generated with Veo in the Gemini app are marked with a visible watermark and SynthID, a digital watermark embedded in each frame, which indicates the videos are AI-generated.\n\nGemini's outputs are primarily determined by user prompts and like any generative AI tool, there may be instances where it generates content that some individuals find objectionable. We’ll continue to listen to your feedback through the thumbs up/down buttons and make ongoing improvements. For more details, you can read about our approach on our website.\n\nResults for illustrative purposes and may vary. Internet and subscription for certain features required. Available to users 18+. Create responsibly.",
              "domain": "gemini.google"
            }
          ],
          "success": true,
          "error": null
        },
        {
          "query_id": "q5",
          "query": "veo3.1 Google software version 2025",
          "claim_id": "claim_1",
          "query_type": "direct_fact",
          "priority": "high",
          "results": [
            {
              "position": 1,
              "title": "Introducing Veo 3.1 and advanced capabilities in Flow - Google Blog",
              "url": "https://blog.google/technology/ai/veo-updates-flow/",
              "snippet": "# Introducing Veo 3.1 and advanced capabilities in Flow\n\nFive months ago, we introduced Flow, our AI filmmaking tool powered by Veo, and have been inspired by the creativity it has sparked with over 275 million videos generated in Flow\n\n1\n\n. We're always listening to your feedback, and we've heard that you want more artistic control within Flow, with increased support for audio across all features.\n\nToday, we’re introducing new and enhanced creative capabilities to edit your clips, giving you more granular control over your final scene. For the first time, we’re also bringing audio to existing capabilities like “Ingredients to Video,” “Frames to Video” and “Extend.”\n\nWe’re also introducing Veo 3.1, which brings richer audio, more narrative control, and enhanced realism that captures true-to-life textures. Veo 3.1 is state-of-the-art and builds on Veo 3, with stronger prompt adherence and improved audiovisual quality when turning images into videos.\n\n## Refine your narrative with audio and more control\n\nWith Veo 3.1, we’re bringing audio to existing capabilities to help you craft the perfect scene. These features are experimental and actively improving, and we’re excited to see what you create as we iterate based on your feedback.\n\nNow, with rich, generated audio, you can:\n\n**Craft the look of your scene.**With \"Ingredients to Video,\" you can use multiple reference images to control the characters, objects and style. Flow uses your ingredients to create a final scene that looks just as you envisioned. **Control the shot from start to finish.**Provide a starting and ending image with “Frames to Video,” and Flow will generate a seamless video that bridges the two, perfect for artful and epic transitions. **Create longer, seamless shots.**With \"Extend,\" you can create longer videos, even lasting for a minute or more, that connect to and continue the action from your original clip. Each video is generated based on the final second of your previous clip, making it most useful for creating a longer establishing shot.... ## Edit your ingredients and videos with more precision\n\nGreat ideas can strike at any point in the creative process. For moments when the first take isn't the final one, we're introducing new editing capabilities directly within Flow to help you reimagine and perfect your scenes.\n\n**Add new elements to any scene.**With “Insert,” introduce anything you can imagine, from realistic details to fantastical creatures. Flow now handles complex details like shadows and scene lighting, making the addition look natural. **Remove unwanted objects or characters seamlessly.**Soon, you’ll be able to take anything out of a scene, and Flow will reconstruct the background and surroundings, making it look as though the object was never there.\n\n## Start creating in Flow today\n\nWith more precise editing capabilities, audio across all existing features and higher-quality outputs powered by Veo 3.1, we're opening up new possibilities for richer, more powerful video storytelling right inside Flow.\n\nThe Veo 3.1 model is also available via the Gemini API for developers, Vertex AI for enterprise customers, and the Gemini app. New capabilities are available in both Gemini API\n\n2\n\nand Vertex AI\n\n3",
              "domain": "blog.google"
            },
            {
              "position": 2,
              "title": "Build with Veo 3, now available in the Gemini API",
              "url": "https://developers.googleblog.com/en/veo-3-now-available-gemini-api/",
              "snippet": "First unveiled at Google I/O 2025, people around the world have already generated tens of millions of high-quality videos with Veo 3 (along with some new fun and interesting video trends). It is our first video model to incorporate high-fidelity video outputs and native audio, first with text-to-video and soon with image-to-video.\n\nDevelopers are already experimenting with Veo 3, discovering how the model can help them brainstorm content, rapidly iterate, and be more efficient.\n\nVeo 3 is designed to handle a range of video generation tasks, from cinematic narratives to dynamic character animations. With Veo 3, you can create more immersive experiences by not only generating stunning visuals, but also audio like dialogue and sound effects.\n\nLet’s take a look at some examples.\n\nExplore these examples and more with Veo 3 in Google AI Studio, available as an SDK template and interactive Starter App to remix, copy and extend. The Starter App and its sample code offer a convenient way for Paid Tier users to rapidly prototype with Veo 3 and more on the Gemini API, directly from Google AI Studio.\n\nClick the Key button in the top right of the AI Studio Build interface to select a Google Cloud Project with billing enabled to use the Paid Tier in AI Studio apps. See the FAQs for more.... Veo 3 will be priced at $0.75 per second for video and audio output. Additionally, Veo 3 Fast will be available soon, offering a faster and more cost-effective option for video creation.\n\nHere’s a basic Python example to create a video:\n\n```\n\nimport time\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.0-generate-preview\",\n\nprompt=\"a close-up shot of a golden retriever playing in a field of sunflowers\",\n\nconfig=types.GenerateVideosConfig(\n\nnegative_prompt=\"barking, woofing\",\n\n),\n\n\n\n# Waiting for the video(s) to be generated\n\nwhile not operation.done:\n\ntime.sleep(20)\n\noperation = client.operations.get(operation)\n\ngenerated_video = operation.result.generated_videos[0]\n\nclient.files.download(file=generated_video.video)\n\ngenerated_video.video.save(\"veo3_video.mp4\")\n\n```\n\nAll videos generated by Veo 3 models will continue to include a digital SynthID watermark. To get started, check out the documentation, cookbook, and a Veo 3 starter app in Google AI Studio:\n\nIn addition to being available via the Gemini API in Google AI Studio, Veo 3 is also available to Google AI subscribers in the Gemini app and Flow, and to enterprise customers via Vertex AI.... 1: Fluffy Characters Stop Motion: Inside a brightly colored, cozy kitchen made of felt and yarn. Professor Nibbles, a plump, fluffy hamster with oversized glasses, nervously stirs a bubbling pot on a miniature stove, muttering, \"Just a little more... 'essence of savory,' as the recipe calls for.\" The camera is a mid-shot, capturing his frantic stirring. Suddenly, the pot emits a loud \"POP!\" followed by a comical \"whoosh\" sound, and a geyser of iridescent green slime erupts, covering the entire kitchen. Professor Nibbles shrieks, \"Oh, dear! Not again!\" and scurries away, leaving a trail of tiny, panicked squeaks.\n\n2: The sequence begins with an extreme close-up of a single gear, slowly turning and reflecting harsh sunlight. The camera gradually pulls back in a continuous movement, revealing this is but one component of a colossal, mechanical heart half-buried in a desolate, rust-colored desert. A sweeping aerial shot establishes its enormous scale and isolation in the barren landscape. The camera descends to capture pipes hissing steam and the rhythmic thumping that echoes across the empty plains. A subtle shake effect synchronizes with each massive heartbeat. A lateral tracking shot discovers tiny, robed figures scurrying across the metallic surface. The camera follows one such figure in a detailed tracking shot as they perform meticulous maintenance, polishing brass valves and tightening immense bolts. A complex movement circles the entire structure, capturing different maintenance teams working in precarious positions across its rusted exterior. The final shot begins tight on the meticulous work of one tiny figure before executing a dramatic pull-out that reveals the true scale of the heart and the minuscule size of its caretakers, tending to the vital organ of an unseen, sleeping giant that extends beyond the frame.",
              "domain": "developers.googleblog.com"
            },
            {
              "position": 3,
              "title": "Google announces Veo 3.1 and updates Flow with more controls, tools",
              "url": "https://9to5google.com/2025/10/15/veo-3-1/",
              "snippet": "Google today announced Veo 3.1 as its latest video generation model, with Flow getting a number of updates to take advantage of the latest capabilities.\n\nCompared to Veo 3, which was announced at I/O 2025 in May, this new version offers richer audio and “enhanced realism that captures true-to-life textures.” Veo 3.1 has a deeper understanding of storytelling, cinematic styles, and character interactions to give you more narrative control. The image-to-video capability benefits from improved audio-visual quality and better follows your prompt.\n\nVeo 3.1 and Veo 3.1 Fast are available in the Gemini app, as well as the Gemini API and Vertex AI, to power text-to-video and image-to-video for horizontal (16×9) and vertical (9×16) outputs.\n\nMeanwhile, Google is updating the Flow filmmaking tool to take advantage of Veo 3.1. Audio generation is coming to:\n\n**Ingredients to Video**: This can include images of multiple characters, objects, and styles **Scene extension**: You can Extend an existing clip, with Google basing it on the last second **Frames to Video**: This lets you specify the starting and ending images, with Flow bridging the two\n\nFlow is getting new editing capabilities. You can insert elements like objects, characters, and details, with Google handling shadows, scene lighting, and other complex details to make everything look natural.\n\nComing soon is the ability to remove objects and characters from a scene, with Flow working to reconstruct the background and surroundings to make the edit seamless.\n\n\n\n## Comments",
              "domain": "9to5google.com"
            },
            {
              "position": 4,
              "title": "Introducing Veo 3.1 and new creative capabilities in the Gemini API",
              "url": "https://developers.googleblog.com/en/introducing-veo-3-1-and-new-creative-capabilities-in-the-gemini-api/",
              "snippet": "**Today, we are releasing Veo 3.1 and Veo 3.1 Fast in paid preview in the** **Gemini API** **.** This updated model offers several improvements, as well as improved outputs when generating video from images. These new models are available via the Gemini API in Google AI Studio and Vertex AI. Veo 3.1 is also available in the Gemini app and Flow.\n\nVeo 3.1 and Veo 3.1 Fast empower developers to create more engaging content through significant upgrades. The models now generate\n\n**richer native audio**, from natural conversations to synchronized sound effects, and offer **greater narrative control** with an improved understanding of cinematic styles. **Enhanced image-to-video** capabilities ensure better prompt adherence while delivering superior audio and visual quality and maintaining character consistency across multiple scenes.\n\nIn addition to the model update, we are introducing new model capabilities, including using reference images to guide video generation, extend existing Veo videos, and generate transitions between a first and last frame.\n\nYou can now guide the generation process by providing up to 3 reference images of a character, object, or scene. This is helpful for maintaining character consistency across multiple shots or applying a specific style to your video.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nconfig=types.GenerateVideosConfig(\n\nreference_images=[reference_image1, reference_image2],\n\n),\n\n\n\n```... Your story is no longer limited by the original generation. With Scene extension, you can create longer videos, even lasting for a minute or more, by generating new clips that connect to your previous video. Each new video is generated based on the final second of the previous clip. This maintains visual continuity, making the feature ideal for extending shots with background audio.\n\n```\n\nfrom google import genai\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nvideo=video_to_extend\n\n\n\n```\n\nCreate smooth, natural scenes that bridge two different images. By providing a starting and an ending image, you can direct Veo 3.1 to generate the transition between them, complete with accompanying audio.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nimage=first_frame,\n\nconfig=types.GenerateVideosConfig(\n\nlast_frame=last_frame,\n\n),\n\n\n\n```\n\n**What people are building with Veo 3.1**\n\nPromise Studios, a GenAI movie studio, uses Veo 3.1 within its MUSE Platform to enhance generative storyboarding and previsualization for director-driven storytelling at production quality.... Latitude is experimenting with Veo 3.1 in its generative narrative engine to instantly bring user-created stories to life.\n\nVeo 3.1 and these new capabilities are now available in paid preview via the Gemini API.\n\nVeo 3.1 is the same price as Veo 3. We are incredibly excited to see what you will build with these new capabilities!",
              "domain": "developers.googleblog.com"
            },
            {
              "position": 5,
              "title": "Google's Veo 3.1 is better at generating videos from images",
              "url": "https://www.engadget.com/ai/googles-veo-31-is-better-at-generating-videos-from-images-220829129.html",
              "snippet": "Google has released a new update to its Veo AI video generation model that should make it do a better job of sticking to prompts and converting images into videos. Veo 3.1 is available to try today through Google's Gemini API and is now also powering the company's Flow video editor.\nVeo 3.1 builds on the new capabilities Google introduced with launch of Veo 3 at Google I/O 2025. The new model offers better \"prompt adherence,\" according to Google, and should have an easier time creating videos based on the image \"ingredients\" you upload alongside your written prompt. Veo 3.1 also makes it possible to convert images to video and generate audio at the same time, a capability that wasn't available with Veo 3.\nIn Flow, Veo 3.1 supports at least a new feature that gives you finer control over the videos you generate. With what Google calls \"Frame to Video,\" Flow lets you upload a first and last frame, and then generates the video in-between. Adobe Firefly, which is powered by Veo 3, offers a similar feature, but Flow will be able to pull it off and create audio at the same time. Those added audio skills will also apply to the video editor's ability to extend clips and insert objects into existing footage, too.\nBased on the samples Google's shared, videos generated with Veo 3.1 still have an uncanny quality that seems to vary greatly depending on the prompt and subject. Even if it's missing some of the realism of OpenAI's Sora 2, though, the company's decision to try and make Veo more useful to people who actually work with video rather than a source of social media spam is a welcome move.",
              "domain": "www.engadget.com"
            },
            {
              "position": 6,
              "title": "Release notes | Gemini API | Google AI for Developers",
              "url": "https://ai.google.dev/gemini-api/docs/changelog",
              "snippet": "This page documents updates to the Gemini API.\n\n## October 17, 2025\n\n**Grounding with Google Maps**is now generally available. For more information, see Grounding with Google Maps documentation.\n\n## October 15, 2025\n\nReleased Veo 3.1 and 3.1 Fast models in public preview, with new features including:\n\n- Extending Veo-created videos.\n\n- Referencing up to three images to generate a video.\n\n- Providing first and last frame images to generate videos from.\n\nThis launch also added more options for Veo 3 output video durations: 4, 6, and 8 seconds.\n\nDeprecation for\n\n`veo-3.0-generate-preview`and\n\n`veo-3.0-fast-generate-preview`coming October 22, 2025.\n\n## October 7, 2025\n\n- Launched Gemini 2.5 Computer Use Preview\n\n## October 2, 2025\n\n- Launched Gemini 2.5 Flash Image GA: Image Generation with Gemini\n\n## September 29, 2025\n\n- The following Gemini 1.5 models are now deprecated:\n\n`gemini-1.5-pro`\n\n`gemini-1.5-flash-8b`\n\n`gemini-1.5-flash`... ## September 9, 2025\n\n- Launched Veo 3 and Veo 3 Fast GA, with lower pricing and new options for aspect ratios, resolution, and seeding. Read the Veo documentation for more information.\n\n## August 26, 2025\n\n- Launched Gemini 2.5 Image Preview, our latest native image generation model.\n\n## August 18, 2025\n\n- Released URL context tool to general\n\navailability (GA), a tool for providing URLs as additional context to\n\nprompts. Support for using URL context with the\n\n`gemini-2.0-flash`model (available during experimental release) will be discontinued in one week.\n\n## August 14, 2025\n\n- Released Imagen 4 Ultra, Standard and Fast models as generally available (GA). To learn more, see the Imagen page.\n\n## August 7, 2025\n\n`allow_adult`setting in Image to Video generation are now available in restricted regions. See the Veo page for details.\n\n## July 31, 2025\n\n- Launched image-to-video generation for the Veo 3 Preview model.\n\n- Released Veo 3 Fast Preview model.\n\n- To learn more about Veo 3, visit the Veo page.... ## July 22, 2025\n\n- Released\n\n`gemini-2.5-flash-lite`, our fast, low-cost, high-performance Gemini 2.5 model. To learn more, see Gemini 2.5 Flash-Lite.\n\n## July 17, 2025\n\nLaunched\n\n`veo-3.0-generate-preview`, the latest update to Veo introducing video with audio generation. To learn more about Veo 3, visit the Veo page.\n\nIncreased rate limits for Imagen 4 Standard and Ultra. Visit the Rate limits page for more details.\n\n## July 14, 2025\n\n- Released\n\n`gemini-embedding-001`, the stable version of our text embedding model. To learn more, see embeddings. The\n\n`gemini-embedding-exp-03-07`model will be deprecated on August 14, 2025.\n\n## July 7, 2025\n\n- Launched Gemini API Batch Mode. Batch up requests and send them to process asynchronously. To learn more, see Batch Mode.\n\n## June 26, 2025\n\nThe preview models\n\n`gemini-2.5-pro-preview-05-06`and\n\n`gemini-2.5-pro-preview-03-25`are now redirecting to the latest stable version\n\n`gemini-2.5-pro`.\n\n`gemini-2.5-pro-exp-03-25`is deprecated.... ## May 7, 2025\n\n- Released\n\n`gemini-2.0-flash-preview-image-generation`, a preview model for generating and editing images. To learn more, see Image generation and Gemini 2.0 Flash Preview Image Generation.\n\n## May 6, 2025\n\n- Released\n\n`gemini-2.5-pro-preview-05-06`, a new version of our most powerful model, with improvements on code and function calling.\n\n`gemini-2.5-pro-preview-03-25`will automatically point to the new version of the model.\n\n## April 17, 2025\n\n- Released\n\n`gemini-2.5-flash-preview-04-17`, a Gemini preview model optimized for price-performance and adaptive thinking. To learn more, see Gemini 2.5 Flash Preview and Thinking.\n\n## April 16, 2025\n\n- Launched context caching for Gemini 2.0 Flash.... ## April 9, 2025\n\n**Model updates:**\n\n- Released\n\n`veo-2.0-generate-001`, a generally available (GA) text- and image-to-video model, capable of generating detailed and artistically nuanced videos. To learn more, see the Veo docs.\n\nReleased\n\n`gemini-2.0-flash-live-001`, a public preview version of the Live API model with billing enabled.\n\n**Enhanced Session Management and Reliability** **Session Resumption:**Keep sessions alive across temporary network disruptions. The API now supports server-side session state storage (for up to 24 hours) and provides handles (session_resumption) to reconnect and resume where you left off. **Longer Sessions via Context Compression:**Enable extended interactions beyond previous time limits. Configure context window compression with a sliding window mechanism to automatically manage context length, preventing abrupt terminations due to context limits. **Graceful Disconnect Notification:**Receive a\n\n`GoAway`server message indicating when a connection is about to close, allowing for graceful handling before termination.\n\n\n\n**More Control over Interaction Dynamics** **Configurable Voice Activity Detection (VAD):**Choose sensitivity levels or disable automatic VAD entirely and use new client events (\n\n`activityStart`,\n\n`activityEnd`) for manual turn control.\n\n**Configurable Interruption Handling:**Decide whether user input should interrupt the model's response. **Configurable Turn Coverage:**Choose whether the API processes all audio and video input continuously or only captures it when the end-user is detected speaking. **Configurable Media Resolution:**Optimize for quality or token usage by selecting the resolution for input media. **Richer Output and Features** **Expanded Voice & Language Options:**Choose from two new voices and 30 new languages for audio output. The output language is now configurable within\n\n`speechConfig`.\n\n**Text Streaming:**Receive text responses incrementally as they are generated, enabling faster display to the user. **Token Usage Reporting:**Gain insights into usage with detailed token counts provided in the\n\n`usageMetadata`field of server messages, broken down by modality and prompt or response phases.... ## April 4, 2025\n\n- Released\n\n`gemini-2.5-pro-preview-03-25`, a public preview Gemini 2.5 Pro version with billing enabled. You can continue to use\n\n`gemini-2.5-pro-exp-03-25`on the free tier.\n\n## March 25, 2025\n\n- Released\n\n`gemini-2.5-pro-exp-03-25`, a public experimental Gemini model with thinking mode always on by default. To learn more, see Gemini 2.5 Pro Experimental.\n\n## March 12, 2025\n\n**Model updates:**\n\n- Launched an experimental Gemini 2.0 Flash model capable of image generation and editing.\n\n- Released\n\n`gemma-3-27b-it`, available on AI Studio and through the Gemini API, as part of the Gemma 3 launch.\n\n**API updates:**\n\n- Added support for YouTube URLs as a media source.\n\n- Added support for including an inline video of less than 20MB.\n\n## March 11, 2025\n\n**SDK updates:**\n\n- Released the Google Gen AI SDK for TypeScript and JavaScript to public preview.\n\n## March 7, 2025\n\n**Model updates:**\n\n- Released\n\n`gemini-embedding-exp-03-07`, an experimental Gemini-based embeddings model in public preview.... ## February 28, 2025\n\n**API updates:**\n\n- Support for Search as a tool\n\nadded to\n\n`gemini-2.0-pro-exp-02-05`, an experimental model based on Gemini 2.0 Pro.\n\n## February 25, 2025\n\n**Model updates:**\n\n- Released\n\n`gemini-2.0-flash-lite`, a generally available (GA) version of Gemini 2.0 Flash-Lite, which is optimized for speed, scale, and cost efficiency.\n\n## February 19, 2025\n\n**AI Studio updates:**\n\n- Support for additional regions (Kosovo, Greenland and Faroe Islands).\n\n**API updates:**\n\n- Support for additional regions (Kosovo, Greenland and Faroe Islands).\n\n## February 18, 2025\n\n**Model updates:**\n\n- Gemini 1.0 Pro is no longer supported. For the list of supported models, see Gemini models.\n\n## February 11, 2025\n\n**API updates:**\n\n- Updates on the OpenAI libraries compatibility.\n\n## February 6, 2025\n\n**Model updates:**\n\n- Released\n\n`imagen-3.0-generate-002`, a generally available (GA) version of Imagen 3 in the Gemini API.\n\n**SDK updates:**\n\n- Released the Google Gen AI SDK for Java for public preview.",
              "domain": "ai.google.dev"
            },
            {
              "position": 7,
              "title": "Google releases Veo 3.1, adds it to Flow video editor",
              "url": "https://techcrunch.com/2025/10/15/google-releases-veo-3-1-adds-it-to-flow-video-editor/",
              "snippet": "In Brief\nGoogle launched its new video model Veo 3.1 with improved audio output, granular editing controls, and better output for image to video. It said that Veo 3.1 builds on May's Veo 3 release and generates more realistic clips and adheres to prompts better.\nThe model allows users to add an object to the video and have it blend into the clip's style, Google said. Soon, users will be able to remove an existing object from the video in Flow, too.\nVeo 3 already has edit features such as adding reference images to drive a character, providing the first and last frame to generate a clip using AI, and the ability to extend an existing video based on the last few frames. With Veo 3.1, Google is adding audio to all these features to make the clips more lively.\nThe company is rolling out the model to its video editor Flow, the Gemini App, along with Vertex and Gemini APIs. It said that since Flow's launch in May, users have created more than 275 million videos on the app.",
              "domain": "techcrunch.com"
            },
            {
              "position": 8,
              "title": "Meet Flow: AI-powered filmmaking with Veo 3",
              "url": "https://blog.google/technology/ai/google-flow-veo-ai-filmmaking-tool/",
              "snippet": "# Meet Flow: AI-powered filmmaking with Veo 3\n\nToday we’re introducing Flow, our new AI filmmaking tool.\n\nIt’s built by and for creatives, and it’s the only AI filmmaking tool custom-designed for Google’s most advanced models — Veo, Imagen and Gemini. Flow can help storytellers explore their ideas without bounds and create cinematic clips and scenes for their stories. It’s early days, and we’re excited to shape the future of Flow with creatives and filmmakers.\n\n### What’s possible with Flow\n\nFlow is inspired by what it feels like when time slows down and creation is effortless, iterative and full of possibility. It’s custom-designed for Veo, Google’s state-of-the-art generative video model, with exceptional prompt adherence and stunning cinematic outputs that excel at physics and realism. Behind the scenes, Gemini models make prompting intuitive, so you can describe your vision in everyday language. You can bring your own assets to create characters, or use Flow to make your own ingredients with Imagen’s text-to-image capabilities.\n\nOnce you’ve created a subject or a scene, you can integrate those same ingredients into different clips and scenes with consistency. Or you can use a scene image to start a new shot.\n\nCreate your ingredients\n\nUse those ingredients to create a clip\n\nReference ingredients in plain language... ### Key features to unlock your storytelling\n\nFlow also comes with a range of features for professionals or those just getting started:\n\n**Camera Controls:**Master your shot with direct control over camera motion, angles and perspectives. **Scenebuilder:**Seamlessly edit and extend your existing shots — revealing more of the action or transitioning to what happens next with continuous motion and consistent characters. **Asset Management:**Easily manage and organize all of your ingredients and prompts. **Flow TV** **:**Spark your creativity with an ever-growing showcase of clips, channels, and content generated with Veo. You can see the exact prompts and techniques used for clips you like, providing a practical way to learn and adapt new styles.\n\nSeamless transitions\n\nCamera controls\n\nCinematic quality\n\n### Get started with Flow\n\nFlow is the evolution of VideoFX, a Google Labs experiment that launched last year. Starting today, Flow is available to subscribers of our Google AI Pro and Google AI Ultra plans in the U.S., with more countries coming soon.\n\nGoogle AI Pro gives you the key Flow features and 100 generations per month, and Google AI Ultra gives you the highest usage limits and early access to Veo 3 with native audio generation, bringing environmental sounds and character dialogue directly into video creation.... ### How we’re collaborating with filmmakers\n\nAs with any groundbreaking technology, we’re still understanding the full potential of AI in filmmaking. We see the emergence of these tools as an enabler, helping a new wave of filmmakers more easily tell their stories. By offering filmmakers early access to Flow, we were able to better understand how our technology could best support and integrate into their creative workflows — and we’ve woven their insights into Flow. Here are some filmmakers we partnered with and the short films they developed using Flow along with other tools and techniques.\n\n**Dave Clark**\n\nDave is an award-winning filmmaker focused on embracing new technology as part of his filmmaking. He used AI to develop two of his most recent short films, “Battalion” and “NinjaPunk.” His newest short film “Freelancers” uses Google’s AI and other tools to tell the story of two estranged adopted brothers on similar quests.\n\n**Henry Daubrez**\n\nHenry has been using tech tools in his art for the last 18 years. Earlier this year he unveiled “Kitsune” using Veo 2 — a moving short film about “love between two souls separated by everything except their shared feelings of loneliness.” Now, Henry is bringing the story of his own creative journey to life in “Electric Pink.”\n\n**Junie Lau**\n\nJunie Lau is a film director and multidisciplinary creative deeply passionate about innovation, viewing AI as a vital collaborator in expanding the boundaries of creative expression. Her work delves into artistic narratives within the hyper-modern era, including themes of virtual identity, digital humanities and digital ontology. Currently, Junie is working on a film titled “Dear Stranger,” which explores the boundless and infinite nature of universal love between a grandmother and grandchild across countless parallel worlds.\n\nAI is ushering in a new chapter of creativity and filmmaking, and while it’s still early, we see so much potential for tools like Flow to unlock new voices and creations.\n\nFor more insights on Flow and how AI helps storytellers take more risks in the creative storytelling process, watch Dave Clark, Junie Lau, and Henry Daubrez in \"Behind the Lens: AI, Creativity, and the Future of Filmmaking Tools.\"",
              "domain": "blog.google"
            },
            {
              "position": 9,
              "title": "Gemini AI video generator powered by Veo 3.1",
              "url": "https://gemini.google/overview/video-generation/",
              "snippet": "# Break the\n\n**silence** with Veo 3.1\n\nCreate high-quality, 8-second videos with Veo 3.1, our latest AI video generator. Simply describe what you have in mind or upload a photo and watch your ideas come to life with native audio generation. Try it with a Google AI Pro plan or get the highest access with the Ultra plan.\n\n**Veo 3.1** speaks for itself\n\n## Dream it. Describe it.\n\n**Done.**\n\n## For Exploring\n\nPlay with diverse styles, bring animated characters to life, and combine objects in ways you never thought possible. See what you can create using text to video with AI.\n\n## For Sharing\n\nCreate funny memes, turn inside jokes into videos, re-imagine special moments, and add a personal touch to make someone smile.\n\n## For Brainstorming\n\nBreak through creative blocks and visualize your ideas in a flash. From product concepts and designs to rapid prototyping and storytelling, Gemini can help.\n\n## Learn more about our\n\n**Veo Models**\n\nCreate videos with sound using our video generation model that maintains high-quality while optimizing for speed.\n\nCreate high-quality, 8-second videos with sound using our state-of-the-art video generation model.... ## Frequently asked questions\n\nYes, you can create and share videos in your mobile Gemini app. To create videos, tap the \"video\" button in your prompt bar. If you don't see it, tap the button with three dots to view more options.\n\nTry Veo 3.1 Fast with a Google AI Pro plan or get the highest access to Veo 3.1 in Google AI Ultra. Country availability here.\n\nFor now, the ability to generate a video from a photo is not available in the European Economic Area, Switzerland, or the United Kingdom.\n\nWe’ve taken several important safety steps to make AI video generation a safe experience. This includes extensive red teaming and evaluation aimed at preventing the generation of content that violates our policies. Additionally, all videos generated with Veo in the Gemini app are marked with a visible watermark and SynthID, a digital watermark embedded in each frame, which indicates the videos are AI-generated.\n\nGemini's outputs are primarily determined by user prompts and like any generative AI tool, there may be instances where it generates content that some individuals find objectionable. We’ll continue to listen to your feedback through the thumbs up/down buttons and make ongoing improvements. For more details, you can read about our approach on our website.\n\nResults for illustrative purposes and may vary. Internet and subscription for certain features required. Available to users 18+. Create responsibly.",
              "domain": "gemini.google"
            },
            {
              "position": 10,
              "title": "Unpacking the magic of our new creative tools - YouTube Blog",
              "url": "https://blog.youtube/news-and-events/generative-ai-creation-tools-made-on-youtube-2025/",
              "snippet": "# Unpacking the magic of our new creative tools\n\nSep 16, 2025 [[read-time]] minute read\n\nSep 16, 2025 [[read-time]] minute read\n\nToday at Made on YouTube, we unveiled a suite of features designed to make creativity on YouTube more playful and effortless than ever before. We’re taking a deeper look at what you can expect when you try them out yourself.\n\nWe’ve partnered with Google DeepMind to bring a custom version of their most powerful video generation model, Veo 3, to YouTube. Veo 3 Fast is designed to work seamlessly in YouTube Shorts for millions of creators and users, for free. It generates outputs with lower latency at 480p so you can easily create video clips – and for the first time, with sound – from any idea, all from your phone.\n\nTap the create button, then the sparkle icon in the top right corner to find our latest gen AI creation tools including Veo 3.\n\nIt’s rolling out now in the United States, United Kingdom, Canada, Australia, and New Zealand, with plans to expand to more of you soon.\n\nWe're also introducing new Veo capabilities on Shorts that put your imagination in the spotlight. Soon, you’ll be able to:... We’ll start to experiment with all of these new capabilities in the coming months on Shorts.\n\nStarting a video from a blank timeline can be daunting. That’s why we’re introducing Edit with AI, a new feature that takes the initial heavy lifting off your shoulders. It transforms your raw camera roll footage into a compelling first draft, intelligently finding and arranging your best moments, adding music, transitions, and even a playful voiceover that can react to what’s happening in the video, in either English or Hindi. This gives you a solid starting point so you can jump straight to the fun part: personalizing your video and bringing your unique vision to life.\n\nWe’re experimenting now with Edit with AI on Shorts and in the YouTube Create app, and will expand the feature in the coming weeks in select markets.\n\nAs the world's largest creative playground, YouTube is where trends are born and where you can draw inspiration from. Imagine hearing a line of dialogue that sparks an idea—a funny phrase, a memorable quote, or a one-of-a-kind sound—and you want to remix it into a new sound. With our new Speech to Song remixing tool, you'll be able to do just that, quickly turning the dialogue from eligible videos into catchy soundtracks for your next Short.\n\nCheck it out for yourself. Listen to the original video, and how Speech to song helped transform it using Lyria 2, Google DeepMind's most advanced AI music model.... Now your GRWM videos take on a whole new level of fun!\n\nBehind the scenes, this uses the dialogue from the original video with Lyria 2 to help create the song. And you're able to add your own vibe for the song, like, chill, danceable, or fun. The final result attributes the original creator.\n\nAs always and across these features, we use SynthID watermarks and content labels to indicate that these creations were generated with AI.\n\nWe hope these features foster a new era of playful experimentation on YouTube!",
              "domain": "blog.youtube"
            }
          ],
          "success": true,
          "error": null
        },
        {
          "query_id": "q7",
          "query": "veo3.1 Google software version 2025 official data",
          "claim_id": "claim_1",
          "query_type": "statistical",
          "priority": "high",
          "results": [],
          "success": false,
          "error": "Rate limit exceeded. Please try again later."
        },
        {
          "query_id": "q6",
          "query": "Google veo3.1 release statement 2025",
          "claim_id": "claim_1",
          "query_type": "source_verification",
          "priority": "high",
          "results": [
            {
              "position": 1,
              "title": "Introducing Veo 3.1 and advanced capabilities in Flow - Google Blog",
              "url": "https://blog.google/technology/ai/veo-updates-flow/",
              "snippet": "# Introducing Veo 3.1 and advanced capabilities in Flow\n\nFive months ago, we introduced Flow, our AI filmmaking tool powered by Veo, and have been inspired by the creativity it has sparked with over 275 million videos generated in Flow\n\n1\n\n. We're always listening to your feedback, and we've heard that you want more artistic control within Flow, with increased support for audio across all features.\n\nToday, we’re introducing new and enhanced creative capabilities to edit your clips, giving you more granular control over your final scene. For the first time, we’re also bringing audio to existing capabilities like “Ingredients to Video,” “Frames to Video” and “Extend.”\n\nWe’re also introducing Veo 3.1, which brings richer audio, more narrative control, and enhanced realism that captures true-to-life textures. Veo 3.1 is state-of-the-art and builds on Veo 3, with stronger prompt adherence and improved audiovisual quality when turning images into videos.\n\n## Refine your narrative with audio and more control\n\nWith Veo 3.1, we’re bringing audio to existing capabilities to help you craft the perfect scene. These features are experimental and actively improving, and we’re excited to see what you create as we iterate based on your feedback.\n\nNow, with rich, generated audio, you can:\n\n**Craft the look of your scene.**With \"Ingredients to Video,\" you can use multiple reference images to control the characters, objects and style. Flow uses your ingredients to create a final scene that looks just as you envisioned. **Control the shot from start to finish.**Provide a starting and ending image with “Frames to Video,” and Flow will generate a seamless video that bridges the two, perfect for artful and epic transitions. **Create longer, seamless shots.**With \"Extend,\" you can create longer videos, even lasting for a minute or more, that connect to and continue the action from your original clip. Each video is generated based on the final second of your previous clip, making it most useful for creating a longer establishing shot.... ## Edit your ingredients and videos with more precision\n\nGreat ideas can strike at any point in the creative process. For moments when the first take isn't the final one, we're introducing new editing capabilities directly within Flow to help you reimagine and perfect your scenes.\n\n**Add new elements to any scene.**With “Insert,” introduce anything you can imagine, from realistic details to fantastical creatures. Flow now handles complex details like shadows and scene lighting, making the addition look natural. **Remove unwanted objects or characters seamlessly.**Soon, you’ll be able to take anything out of a scene, and Flow will reconstruct the background and surroundings, making it look as though the object was never there.\n\n## Start creating in Flow today\n\nWith more precise editing capabilities, audio across all existing features and higher-quality outputs powered by Veo 3.1, we're opening up new possibilities for richer, more powerful video storytelling right inside Flow.\n\nThe Veo 3.1 model is also available via the Gemini API for developers, Vertex AI for enterprise customers, and the Gemini app. New capabilities are available in both Gemini API\n\n2\n\nand Vertex AI\n\n3",
              "domain": "blog.google"
            },
            {
              "position": 2,
              "title": "Google announces Veo 3.1 and updates Flow with more controls, tools",
              "url": "https://9to5google.com/2025/10/15/veo-3-1/",
              "snippet": "Google today announced Veo 3.1 as its latest video generation model, with Flow getting a number of updates to take advantage of the latest capabilities.\n\nCompared to Veo 3, which was announced at I/O 2025 in May, this new version offers richer audio and “enhanced realism that captures true-to-life textures.” Veo 3.1 has a deeper understanding of storytelling, cinematic styles, and character interactions to give you more narrative control. The image-to-video capability benefits from improved audio-visual quality and better follows your prompt.\n\nVeo 3.1 and Veo 3.1 Fast are available in the Gemini app, as well as the Gemini API and Vertex AI, to power text-to-video and image-to-video for horizontal (16×9) and vertical (9×16) outputs.\n\nMeanwhile, Google is updating the Flow filmmaking tool to take advantage of Veo 3.1. Audio generation is coming to:\n\n**Ingredients to Video**: This can include images of multiple characters, objects, and styles **Scene extension**: You can Extend an existing clip, with Google basing it on the last second **Frames to Video**: This lets you specify the starting and ending images, with Flow bridging the two\n\nFlow is getting new editing capabilities. You can insert elements like objects, characters, and details, with Google handling shadows, scene lighting, and other complex details to make everything look natural.\n\nComing soon is the ability to remove objects and characters from a scene, with Flow working to reconstruct the background and surroundings to make the edit seamless.\n\n\n\n## Comments",
              "domain": "9to5google.com"
            },
            {
              "position": 3,
              "title": "Google releases Veo 3.1, adds it to Flow video editor",
              "url": "https://techcrunch.com/2025/10/15/google-releases-veo-3-1-adds-it-to-flow-video-editor/",
              "snippet": "In Brief\nGoogle launched its new video model Veo 3.1 with improved audio output, granular editing controls, and better output for image to video. It said that Veo 3.1 builds on May's Veo 3 release and generates more realistic clips and adheres to prompts better.\nThe model allows users to add an object to the video and have it blend into the clip's style, Google said. Soon, users will be able to remove an existing object from the video in Flow, too.\nVeo 3 already has edit features such as adding reference images to drive a character, providing the first and last frame to generate a clip using AI, and the ability to extend an existing video based on the last few frames. With Veo 3.1, Google is adding audio to all these features to make the clips more lively.\nThe company is rolling out the model to its video editor Flow, the Gemini App, along with Vertex and Gemini APIs. It said that since Flow's launch in May, users have created more than 275 million videos on the app.",
              "domain": "techcrunch.com"
            },
            {
              "position": 4,
              "title": "Introducing Veo 3.1 and new creative capabilities in the Gemini API",
              "url": "https://developers.googleblog.com/en/introducing-veo-3-1-and-new-creative-capabilities-in-the-gemini-api/",
              "snippet": "**Today, we are releasing Veo 3.1 and Veo 3.1 Fast in paid preview in the** **Gemini API** **.** This updated model offers several improvements, as well as improved outputs when generating video from images. These new models are available via the Gemini API in Google AI Studio and Vertex AI. Veo 3.1 is also available in the Gemini app and Flow.\n\nVeo 3.1 and Veo 3.1 Fast empower developers to create more engaging content through significant upgrades. The models now generate\n\n**richer native audio**, from natural conversations to synchronized sound effects, and offer **greater narrative control** with an improved understanding of cinematic styles. **Enhanced image-to-video** capabilities ensure better prompt adherence while delivering superior audio and visual quality and maintaining character consistency across multiple scenes.\n\nIn addition to the model update, we are introducing new model capabilities, including using reference images to guide video generation, extend existing Veo videos, and generate transitions between a first and last frame.\n\nYou can now guide the generation process by providing up to 3 reference images of a character, object, or scene. This is helpful for maintaining character consistency across multiple shots or applying a specific style to your video.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nconfig=types.GenerateVideosConfig(\n\nreference_images=[reference_image1, reference_image2],\n\n),\n\n\n\n```... Your story is no longer limited by the original generation. With Scene extension, you can create longer videos, even lasting for a minute or more, by generating new clips that connect to your previous video. Each new video is generated based on the final second of the previous clip. This maintains visual continuity, making the feature ideal for extending shots with background audio.\n\n```\n\nfrom google import genai\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nvideo=video_to_extend\n\n\n\n```\n\nCreate smooth, natural scenes that bridge two different images. By providing a starting and an ending image, you can direct Veo 3.1 to generate the transition between them, complete with accompanying audio.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nimage=first_frame,\n\nconfig=types.GenerateVideosConfig(\n\nlast_frame=last_frame,\n\n),\n\n\n\n```\n\n**What people are building with Veo 3.1**\n\nPromise Studios, a GenAI movie studio, uses Veo 3.1 within its MUSE Platform to enhance generative storyboarding and previsualization for director-driven storytelling at production quality.... Latitude is experimenting with Veo 3.1 in its generative narrative engine to instantly bring user-created stories to life.\n\nVeo 3.1 and these new capabilities are now available in paid preview via the Gemini API.\n\nVeo 3.1 is the same price as Veo 3. We are incredibly excited to see what you will build with these new capabilities!",
              "domain": "developers.googleblog.com"
            },
            {
              "position": 5,
              "title": "Build with Veo 3, now available in the Gemini API",
              "url": "https://developers.googleblog.com/en/veo-3-now-available-gemini-api/",
              "snippet": "First unveiled at Google I/O 2025, people around the world have already generated tens of millions of high-quality videos with Veo 3 (along with some new fun and interesting video trends). It is our first video model to incorporate high-fidelity video outputs and native audio, first with text-to-video and soon with image-to-video.\n\nDevelopers are already experimenting with Veo 3, discovering how the model can help them brainstorm content, rapidly iterate, and be more efficient.\n\nVeo 3 is designed to handle a range of video generation tasks, from cinematic narratives to dynamic character animations. With Veo 3, you can create more immersive experiences by not only generating stunning visuals, but also audio like dialogue and sound effects.\n\nLet’s take a look at some examples.\n\nExplore these examples and more with Veo 3 in Google AI Studio, available as an SDK template and interactive Starter App to remix, copy and extend. The Starter App and its sample code offer a convenient way for Paid Tier users to rapidly prototype with Veo 3 and more on the Gemini API, directly from Google AI Studio.\n\nClick the Key button in the top right of the AI Studio Build interface to select a Google Cloud Project with billing enabled to use the Paid Tier in AI Studio apps. See the FAQs for more.... Veo 3 will be priced at $0.75 per second for video and audio output. Additionally, Veo 3 Fast will be available soon, offering a faster and more cost-effective option for video creation.\n\nHere’s a basic Python example to create a video:\n\n```\n\nimport time\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.0-generate-preview\",\n\nprompt=\"a close-up shot of a golden retriever playing in a field of sunflowers\",\n\nconfig=types.GenerateVideosConfig(\n\nnegative_prompt=\"barking, woofing\",\n\n),\n\n\n\n# Waiting for the video(s) to be generated\n\nwhile not operation.done:\n\ntime.sleep(20)\n\noperation = client.operations.get(operation)\n\ngenerated_video = operation.result.generated_videos[0]\n\nclient.files.download(file=generated_video.video)\n\ngenerated_video.video.save(\"veo3_video.mp4\")\n\n```\n\nAll videos generated by Veo 3 models will continue to include a digital SynthID watermark. To get started, check out the documentation, cookbook, and a Veo 3 starter app in Google AI Studio:\n\nIn addition to being available via the Gemini API in Google AI Studio, Veo 3 is also available to Google AI subscribers in the Gemini app and Flow, and to enterprise customers via Vertex AI.... 1: Fluffy Characters Stop Motion: Inside a brightly colored, cozy kitchen made of felt and yarn. Professor Nibbles, a plump, fluffy hamster with oversized glasses, nervously stirs a bubbling pot on a miniature stove, muttering, \"Just a little more... 'essence of savory,' as the recipe calls for.\" The camera is a mid-shot, capturing his frantic stirring. Suddenly, the pot emits a loud \"POP!\" followed by a comical \"whoosh\" sound, and a geyser of iridescent green slime erupts, covering the entire kitchen. Professor Nibbles shrieks, \"Oh, dear! Not again!\" and scurries away, leaving a trail of tiny, panicked squeaks.\n\n2: The sequence begins with an extreme close-up of a single gear, slowly turning and reflecting harsh sunlight. The camera gradually pulls back in a continuous movement, revealing this is but one component of a colossal, mechanical heart half-buried in a desolate, rust-colored desert. A sweeping aerial shot establishes its enormous scale and isolation in the barren landscape. The camera descends to capture pipes hissing steam and the rhythmic thumping that echoes across the empty plains. A subtle shake effect synchronizes with each massive heartbeat. A lateral tracking shot discovers tiny, robed figures scurrying across the metallic surface. The camera follows one such figure in a detailed tracking shot as they perform meticulous maintenance, polishing brass valves and tightening immense bolts. A complex movement circles the entire structure, capturing different maintenance teams working in precarious positions across its rusted exterior. The final shot begins tight on the meticulous work of one tiny figure before executing a dramatic pull-out that reveals the true scale of the heart and the minuscule size of its caretakers, tending to the vital organ of an unseen, sleeping giant that extends beyond the frame.",
              "domain": "developers.googleblog.com"
            },
            {
              "position": 6,
              "title": "Newest generative media...",
              "url": "https://blog.google/technology/ai/generative-media-models-io-2025/",
              "snippet": "# Fuel your creativity with new generative media models and tools\n\nToday, we’re announcing our newest generative media models, which mark significant breakthroughs. These models create breathtaking images, videos and music, empowering artists to bring their creative vision to life. They also power amazing tools for everyone to express themselves.\n\nVeo 3 and Imagen 4, our newest video and image generation models, push the frontier of media generation, with their groundbreaking new capabilities. We're also expanding access to Lyria 2, giving musicians more tools to create music. Finally, we’re inviting visual storytellers to try Flow, our new AI filmmaking tool. Using Google DeepMind’s most advanced models, Flow lets you weave cinematic films with more sophisticated control of characters, scenes and styles, to bring your story to life.\n\nWe’ve partnered closely with the creative industries — filmmakers, musicians, artists, YouTube creators — to help shape these models and products responsibly and to give creators new tools to realize the possibilities of AI in their art.\n\n## Veo 3: Video, meet audio\n\nVeo 3, our new state-of-the-art video generation model, not only improves on the quality of Veo 2, but for the first time, can also generate videos with audio — traffic noises in the background of a city street scene, birds singing in a park, even dialogue between characters.\n\nAcross the board, Veo 3 excels from text and image prompting to real-world physics and accurate lip syncing. It’s great at understanding; you can tell a short story in your prompt, and the model gives you back a clip that brings it to life. Veo 3 is available today for Ultra subscribers in the United States in the Gemini app and in Flow. It’s also available for enterprise users on Vertex AI.... ## Veo 2 updates: New capabilities built with and for filmmakers\n\nAs we advance Veo 3, we’ve also added new capabilities to our popular Veo 2 model informed by our work with creators and filmmakers. Today, we’re launching several of these new capabilities, including:\n\n**Our state-of-the-art reference powered video**capability allows you to give Veo images of characters, scenes, objects, and even styles for better creative control and consistency. **Camera controls**help you define precise camera movements, including rotations, dollies and zooms, to achieve the perfect shot. **Outpainting**allows you to broaden your frame, turning your video from portrait to landscape, and making it easier to fit any screen size, intelligently adding to the scene. **Object add and remove**lets you add or erase objects from your videos. Veo understands scale, interactions, and shadows, and uses this understanding to create a natural, realistic-looking scene.\n\nReference powered video and camera controls are available now in Flow. We're excited to bring all these new capabilities to the Vertex AI API in the coming weeks, and to more products over the next few months.\n\nOriginal\n\nOutpaint and add a castle\n\nOriginal\n\nRemove spaceship\n\n## Flow: An AI filmmaking tool designed for Veo\n\nBuilt with and for creatives, Flow is an AI filmmaking tool that lets you seamlessly create cinematic clips, scenes and stories by bringing together Google DeepMind’s most advanced models: Veo, Imagen and Gemini. Use natural language to describe your shots to Flow, manage the ingredients for your story — cast, locations, objects and styles — in a single convenient place, and use Flow to weave your narrative into beautiful scenes.\n\nFlow is available today for Google AI Pro and Ultra plan subscribers in the U.S., with more countries coming soon.... ## Imagen 4: Stunning quality and superior typography\n\nOur latest Imagen model combines speed with precision to create stunning images. Imagen 4 has remarkable clarity in fine details like intricate fabrics, water droplets, and animal fur, and excels in both photorealistic and abstract styles. Imagen 4 can create images in a range of aspect ratios and up to 2k resolution - even better for printing or presentations. It is also significantly better at spelling and typography, making it easier to create your own greeting cards, posters and even comics.\n\nImagen 4 is available today in the Gemini app, Whisk, Vertex AI and across Slides, Vids, Docs and more in Workspace.\n\nSoon we’ll also be launching a fast variant of Imagen 4 that’s up to 10x faster than Imagen 3 — so you can explore ideas even faster.\n\n## Lyria 2: Powerful composition and endless exploration\n\nIn April, we expanded access to Music AI Sandbox, powered by Lyria 2. Music AI Sandbox offers musicians, producers and songwriters a set of experimental tools, which can spark new creative possibilities and help artists explore unique musical ideas. The expertise and valuable feedback from the music industry help us ensure our tools empower creators, while inviting creatives to realize the possibilities of AI in their art.\n\nLyria 2 brings powerful composition and endless exploration, and is now available for creators through YouTube Shorts and enterprises in Vertex AI. We've also made Lyria RealTime, our interactive music generation model which powers MusicFX DJ, available via an API and in AI Studio. Lyria RealTime allows anyone to interactively create, control, and perform generative music in real time.... ## Responsible creation and collaboration with the creative community\n\nSince launching in 2023, SynthID has watermarked over 10 billion images, videos, audio files and texts, helping identify them as AI-generated and reduce the chances of misinformation and misattribution. Outputs generated by Veo 3, Imagen 4 and Lyria 2 will continue to have SynthID watermarks.\n\nToday, we’re launching SynthID Detector, a verification portal to help people identify AI-generated content. Upload a piece of content and the SynthID Detector will identify if either the entire file or just a part of it has SynthID in it.\n\nWith all our generative AI models, we aim to unleash human creativity and enable artists and creators to bring their ideas to life faster and more easily than ever before.",
              "domain": "blog.google"
            },
            {
              "position": 7,
              "title": "What Is Google Veo 3.1? A Beginner's Guide to AI Video ...",
              "url": "https://skywork.ai/blog/google-veo-3-1-beginners-guide-ai-video-model/",
              "snippet": "If you’ve seen short, cinematic clips made “from a prompt” and wondered how it works, you’re in the right place. This guide will walk you through Google’s latest video generation model, Veo 3.1, in plain English—what it is, why it matters, where you can try it, and a simple first project you can finish in about 10 minutes. Don’t worry if you’ve never touched AI video before. We’ll move step by step and flag common gotchas so you can avoid them.\n\n## What is Google Veo 3.1?\n\nGoogle Veo 3.1 is a text-to-video AI model that turns short written prompts and references into short video clips, with native audio support and more control over storytelling and style. In October 2025, Google announced Veo 3.1 (and a faster variant) in paid preview for developers and creators. According to the Google Developers Blog announcement (Oct 15, 2025), Veo 3.1 is available via the Gemini API in Google AI Studio and Vertex AI, and it adds richer native audio, improved adherence to cinematic styles, and new creative controls.\n\nWhy it matters for beginners: Veo 3.1’s improvements make it easier to describe the mood and motion you want—like “a slow dolly-in on a vintage typewriter with soft rain ambience”—and actually get something close, without wrestling with advanced settings.... ## What’s new compared to earlier Veo versions?\n\nGoogle calls out a few key upgrades in Veo 3.1:\n\n- Richer native audio generation and better narrative control, with improved understanding of cinematic styles (publisher: Google; see the Developers Blog announcement (2025)).\n\n- New creative tools in supporting apps: you can guide generation with multiple reference images, extend existing Veo clips, or bridge between a first and last frame to create transitions. These capabilities are highlighted in the Google Blog on Veo 3.1 and Flow (Oct 2025).\n\n- Native audio emphasis and model positioning are also described on the DeepMind Veo model page, which introduces Veo’s “video, meet audio” concept.\n\nIndependent coverage, like TechCrunch’s report on the Veo 3.1 release (Oct 2025), provides context on rollout and app integration. For hard limits (length, resolution), always defer to Google’s official docs, as those details can change.\n\n## Where can you use Veo 3.1 today?\n\nAs of October 2025:\n\n- Flow (Google’s AI video editor): Google’s announcement notes Veo 3.1 and advanced creative controls in Flow, including “Ingredients to Video” (use multiple images as style/character references), “Frames to Video” (bridge between start/end frames), and “Extend” (lengthen a clip). Details are outlined in the Google Blog on Veo 3.1 and Flow (2025).\n\n- Gemini API (Google AI Studio and Vertex AI): Veo 3.1 and Veo 3.1 Fast are in paid preview via the Gemini API, per the Google Developers Blog announcement (2025).\n\nImportant: Access tiers, regions, and pricing can vary and may change. If you’re not seeing options in your account, check Google’s documentation or support for your account type and region.... ## What can Veo 3.1 generate right now? (Practical limits to know)\n\n- In Vertex AI’s preview for Veo 3.1 base generations, the documented lengths are currently short (choose 4, 6, or 8 seconds). This is specified on the Vertex AI Veo 3.1 preview page.\n\n- In Gemini API extension workflows, you can extend Veo-generated clips. The API docs describe a maximum of up to 141 seconds for input Veo videos in those extension scenarios, with 720p listed in that context. See the Gemini API video documentation (Google).\n\nThese numbers help set expectations: start with short shots, then extend or chain shots together. Avoid assuming 1080p or minute-long base generations unless Google’s official docs explicitly state it for your environment.... ## A 10-minute quickstart: your first Veo 3.1 clip\n\nWe’ll make a simple “coffee shop mood” shot you can adapt for social posts.\n\n- Choose your aspect ratio\n\n- 16:9 (landscape) for YouTube and desktop-first screens\n\n- 9:16 (vertical) for TikTok, Reels, and Shorts Pick one at the start to avoid accidental cropping later.\n\n- Write a clear, concrete prompt Use this structure to get reliable results: subject + action + setting + style + camera + audio.\n\n- Example prompt: “A barista gently places a ceramic cup on a wooden counter; shallow depth of field; warm morning light streaming through windows; slow dolly-in; subtle steam rising; soft cafe ambience, no dialogue, light jazz in the background.”\n\nIf you want a deeper primer on prompt structure and clarity, see these prompt engineering best practices for beginners.\n\n- Generate in your chosen interface\n\n- In Flow: Start a new project, choose Veo 3.1, paste your prompt, set aspect ratio, and generate.\n\n- In Gemini API/Vertex AI: Use the model/version and parameters documented for Veo 3.1 in your environment. Begin with short durations and default settings.\n\n- Review the output like a director\n\n- Does the subject and action match? Is the lighting/mood close?\n\n- Is the camera movement smooth? Is audio what you expected?\n\n- Make one or two changes per iteration—small edits beat big rewrites.\n\n- Refine with controls\n\n- Reference images: If you want consistency (e.g., the same mug or barista style), use up to three reference images to guide the look. This capability is described in Google’s announcements (2025).\n\n- Frames to Video: Provide a starting and ending frame to shape the motion between them.\n\n- Extend: Lengthen your favorite moment to create a longer beat.\n\n- Export Choose the format/aspect ratio you started with. If you need both vertical and horizontal versions, plan to reframe or regenerate with the other aspect ratio rather than cropping aggressively.... ## A practical planning example using Skywork AI (optional, 5 minutes)\n\nSkywork AI can help you prepare the words before you ever hit “Generate.” Disclosure: Skywork AI is our product.\n\nHere’s a neutral, step-by-step way to use it purely for planning:\n\n- Open Skywork and create a new document. Ask for a short video outline: “30-second coffee shop mood piece: 3 shots, warm tone, slow camera.”\n\n- Have it draft a compact shot list with camera moves and audio notes, like:\n\n- Close-up of cup; slow dolly-in; soft steam; light jazz; no dialogue\n\n- Medium barista hands; gentle rack focus; cafe ambience; espresso hiss\n\n- Wide room tone; sunbeams; slow tilt up; footsteps and cups\n\n- Ask for a final prompt assembled from the shot you want to generate first. Copy that prompt into Veo 3.1.\n\nIf you prefer to outline prompts and story beats yourself, this short guide to Skywork’s General Mode for planning and outlining walks through a simple, distraction-free workflow.... ## FAQs for first-time users\n\n\n\nIs Veo 3.1 free? No. Google describes Veo 3.1 and Veo 3.1 Fast as being in paid preview via the Gemini API as of October 2025. See the Google Developers Blog announcement (2025). Pricing varies by product and usage.\n\n\n\nWhere can I try Veo 3.1? Flow (for a visual editor), and the Gemini API via Google AI Studio and Vertex AI (for developers). Google outlines these options in the Veo 3.1 and Flow post (2025) and the Developers Blog announcement (2025).\n\n\n\nHow long can my video be? It depends on the environment and workflow. In Vertex AI’s Veo 3.1 preview, base generations are short (4, 6, or 8 seconds). In Gemini API extension workflows, input Veo videos can be extended up to 141 seconds. See the Vertex AI preview page and the Gemini API video docs.\n\n\n\nWhat resolutions are supported? The Gemini API documentation for extension workflows references 720p in that context. Official materials do not universally guarantee 1080p for base Veo 3.1 generations at this time.\n\n\n\nCan I keep a character or object consistent across shots? Yes. Google notes you can guide Veo 3.1 with up to three reference images for a character, object, or scene. This is described in the Developers Blog announcement (2025).\n\n\n\nCan I add my own audio and sync it? Veo 3.1 focuses on native audio generation controlled via your prompt. The public pages cited here do not detail a full “upload and auto-sync” workflow, so avoid relying on that unless you see it documented for your account.... ## Next steps\n\n- Try a tiny project: one 6–8 second shot with a clear prompt. If you like the result, use “Extend” or stitch a second shot with “Frames to Video.”\n\n- Build a simple storyboard first. If you want help outlining ideas, you can use Skywork AI to draft prompts and shot lists before you generate. Keep it simple: one scene, one action, one camera move.\n\n- When you’re ready to go deeper, explore Google’s official resources: the Veo 3.1 and Flow post (2025), the Developers Blog announcement (2025), the DeepMind Veo overview, the Gemini API video docs, and the Vertex AI Veo 3.1 preview.\n\nYou don’t need special “artistic talent” to start—just one clear sentence and a little patience. Generate, review, tweak, repeat. That’s how everyone learns, and you’ll be surprised how quickly it clicks.",
              "domain": "skywork.ai"
            },
            {
              "position": 8,
              "title": "Google announces Veo 3.1 and Flow updates - 9to5Google",
              "url": "https://9to5google.com/2025/10/15/veo-3-1/?extended-comments=1",
              "snippet": "Google today announced Veo 3.1 as its latest video generation model, with Flow getting a number of updates to take advantage of the latest capabilities.\n\nCompared to Veo 3, which was announced at I/O 2025 in May, this new version offers richer audio and “enhanced realism that captures true-to-life textures.” Veo 3.1 has a deeper understanding of storytelling, cinematic styles, and character interactions to give you more narrative control. The image-to-video capability benefits from improved audio-visual quality and better follows your prompt.\n\nVeo 3.1 and Veo 3.1 Fast are available in the Gemini app, as well as the Gemini API and Vertex AI, to power text-to-video and image-to-video for horizontal (16×9) and vertical (9×16) outputs.\n\nMeanwhile, Google is updating the Flow filmmaking tool to take advantage of Veo 3.1. Audio generation is coming to:\n\n**Ingredients to Video**: This can include images of multiple characters, objects, and styles **Scene extension**: You can Extend an existing clip, with Google basing it on the last second **Frames to Video**: This lets you specify the starting and ending images, with Flow bridging the two\n\nFlow is getting new editing capabilities. You can insert elements like objects, characters, and details, with Google handling shadows, scene lighting, and other complex details to make everything look natural.\n\nComing soon is the ability to remove objects and characters from a scene, with Flow working to reconstruct the background and surroundings to make the edit seamless.\n\n\n\n## Comments",
              "domain": "9to5google.com"
            },
            {
              "position": 9,
              "title": "Google rolls out its new Veo 3 video-generation model ...",
              "url": "https://techcrunch.com/2025/07/03/google-rolls-out-its-new-veo-3-video-generation-model-globally/",
              "snippet": "Google on Thursday said it has begun rolling out its Veo 3 video generation model to Gemini users in more than 159 countries.\n\nVideo generation via the new model is available only to paying subscribers of Google’s AI Pro plan and is capped at three videos per day.\n\nVeo 3, which Google showed off in May, lets users generate videos up to eight seconds long using text prompts.\n\nGoogle’s Josh Woodward has said that the company is working on adding image-to-video generation capabilities to Gemini.",
              "domain": "techcrunch.com"
            },
            {
              "position": 10,
              "title": "Google's Veo 3.1 is better at generating videos from images",
              "url": "https://www.engadget.com/ai/googles-veo-31-is-better-at-generating-videos-from-images-220829129.html",
              "snippet": "Google has released a new update to its Veo AI video generation model that should make it do a better job of sticking to prompts and converting images into videos. Veo 3.1 is available to try today through Google's Gemini API and is now also powering the company's Flow video editor.\nVeo 3.1 builds on the new capabilities Google introduced with launch of Veo 3 at Google I/O 2025. The new model offers better \"prompt adherence,\" according to Google, and should have an easier time creating videos based on the image \"ingredients\" you upload alongside your written prompt. Veo 3.1 also makes it possible to convert images to video and generate audio at the same time, a capability that wasn't available with Veo 3.\nIn Flow, Veo 3.1 supports at least a new feature that gives you finer control over the videos you generate. With what Google calls \"Frame to Video,\" Flow lets you upload a first and last frame, and then generates the video in-between. Adobe Firefly, which is powered by Veo 3, offers a similar feature, but Flow will be able to pull it off and create audio at the same time. Those added audio skills will also apply to the video editor's ability to extend clips and insert objects into existing footage, too.\nBased on the samples Google's shared, videos generated with Veo 3.1 still have an uncanny quality that seems to vary greatly depending on the prompt and subject. Even if it's missing some of the realism of OpenAI's Sora 2, though, the company's decision to try and make Veo more useful to people who actually work with video rather than a source of social media spam is a welcome move.",
              "domain": "www.engadget.com"
            }
          ],
          "success": true,
          "error": null
        },
        {
          "query_id": "q8",
          "query": "Google veo3.1 release 2025 debunked",
          "claim_id": "claim_1",
          "query_type": "contradiction",
          "priority": "high",
          "results": [
            {
              "position": 1,
              "title": "VEO 3 is UNREAL...it might actually take my job",
              "url": "https://www.youtube.com/watch?v=Xj4BDwgEwd4",
              "snippet": "{ts:0} VO3 is here and it's shocked the entire industry. Uh, Alex, what was that? It's\n{ts:8} nothing. Don't worry about it. Are you trying to replace me with AI? What? No, I'm just trying something new. Just do\n{ts:16} the normal intro. It It's fine. All right. V3 just dropped and I'm going to show you so many incredible examples of\n{ts:24} V3 in action. Let's get into it. Oh, and Alex, don't do that again. Anyways, let's get into it. All right. I've been\n{ts:32} seeing a ton of these street style interviews, hyper realistic, where someone is interviewing people on the\n{ts:38} street who kind of just stumbled out of a bar. They all are either a little bit tipsy or drunk, and V3 has been\n{ts:45} recreating these so well. So, here are two that I've made. The dialogue between them didn't exactly match my prompt, but\n{ts:53} I'll show you some others where it's pretty compelling. So, this is street interview. Hyperrealistic guy\n{ts:59} interviewing two girls, all Gen Z. They just came out of a bar kind of tipsy drunk. The interviewer asks,... \"Okay,\n{ts:65} low-key, can you believe we aren't real?\" Then girl one says, \"I don't know about you, honey. I'm 100% real.\" In a\n{ts:72} sassy attitude, \"Please do not clip that.\" And girl two says, \"Yeah, yeah, bet. We're as real as they come.\" Almost\n{ts:78} interrupting the first girl. Okay, so here's the first video. Okay, low key, can you believe we aren't real? I don't\n{ts:85} know about you, honey. I'm 100% real. Yeah. Yeah. Yeah. Bet. We're as real as they come. All right. And then here's\n{ts:90} the second generation, the second version that V3 created. Okay. Low key. Can you believe we aren't real? I don't\n{ts:96} know about you, honey. I'm 100% real. Yeah. Yeah. Bet. We're as real as they come. All right. So, in this next one, I\n{ts:103} tried to get two trains heading towards each other. They smash into each other. Huge explosion. And for some reason, I\n{ts:110} just could not get it to work. But let me show you what did generate. So, here was my first attempt. Two massive trains... {ts:255} second one, the hand looks better. That's weird. And it kind of just changes\n{ts:263} frames for a second. But uh yeah, either way, that is not what's in a Rubik's cube. Look at this. It actually looks\n{ts:269} really cool. Look at all of these detailed gears inside the Rubik's Cube. All right, but of course you are all\n{ts:275} familiar with this meme. And what if you get V3 to make a video out of it? Are you serious right now? I can't\n{ts:284} believe you. You are unbelievable. Do you have an idea that you've been putting off for a\n{ts:290} while because you don't have the technical knowledge to turn it into reality? With Hostinger Horizons, this\n{ts:296} is now possible. Hostinger just launched Hostinger Horizons, which is the easiest way to launch full applications with no\n{ts:305} code. This is vibe coding, but even easier because the deployment happens automatically. Hostinger Horizons is an\n{ts:312} all-in-one solution. Manage hosting, domains, and email all in one place while being able to take your idea from... {ts:459} kind of wanted to see if it could nail the horror vibe with like aliens in a dark alley. So, check this out.\n{ts:469} I think the only thing that it didn't do super well in this video is the sound effects are just okay. If we look over\n{ts:475} at the second one, it did a way better job with the sound effects. Yeah, I mean that's great. Even\n{ts:483} when the alien kind of like walks in front of the light, it covers the light on the ground. That's pretty dang... 't know\n{ts:558} what VO was doing. All right, Matt, back to you. But remember, Flow from Google, which houses VO3, is not just about\n{ts:566} creating 7-second clips. You should be able to create entire videos from it. It basically allows you to take these clips\n{ts:572} and put them together in really unique ways. Here's one that is if Jurassic Park were actually real. Check this out.\n{ts:578} We are on our way to Jurassic Park. I am so excited. I've always wanted to go and finally we are doing it. It's going to\n{ts:584} be great. Right, Jason? Yeah. I can't wait. Okay, bye. All right, we're [Music]\n{ts:596} here. Got the dinosaurs. Got all the people watching. I mean, everybody looks really good. There's no limbs coming out\n{ts:602} of them. This dinosaur doesn't look fantastic. All of these dinosaurs don't look hyper realistic. I think they could\n{ts:608} have done a little bit better of a job there. It looks like animatronics to be honest, but still. This is all from AI... {ts:742} screen. Yeah. And the crazy thing about this thing is it has All right, so this is one thing I've noticed with these V3\n{ts:751} videos. Whenever it has a human talking, there's always these awkward pauses. It's just a half second too long of a\n{ts:757} pause and that's where you really can see it's AI generated, but most of the time you can't. In fact, you know that\n{ts:765} opening clip that we did in this video, I showed that to my wife and said, \"Somebody copied our channel and I just\n{ts:771} showed it to her and didn't say anything else.\" And she looked at it and she was like, \"Oh, oh, that sucks.\" And then I\n{ts:777} had to tell her it was AI because it looked that real. All right, let's keep watching. Heated\n{ts:785} seats. Check this out. Look at that. Just slice right through. The N9 portable fusion reactor is small and\n{ts:794} almost meltdown free. Almost. This is the best flying experience\n{ts:804} ever. These controls are amazing. All right, so I thought that was really good. But it... 's it's pretty good. The first\n{ts:1126} time that I watched it, I did actually laugh out loud. That was actually pretty funny. Next, I just asked it to make a\n{ts:1132} detailed look at the solar system. And yeah, I think it did an all right job. Let's check out the second\n{ts:1140} clip. Now, this one I think is a bit better, but V3 kind of likes to put objects in front of the camera when it's\n{ts:1148} backing out like that right there. And it kind of just like spawns a planet right in front of the camera. And yeah,\n{ts:1154} it doesn't look the best, but honestly, it's pretty good. All right, back to Matt. All right, next. I wanted to see\n{ts:1161} if it could create the game Portal, but hyperrealistic. Essentially, what I wanted it to do is what I say here. A\n{ts:1167} mediumbuild man in his 30s wearing a futuristic tactical suit with glowing blue accents stands in a dimly lit\n{ts:1173} industrial room with exposed pipes, metal walls, and flickering lights. He holds a sleek high-tech portal gun with... {ts:1233} in terms of just visuals looks incredible. Although it's not what I asked\n{ts:1242} for. Yeah. So, pretty good. You only saw his reflection through the portal for about a frame or two, but it wasn't that\n{ts:1250} good. Anyways, but the visuals again, the visuals, the detail, the clarity, all really, really impressive. All\n{ts:1257} right, next. Meta Puppet made a video called This is Plastic made with VO3. Spoilers in next post. Watch before\n{ts:1264} reading. So, this is a 2 minute 45 second video. Quite long. I'm not going to play it in full. I'm going to skip\n{ts:1269} around a little bit, but let me show you. Studies have revealed that microplastics are being found in human\n{ts:1274} testicles, raising concern. You can never trust these studies on male reproductive health.\n{ts:1279} [Music] Okay, that is hilarious. And remember, all of this was put together using Flow\n{ts:1291} Plus V3. These are both Google products. So, you have a little plastic baby. God, that",
              "domain": "www.youtube.com"
            },
            {
              "position": 2,
              "title": "I was wrong - AI video is nuts (don't sleep on Veo 3)",
              "url": "https://www.youtube.com/watch?v=_3PCta2uyvc",
              "snippet": "## Theo - t3․gg\n##### May 26, 2025 (0:16:08)\nI severely underestimated Google's Veo 3 model. The output quality is insane, we need to talk about this...\n\nThank you Imagekit for sponsoring! Check them out at: https://soydev.link/imagekit\n\nUse code VEO for 1 month of T3 Chat for just $1: https://soydev.link/chat\n(only valid for new customers)\n\nSOURCES\nhttps://x.com/ArtificialAnlys/status/1925159679824744804\nhttps://x.com/ArtificialAnlys/status/1925549565303763269\nhttps://x.com/theo/status/1925125767371149823\nhttps://x.com/theo/status/1925134963978207319\n\nWant to sponsor a video? Learn more here: https://soydev.link/sponsor-me\n\nCheck out my Twitch, Twitter, Discord more at https://t3.gg\n\nS/O Ph4se0n3 for the awesome edit 🙏... {ts:0} I just did a video about Google IO, but I missed something. I thought the video\n{ts:4} model was mediocre. I was wrong. Pretty nuts for a oneshot, right? Like, I just generated that trivially. It still costs\n{ts:13} 250 bucks a month to use any of this right now. And the UI is garbage and it's annoying as hell to use. But the\n{ts:18} quality of what you can get out of V3 is significantly better than I thought. My tests were bad. I didn't look into it\n{ts:25} enough. And I'm making this video both because I was wrong for not better covering it, but also because I found it\n{ts:32} actually very, very fun to play with and I wanted to share with you guys. That all said, I've already burned through\n{ts:37} most of the credits I get for the $250 and I want more. So, quick break from today's sponsor and then we'll get right\n{ts:44} to it. I've been a webdev for a while and one of the most annoying things to get right is images. Seriously, I can't\n{ts:50} believe I... {ts:265} other people doing demos with it. like, \"Wait, it can do that much?\" I went and played more. There was a lot of edges\n{ts:270} that I had to get through. The biggest one being the Flow website, which is so bad. We'll go over some of the ways it's\n{ts:276} bad in just a bit. I was trying to prompt it to look like me back when I still had the blonde hair and mustache,\n{ts:280} and it came out looking like Prime. But another test, I tried this one like eight times, and this is the best I\n{ts:285} could do. Something caused the first still to look awful. I don't know why it's like that. None of the rest had\n{ts:291} that problem. Once you It plays, it's fine, but you'll notice some details on this one.\n{ts:296} Use code VEO at checkout for one month free on T3 Chat. Yeah, it isn't great at\n{ts:305} text. It tried, but it's not great at it. You need to give it a very small amount of text to render. And even if\n{ts:311} you tell it to not put in subtitles, it just will sometimes. The free month code included there did work, but we... {ts:372} made the mistake of here is I assumed when you do frames to video and you give it a frame that you've saved that it\n{ts:380} would still use the thing you selected because if you do ingredients to video and you select something for it to start\n{ts:386} and you try to submit it with V3 selected, it will fail. It says in the corner here and I need it on full screen\n{ts:392} for you to see it. Switching you to a compatible model for this feature. Submit again to confirm or check\n{ts:397} settings for details. I wish it told me where in settings to check. I don'... {ts:561} was wrong. I just Yeah, it's the weird breath at the end. Cool. Stop it there. Then we will extend it and say make sure\n{ts:570} we're on the right model because again it keeps changing back to V2 even though this is the VO3 clip I'm trying to\n{ts:575} extend. I almost want to try it so you can see how much worse it is in comparison. Switching you to a\n{ts:580} compatible model for this feature. Submit again to confirm. Look at that. You can't even use it on V2 quality. It\n{ts:587} bumps you to fast. There's so much potential here and just none of it's being realized because\n{ts:594} this UI is awful. It it tricked me into thinking this was all much worse than it actually is. I wish they just gave us\n{ts:600} the model in a more reasonable like shape for us to play with and consume. But V3 is not on the API yet. There's no\n{ts:606} way for us to use any of it yet. So sorry T3 chat can't add this. But despite all of that, it's still just an\n{ts:613} incredible model. Do you know what... 's even better than this spaghetti? T3 chat. Like what? What do\n{ts:625} you guys remember like a year and a half ago how far we were from Will Smith eating spaghetti? It's not Will Smith,\n{ts:632} but that is absolutely spaghetti being eaten. It's kind of crazy where that's all at. Google doesn't know how to make\n{ts:640} creative tools or really power tools in general. They make decent enough consumerf facing software. They make\n{ts:647} decent enough infrastructure and they make incredible models in generative tools, but they don't know how to make\n{ts:653} like a good video editor. If you don't believe me, go try the one they built for YouTube. It's it's interesting. It's\n{ts:658} a it's often cited as a good example of a Flutter app. If you can predict what that means for the quality of\n{ts:665} experience, but the model here is so good. And once again, what I'm excited about is what people will do with this\n{ts:672} tool. But I'm also a bit terrified because this looks better than some like iPhone video. I see things like\n{ts:681} verifying your identity just got a lot sketchier because if I... t going to trust it as much. This is going to really change our like trust vectors for\n{ts:839} what is or isn't real. I don't even know now how I will be able to tell if a given video that is sent to me is real\n{ts:844} or not because this stuff is actually that compelling. And if somebody makes a less restricted version of this model or\n{ts:850} gets something close to this in the open source world or with stable diffusion, I'm scared. I'm legitimately scared. You\n{ts:858} are telling me to try again generating with my blurred photo. I'll be more specific. Clean shaven white\n{ts:866} man. Be sure to include the audio of him speaking. Make sure it's still V3. Yep. Cool. Let's see how it does. Switching\n{ts:875} you to a compatible model. So, it's too fast. Not even quality. Yeah, you can't do it. you you can't do anything but\n{ts:883} text the video for V3 right now, which I'm pretty sure is a safety thing just due to the nature of what this model is\n{ts:890} capable of. And as we've now seen, and I can show more examples of the gap between two and three is a bit... {ts:898} absurd. This is one I accidentally did with two. You can see the audio doesn't exist. It got the text okay there, but\n{ts:905} it went a little absurd with the subtitles. This one was really funny. It feels like a Bollywood\n{ts:915} movie. The way the T3 chat fades into the screen is so hilarious. Yeah, this is why I didn't care because none of the\n{ts:923} video models have felt like a significant improvement from that to this point. I did not realize how absurd\n{ts:930} this got, especially with how bad the UX is. Like I hit the upscale button cuz when you download, you can choose what\n{ts:937} format you want to download in. If it's not frozen, which it was there for a sec. You can pick animated GIF,\n{ts:942} original, or upscaled. Upscale just doesn't work. I've been sitting here waiting for this to upscale for like an\n{ts:947} hour now, and it just hangs forever. It does say this can take a few minutes, but like what's a few minutes, Google?\n{ts:956} It's been an hour. Yeah. What did you think? Is this exciting or scary? Until next time, peace nerds.",
              "domain": "www.youtube.com"
            },
            {
              "position": 3,
              "title": "Veo (text-to-video model) - Wikipedia",
              "url": "https://en.wikipedia.org/wiki/Veo_(text-to-video_model)",
              "snippet": "**Veo**, or **Google Veo**, is a text-to-video model developed by Google DeepMind and announced in May 2024. As a generative AI model, it creates videos based on user prompts. Veo 3, released in May 2025, can also generate accompanying audio.\n\n## Development\n\nIn May 2024, a multimodal video generation model called Veo was announced at Google I/O 2024. Google claimed that it could generate 1080p videos over a minute long. In December 2024, Google released Veo 2, available via VideoFX. It supports 4K resolution video generation and has an improved understanding of physics. In April 2025, Google announced that Veo 2 became available for advanced users on the Gemini app.\n\nIn May 2025, Google released Veo 3, which not only generates videos but also creates synchronized audio — including dialogue, sound effects, and ambient noise — to match the visuals. Google also announced **Flow**, a video-creation tool powered by Veo and Imagen. Google DeepMind CEO Demis Hassabis described the release as the moment when AI video generation left the era of the silent film.... ## Capabilities and limitations\n\nGoogle Veo can be bought by several subscription/membership tiers, and/or by using Google \"AI credits\". The software itself can be run by two different consoles called Google Gemini and Google Flow, with Gemini being geared towards shorter, quicker, and faster projects, using the Gemini AI chat model, or through Google Flow, which is essentially a movie editor, as well, allowing users to create longer projects, and continuity using the same characters and actors. Users can create a maximum length of eight seconds per clip.\n\nGoogle Veo, has a relatively simple interface and dashboard, however writing prompts, for those who have little to no experience in transcribing or filmmaking may face issues with the software misunderstanding what the user intended by their prompt (no matter how detailed it was). So although Veo does have a friendly and simple setup, prompts, which are the forefront of the software, need to be not only short and to the point, but they also must be very specific, if the user wants the right vision for their project. Google Veo, when it comes to human models, is able to generate several ethnicity and body types. The software is also capable of generating stand up comedy routines, and Music videos. It can as well generate animals, cartoons, and animation. Prompts must accurately describe places, people, and things in each scene, in addition knowledge of film and camera lingo such as panning, zooming, and terms for camera angles, are also important.... Google Veo however, has strict guidelines and blockades to their software. Before a clip is generated, the algorithm computer software reviews it, and if it is anything deemed inappropriate, too graphically sexual, illegal, showcasing graphic abuse/assault/fighting (unless the prompt specifies that it is a fictitious martial arts scene etc.) gross behaviors, antisemitism, racist, homophobic, anything depicting reigning regimes, rioting, blood, gore, or warfare, (unless in some cases the prompt specifies that it is fictitious period drama, the clip may still be generated), the clip will not be generated. In addition, Google Veo cannot and will not generate character actors that look identical to celebrities or real-life individuals. Users have primarily complained that, regardless of how descriptive and detailed their prompts are, Google Veo often misunderstands the input, resulting in completely different outputs. Common issues include the emulation of incorrect subtitles and captions, the generation of complex scenes that are incomplete due to the maximum eight-second length, the production of garbled and nonsensical speech, and character models that appear deformed in both appearance and movement. Users have also reported that their prompts and generated content are falsely flagged as violating guidelines, along with a variety of other issues and complaints. However, trial and error may have to be used with Veo for optimal results.... ## Reactions\n\nA reporter for *Gizmodo* reacted to the release of Veo 3 by observing that users were directing the model to generate low-quality content, such as man on the street interviews or haul videos of people unboxing products. Another media commentator reported that the tool tended to repeat the same joke in response to different prompts.\n\nCommentators speculated that Google had trained the service on YouTube videos or Reddit posts. Google itself had not stated the source of its training content.\n\nIn July 2025, Media Matters for America reported that racist and antisemitic videos generated using Veo 3 were being uploaded to TikTok. Ryan Whitwam of *Ars Technica* commented, \"In a perfect world, Veo 3 would refuse to create these videos, but vagueness in the prompt and the AI's inability to understand the subtleties of racist tropes (i.e., the use of monkeys instead of humans in some videos) make it easy to skirt the rules.\"\n\n## See also\n- Sora (text-to-video model)\n- VideoPoet – Text-to-video model by Google\n- Dream Machine (text-to-video model)\n\n## References\n\n## External links\n- Official website\n- *Introducing Veo 3.1 and advanced capabilities in Flow*\n\nCategories: - 2024 software\n- Applications of artificial intelligence\n- Film and video technology\n- Google DeepMind\n- Text-to-video generation\n- Video processing\n- Generative artificial intelligence\n- 2024 in artificial intelligence",
              "domain": "en.wikipedia.org"
            },
            {
              "position": 4,
              "title": "Google's Veo 3 Has People Crashing Out Over AI Slop",
              "url": "https://gizmodo.com/googles-veo-3-has-people-crashing-out-over-ai-slop-2000608803",
              "snippet": "Depending on who you ask, generative AI is either a thrilling tech revolution or an existential threat, and there's little in-between. It's hard to blame anyone for an extreme reaction, too, given the magnitude of capital investment, hyperbolic marketing, and rapid progress of generative AI in such a short amount of time. But it's not just the economics and technical feats of AI that have people losing their minds; there's also something more philosophical percolating, and it's driving some people 'to the brink.'\nPeople are literally having a mental breakdown over Veo-3 pic.twitter.com/ym5oZDYZGr\n— Chubby♨️ (@kimmonismus) May 27, 2025\nThe latest AI advancement to send people down an existential rabbit hole comes courtesy of Google, which just announced its latest video generation model called Veo 3. As I've reported a few times now, Veo 3 is already getting into some wild stuff—turning up the dial on AI slop, deepfaking smooth-brained YouTube content, and potentially upending game development, to name a few things. As it turns out, people are taking note of all of those feats, and some of them are not exactly happy about what they see.... As evidenced by a thread from the subreddit r/artificialintelligence posted this week titled 'VEO3 is kind of bringing me to a mental brink. What are we even doing anymore?' Google's Veo 3 and the implications therein have some people spiraling. 'I'm just kind of speechless. The concept of existential crisis has taken a whole new form. I was unhappy with my life just now but thought I can turn it around, but if I turn it around, what is left of our world in 2 decades?' the post's author writes.\n'Actors as a concept are gone? Manually creating music? Wallpapers? Game assets? Believing comments on the internet are from real people? AI edited photos are just as real as the original samples? Voicenotes can be perfectly faked?… Literally what value is being left for us?'\nReactions to the thread are mixed, with suggestions that the author should go 'touch grass' or maybe 'go to therapy,' but there's also a chorus in agreement. The consensus from the latter group? AI slop is coming to ruin your art, and there's not much we can do about it.\nI, for what it's worth, fall unhelpfully in between the two camps. I think there is a deluge of AI slop incoming, and, if we're being honest, we're already up to our ankles. Between Veo and OpenAI's Sora and the clear interest in automating human creativity, I think we can reasonably buckle in and expect the world of movies, music, and entertainment writ large to get a little choppy. Whether any of those efforts to automate entertainment will stick is less obvious. The thing about art is that the kind that people tend to like is the kind that has something substantial to say. Right now, for all of its mimicry, generative AI doesn't actually have anything to say, because technically all it can do is remix and repeat.... I did more tests with Google's #Veo3. Imagine if AI characters became aware they were living in a simulation! pic.twitter.com/nhbrNQMtqv\n— Hashem Al-Ghaili (@HashemGhaili) May 21, 2025\nCall me an optimist, but most people can likely sniff out the difference between slop and art, and as much as studios would love to wave a magic wand and rid themselves of human creatives and the cost of their labor, deep down they know that they'd have to Ctrl+Z that move just as fast. That's not to say there won't be casualties in the AI age—if there's one lesson we can learn from mass waves of automation in years past, it's that labor forces are usually the most affected.\nBut when it comes to art, things aren't so simple. Art, at least the good kind, is about human connection, and until AI can think and feel like we do, there's nothing that can replace that. So, before you crash out over AI slop, just remember: AI still thinks putting glue on your pizza is a good idea, so we may have a few more good years left in the tank.",
              "domain": "gizmodo.com"
            },
            {
              "position": 5,
              "title": "What to Expect Veo 3.1 Google's Next-Gen Video AI Release",
              "url": "https://gptproto.com/blog/veo-3-1",
              "snippet": "The AI video generation space is heating up. Following recent improvements to Veo 3 announced in September 2025, early signs suggest Google DeepMind is preparing to launch Veo 3.1 on October 10, 2025. While Google hasn’t officially confirmed the release, leaked information from various sources points to significant upgrades that could reshape how creators make video content. This incremental update appears designed to keep pace with OpenAI’s Sora 2, which launched just last month with impressive capabilities.\n\nIf you’re a content creator, filmmaker, or developer wondering whether this matters for your workflow, here’s what the leaks suggest about tomorrow’s release:\n\n- Ultimate character consistency that solves the morphing problem\n\n- Native 1080p output with professional cinematic presets\n\n- Extended video generation up to 60 seconds\n\n- Multi-prompting for creating connected scene sequences\n\n- Better prompt understanding and motion quality\n\n- Access through unified platforms like GPT Proto\n\n## Understanding Veo 3.1\n\nVeo 3.1 represents the latest evolution of Google DeepMind’s video generation technology. Building on the foundation of Veo 3, which introduced native audio generation in May 2025, this new version focuses on solving practical problems that creators face daily. Think of it as Google’s answer to the competitive pressure from OpenAI and other players in the AI video market.\n\nThe timing is strategic. OpenAI released Sora 2 on September 30, 2025, with features that emphasize physical realism and multi-scene storytelling. Google appears ready to counter with its own improvements that address similar needs while leveraging its existing infrastructure across Gemini and Vertex AI.... ## How We Know About Veo 3.1\n\nThe first hints appeared on Higgsfield AI’s waitlist page, where references to Veo 3.1 surfaced unexpectedly. Shortly after, developers noticed the model name in Vertex AI’s internal codebase, suggesting Google was preparing for deployment. Community discussions on platforms like Reddit and Discord added fuel to the fire, with some users claiming they received early access invitations.\n\nNone of this constitutes official confirmation from Google, but the pattern matches previous releases. Veo 2 and Veo 3 both followed similar leak patterns before their official announcements. The lack of denial from Google has only increased speculation that tomorrow’s launch is real.... ## Major Feature Upgrades Based on Leaks\n\n### Character Consistency Finally Solved\n\nOne of the biggest complaints about current AI video models is character morphing. You start with someone who has brown hair, and by second four, they suddenly have blonde hair. Leaked information suggests Veo 3.1 tackles this head-on with what insiders call “ultimate character consistency.” The system reportedly maintains facial features, clothing, and physical characteristics throughout the entire clip without the annoying shifts that break immersion.\n\n### True 1080p with Cinematic Presets\n\nWhile Veo 3 added 1080p support in September, Veo 3.1 apparently takes this further with built-in cinematic presets. Instead of just getting higher resolution, you’ll have options for film noir, sci-fi, documentary style, and other professional looks right out of the box. This matters because it reduces the post-production work needed to achieve a polished result.\n\n### Longer Videos Change the Game\n\nCurrent limits on Veo 3 cap most generations at eight seconds. Leaked specs indicate Veo 3.1 will support videos up to one minute in length. That’s a huge jump. Sixty seconds gives you enough time to tell an actual story rather than just showing a moment. Think about what you can convey in a full minute versus eight seconds. It opens up entirely new use cases.... ## Technical Improvements Under the Hood\n\nBeyond the flashy new features, Veo 3.1 apparently includes substantial technical refinements. Better prompt understanding means the model actually gets what you’re asking for without requiring perfect technical language. Reduced morphing artifacts should eliminate those weird transitions where objects or people briefly distort.\n\nImproved motion quality addresses another common issue where movement looks unnatural or jerky. Enhanced character tracking across scenes ties into the consistency improvements, ensuring people and objects remain recognizable as they move through your video.\n\nThese aren’t the kinds of upgrades that make for exciting marketing copy, but they’re what separate a tool you use once out of curiosity from one that becomes part of your regular workflow.\n\n## How Veo 3.1 Stacks Up Against Competitors\n\nThe AI video generation market is crowded right now. OpenAI’s Sora 2 emphasizes physical accuracy and realistic dialogue synchronization. Runway’s Gen-3 focuses on speed and iteration. Pika Labs carved out a niche with its editing capabilities. Where does Veo 3.1 fit?\n\nGoogle’s advantage has always been infrastructure and integration. Veo works natively with Gemini’s language models for better prompt understanding. It’s built into Google Cloud through Vertex AI, making it accessible for enterprise applications. The company can leverage its massive compute resources for faster generation times.\n\nIf the leaks are accurate, Veo 3.1 will challenge Sora 2 directly on multi-scene generation while maintaining better integration with existing Google services. For developers and businesses already using Google Cloud, this matters more than raw feature comparisons might suggest.... ## What This Means for Content Creators\n\nAssuming tomorrow’s release happens as rumored, Veo 3.1 could genuinely change how people approach video production. Not by replacing human creativity, but by removing bottlenecks that slow down the creative process.\n\nNeed B-roll footage for a documentary but don’t have the budget for a full shoot? Generate it. Want to prototype a music video concept before investing in production? Mock it up. Testing different visual styles for a client presentation? Try five approaches in an hour.\n\nThe workflow implications extend beyond just making videos faster. When you can quickly test ideas, you take more creative risks. When consistent characters across longer sequences become possible, narrative projects become viable. When you don’t need to manage multiple tools and APIs, you spend more time creating and less time troubleshooting.\n\n## Accessing Veo 3.1 Through GPT Proto\n\nHere’s where platforms like GPT Proto become relevant. Rather than dealing directly with Google’s Vertex AI setup, GPT Proto offers unified access to multiple AI models through a single API. When Veo 3.1 launches, developers using AI API Service should be able to integrate it without major code changes.\n\nThe platform handles the infrastructure complexity, providing stable connections and reliable access without requiring you to become an expert in each model’s specific implementation. For solo developers and small teams, this matters enormously. You can use Veo 3.1 alongside GPT models for text generation, image AI tools, and other capabilities without juggling multiple API keys and billing systems.\n\nGPT Proto’s pay-as-you-go model also removes the pressure of monthly subscriptions. You use what you need when you need it, making it easier to experiment with Veo 3.1 without financial commitment.... ## Predictions for Launch and Beyond\n\nAssuming the October 10 launch happens, expect Google to follow its previous pattern. The first two Veo generations offered free tiers with usage limits. Veo 3.1 will likely continue this approach, giving people a chance to test the technology before committing to paid plans.\n\nIntegration with other Google AI tools seems certain. Expect Veo 3.1 to work seamlessly with Imagen 4 for image generation and Gemini models for prompt enhancement. Google’s strength has always been ecosystem integration, and this release should reinforce that advantage.\n\nFuture features probably include longer video generation, real-time editing capabilities, and better control over specific elements within scenes. The competitive pressure from OpenAI and others ensures that development will continue at a rapid pace.... ## Final Thoughts\n\nWhether Veo 3.1 launches tomorrow as rumored or arrives later with different features, the direction is clear. AI video generation is moving from experimental toy to practical tool. Google’s focus on character consistency, longer videos, and multi-scene generation addresses real creator needs rather than just chasing technical benchmarks.\n\nThe broader significance lies in democratization. Professional video production has always required significant resources. Not just money for equipment, but time for learning complex software and expertise for managing all the technical details. Tools like Veo 3.1 lower these barriers without eliminating the need for creative vision and storytelling skill.\n\nHaving unified access through AI API Platform makes adoption even easier. When you can test multiple AI tools through one interface without complex setup, experimentation becomes natural rather than intimidating.",
              "domain": "gptproto.com"
            },
            {
              "position": 6,
              "title": "Report says Google is about to release VEO 3.1 version on Gemini ...",
              "url": "https://www.aibase.com/news/21990",
              "snippet": "# Report says Google is about to release VEO 3.1 version on Gemini and API\n\n#### AIbase基地Published inAI News · 4 min read · Oct 15, 202518\n\nRecent reports indicate that the public release of VEO3.1 is imminent for Google. With related disclaimers appearing in the Gemini application, Google is showcasing the features of VEO3.1 to a broad user base, which may be implemented within the familiar Gemini interface.\n\nThe post shared by prominent figure Logan Kilpatrick in the community on the social media platform X is widely seen as an early confirmation of Google's new AI product release. Additionally, references to preview models such as \"VEO3.0Generate\" and \"VEO3.0Fast Generate\" have appeared in Vertex AI, indicating that Google is offering multiple access channels for early users and enterprises, in line with their traditional strategy in the release of video generation tools.\n\nDiscussions in the community about output duration are intense, with evidence suggesting that video length may be extended from the previous 8 seconds to 30 seconds, although this remains to be confirmed. Previous leaks indicated that the fast mode has lower quality, while the standard mode is expected to unlock higher output quality, which is particularly important for creators looking to enhance visual quality and narrative potential. According to TestingCatalog, previous 720p video generation has shown improvements in VEO3.1, including new audio capabilities and enhanced visual effects, making it a competitor to similar products like Sora2.\n\nGoogle's overall product strategy is to position Gemini as a central workspace, with VEO models integrated for use by both consumers and enterprise users. Through the preview release on Vertex AI, enterprises can try generating videos, while the mainstream promotion of the Gemini application can reach regular users. This phased release strategy not only maximizes developer feedback but also promotes public acceptance, aligning with Google's ongoing efforts to bridge functional gaps and solidify its position in the generative media field.\n\nKey Points:\n\n🌟 VEO3.1 is about to be released, allowing users to experience its new features in the Gemini application.\n\n🎥 Video duration may be extended from 8 seconds to 30 seconds, providing creators with more narrative space.\n\n🚀 Google's phased release strategy maximizes developer feedback and public acceptance.... #### This article is from AIbase Daily\n\nWelcome to the [AI Daily] column! This is your daily guide to exploring the world of artificial intelligence. Every day, we present you with hot topics in the AI field, focusing on developers, helping you understand technical trends, and learning about innovative AI product applications.",
              "domain": "www.aibase.com"
            },
            {
              "position": 7,
              "title": "Release notes | Gemini API | Google AI for Developers",
              "url": "https://ai.google.dev/gemini-api/docs/changelog",
              "snippet": "This page documents updates to the Gemini API.\n\n## October 17, 2025\n\n**Grounding with Google Maps**is now generally available. For more information, see Grounding with Google Maps documentation.\n\n## October 15, 2025\n\nReleased Veo 3.1 and 3.1 Fast models in public preview, with new features including:\n\n- Extending Veo-created videos.\n\n- Referencing up to three images to generate a video.\n\n- Providing first and last frame images to generate videos from.\n\nThis launch also added more options for Veo 3 output video durations: 4, 6, and 8 seconds.\n\nDeprecation for\n\n`veo-3.0-generate-preview`and\n\n`veo-3.0-fast-generate-preview`coming October 22, 2025.\n\n## October 7, 2025\n\n- Launched Gemini 2.5 Computer Use Preview\n\n## October 2, 2025\n\n- Launched Gemini 2.5 Flash Image GA: Image Generation with Gemini\n\n## September 29, 2025\n\n- The following Gemini 1.5 models are now deprecated:\n\n`gemini-1.5-pro`\n\n`gemini-1.5-flash-8b`\n\n`gemini-1.5-flash`... ## September 9, 2025\n\n- Launched Veo 3 and Veo 3 Fast GA, with lower pricing and new options for aspect ratios, resolution, and seeding. Read the Veo documentation for more information.\n\n## August 26, 2025\n\n- Launched Gemini 2.5 Image Preview, our latest native image generation model.\n\n## August 18, 2025\n\n- Released URL context tool to general\n\navailability (GA), a tool for providing URLs as additional context to\n\nprompts. Support for using URL context with the\n\n`gemini-2.0-flash`model (available during experimental release) will be discontinued in one week.\n\n## August 14, 2025\n\n- Released Imagen 4 Ultra, Standard and Fast models as generally available (GA). To learn more, see the Imagen page.\n\n## August 7, 2025\n\n`allow_adult`setting in Image to Video generation are now available in restricted regions. See the Veo page for details.\n\n## July 31, 2025\n\n- Launched image-to-video generation for the Veo 3 Preview model.\n\n- Released Veo 3 Fast Preview model.\n\n- To learn more about Veo 3, visit the Veo page.... ## July 22, 2025\n\n- Released\n\n`gemini-2.5-flash-lite`, our fast, low-cost, high-performance Gemini 2.5 model. To learn more, see Gemini 2.5 Flash-Lite.\n\n## July 17, 2025\n\nLaunched\n\n`veo-3.0-generate-preview`, the latest update to Veo introducing video with audio generation. To learn more about Veo 3, visit the Veo page.\n\nIncreased rate limits for Imagen 4 Standard and Ultra. Visit the Rate limits page for more details.\n\n## July 14, 2025\n\n- Released\n\n`gemini-embedding-001`, the stable version of our text embedding model. To learn more, see embeddings. The\n\n`gemini-embedding-exp-03-07`model will be deprecated on August 14, 2025.\n\n## July 7, 2025\n\n- Launched Gemini API Batch Mode. Batch up requests and send them to process asynchronously. To learn more, see Batch Mode.\n\n## June 26, 2025\n\nThe preview models\n\n`gemini-2.5-pro-preview-05-06`and\n\n`gemini-2.5-pro-preview-03-25`are now redirecting to the latest stable version\n\n`gemini-2.5-pro`.\n\n`gemini-2.5-pro-exp-03-25`is deprecated.... ## May 7, 2025\n\n- Released\n\n`gemini-2.0-flash-preview-image-generation`, a preview model for generating and editing images. To learn more, see Image generation and Gemini 2.0 Flash Preview Image Generation.\n\n## May 6, 2025\n\n- Released\n\n`gemini-2.5-pro-preview-05-06`, a new version of our most powerful model, with improvements on code and function calling.\n\n`gemini-2.5-pro-preview-03-25`will automatically point to the new version of the model.\n\n## April 17, 2025\n\n- Released\n\n`gemini-2.5-flash-preview-04-17`, a Gemini preview model optimized for price-performance and adaptive thinking. To learn more, see Gemini 2.5 Flash Preview and Thinking.\n\n## April 16, 2025\n\n- Launched context caching for Gemini 2.0 Flash.... ## April 9, 2025\n\n**Model updates:**\n\n- Released\n\n`veo-2.0-generate-001`, a generally available (GA) text- and image-to-video model, capable of generating detailed and artistically nuanced videos. To learn more, see the Veo docs.\n\nReleased\n\n`gemini-2.0-flash-live-001`, a public preview version of the Live API model with billing enabled.\n\n**Enhanced Session Management and Reliability** **Session Resumption:**Keep sessions alive across temporary network disruptions. The API now supports server-side session state storage (for up to 24 hours) and provides handles (session_resumption) to reconnect and resume where you left off. **Longer Sessions via Context Compression:**Enable extended interactions beyond previous time limits. Configure context window compression with a sliding window mechanism to automatically manage context length, preventing abrupt terminations due to context limits. **Graceful Disconnect Notification:**Receive a\n\n`GoAway`server message indicating when a connection is about to close, allowing for graceful handling before termination.\n\n\n\n**More Control over Interaction Dynamics** **Configurable Voice Activity Detection (VAD):**Choose sensitivity levels or disable automatic VAD entirely and use new client events (\n\n`activityStart`,\n\n`activityEnd`) for manual turn control.\n\n**Configurable Interruption Handling:**Decide whether user input should interrupt the model's response. **Configurable Turn Coverage:**Choose whether the API processes all audio and video input continuously or only captures it when the end-user is detected speaking. **Configurable Media Resolution:**Optimize for quality or token usage by selecting the resolution for input media. **Richer Output and Features** **Expanded Voice & Language Options:**Choose from two new voices and 30 new languages for audio output. The output language is now configurable within\n\n`speechConfig`.\n\n**Text Streaming:**Receive text responses incrementally as they are generated, enabling faster display to the user. **Token Usage Reporting:**Gain insights into usage with detailed token counts provided in the\n\n`usageMetadata`field of server messages, broken down by modality and prompt or response phases.... ## April 4, 2025\n\n- Released\n\n`gemini-2.5-pro-preview-03-25`, a public preview Gemini 2.5 Pro version with billing enabled. You can continue to use\n\n`gemini-2.5-pro-exp-03-25`on the free tier.\n\n## March 25, 2025\n\n- Released\n\n`gemini-2.5-pro-exp-03-25`, a public experimental Gemini model with thinking mode always on by default. To learn more, see Gemini 2.5 Pro Experimental.\n\n## March 12, 2025\n\n**Model updates:**\n\n- Launched an experimental Gemini 2.0 Flash model capable of image generation and editing.\n\n- Released\n\n`gemma-3-27b-it`, available on AI Studio and through the Gemini API, as part of the Gemma 3 launch.\n\n**API updates:**\n\n- Added support for YouTube URLs as a media source.\n\n- Added support for including an inline video of less than 20MB.\n\n## March 11, 2025\n\n**SDK updates:**\n\n- Released the Google Gen AI SDK for TypeScript and JavaScript to public preview.\n\n## March 7, 2025\n\n**Model updates:**\n\n- Released\n\n`gemini-embedding-exp-03-07`, an experimental Gemini-based embeddings model in public preview.... ## February 28, 2025\n\n**API updates:**\n\n- Support for Search as a tool\n\nadded to\n\n`gemini-2.0-pro-exp-02-05`, an experimental model based on Gemini 2.0 Pro.\n\n## February 25, 2025\n\n**Model updates:**\n\n- Released\n\n`gemini-2.0-flash-lite`, a generally available (GA) version of Gemini 2.0 Flash-Lite, which is optimized for speed, scale, and cost efficiency.\n\n## February 19, 2025\n\n**AI Studio updates:**\n\n- Support for additional regions (Kosovo, Greenland and Faroe Islands).\n\n**API updates:**\n\n- Support for additional regions (Kosovo, Greenland and Faroe Islands).\n\n## February 18, 2025\n\n**Model updates:**\n\n- Gemini 1.0 Pro is no longer supported. For the list of supported models, see Gemini models.\n\n## February 11, 2025\n\n**API updates:**\n\n- Updates on the OpenAI libraries compatibility.\n\n## February 6, 2025\n\n**Model updates:**\n\n- Released\n\n`imagen-3.0-generate-002`, a generally available (GA) version of Imagen 3 in the Gemini API.\n\n**SDK updates:**\n\n- Released the Google Gen AI SDK for Java for public preview.",
              "domain": "ai.google.dev"
            },
            {
              "position": 8,
              "title": "Google DeepMind's Veo 3 floods internet with realistic videos",
              "url": "https://www.axios.com/2025/05/23/google-ai-videos-veo-3",
              "snippet": "# Google's new AI video tool floods internet with real-looking clips\n\nGoogle's newest AI video generator, Veo 3, generates clips that most users online can't seem to distinguish from those made by human filmmakers and actors.\n\n**Why it matters: **Veo 3 videos shared online are amazing viewers with their realism — and also terrifying them with a sense that real and fake have become hopelessly blurred.\n\n**The big picture: **Unlike OpenAI's video generator Sora, released more widely last December, Google DeepMind's Veo 3 can include dialogue, soundtracks and sound effects.\n\n- The model excels at following complex prompts and translating detailed descriptions into realistic videos.\n\n- The AI engine abides by real-world physics, offers accurate lip-syncing, rarely breaks continuity and generates people with lifelike human features, including five fingers per hand.\n\n- According to examples shared by Google and from users online, the telltale signs of synthetic content are mostly absent.\n\n**Case in point: **In one viral example posted on X, filmmaker and molecular biologist Hashem Al-Ghaili shows a series of short films of AI-generated actors railing against their AI creators and prompts.\n\n**Special effects technology,** video-editing apps and camera tech advances have been changing Hollywood for many decades, but artificially generated films pose a novel challenge to human creators.... - In a promo video for Flow, Google's new video tool that includes Veo 3, filmmakers say the AI engine gives them a new sense of freedom with a hint of eerie autonomy.\n\n- \"It feels like it's almost building upon itself,\" filmmaker Dave Clark says.\n\n**How it works: **Veo 3 was announced at Google I/O on Tuesday and is available now to $249-a-month Google AI Ultra subscribers in the United States.\n\n**Between the lines: **Google says Veo 3 was \"informed by our work with creators and filmmakers,\" and some creators have embraced new AI tools. But the spread of the videos online is also dismaying many video professionals and lovers of art.\n\n- Some dismiss any AI-generated video as \"slop,\" regardless of its technical proficiency or lifelike qualities — but, as Axios' Ina Fried points out, AI slop is in the eye of the beholder.\n\n- The tool could also be useful for more commercial marketing and media work, AI analyst Ethan Mollick writes.\n\n**It's unclear how Google trained Veo 3 **and how that might affect the creativity of its outputs.\n\n- 404 Media found that Veo 3 generated the same lame dad joke for several users who prompted it to create a video of a man doing stand-up comedy.\n\n- Likewise, last year, YouTuber Marques Brownlee asked Sora to create a video of a \"tech reviewer sitting at a desk.\" The generated video featured a fake plant that's nearly identical to the shrub Brownlee keeps on his desk for many of his videos — suggesting the tool may have been trained on them.\n\n**What we're watching:** As hyperrealistic AI-generated videos become even easier to produce, the world hasn't even begun to sort out how to manage authorship, consent, rights and the film industry's future.\n\n##### Go deeperJul 10, 2025 - Technology... ## Google AI's new trick: Turn any image into a brief video\n\nGoogle's latest AI video tool, Veo 3, now generates short movies with sound based only on still photos and prompts.\n\nGo deeper (1 min. read)\n\n**The big picture: **The feature, released Thursday, is available to Ultra and Pro users on the web and soon on mobile for subscribers in select regions, Google shared with Axios.\n\n## Google avatars shake up workplace video making\n\nGoogle Vids is now providing users of the workplace video creation tool with a set of pre-made avatars for use in brief AI-generated videos, the company said Wednesday.\n\nGo deeper (2 min. read)\n\n**Why it matters: **The rise of cheap, convenient AI video generation threatens jobs for video producers, editors, camera operators and even commercial actors.\n\n## AI slop is ruining all of our favorite places to scroll\n\nAn AI-generated video of rabbits jumping on a trampoline that went viral this week — and was widely believed to be real — proved even cute animal vids aren't safe from convincing slop machines.\n\nGo deeper (2 min. read)\n\n**Why it matters: **All the fake AI-generated content online is sapping the joy of casual scrolling.",
              "domain": "www.axios.com"
            },
            {
              "position": 9,
              "title": "Google's Veo 3.1 is better at generating videos from images",
              "url": "https://www.engadget.com/ai/googles-veo-31-is-better-at-generating-videos-from-images-220829129.html",
              "snippet": "Google has released a new update to its Veo AI video generation model that should make it do a better job of sticking to prompts and converting images into videos. Veo 3.1 is available to try today through Google's Gemini API and is now also powering the company's Flow video editor.\nVeo 3.1 builds on the new capabilities Google introduced with launch of Veo 3 at Google I/O 2025. The new model offers better \"prompt adherence,\" according to Google, and should have an easier time creating videos based on the image \"ingredients\" you upload alongside your written prompt. Veo 3.1 also makes it possible to convert images to video and generate audio at the same time, a capability that wasn't available with Veo 3.\nIn Flow, Veo 3.1 supports at least a new feature that gives you finer control over the videos you generate. With what Google calls \"Frame to Video,\" Flow lets you upload a first and last frame, and then generates the video in-between. Adobe Firefly, which is powered by Veo 3, offers a similar feature, but Flow will be able to pull it off and create audio at the same time. Those added audio skills will also apply to the video editor's ability to extend clips and insert objects into existing footage, too.\nBased on the samples Google's shared, videos generated with Veo 3.1 still have an uncanny quality that seems to vary greatly depending on the prompt and subject. Even if it's missing some of the realism of OpenAI's Sora 2, though, the company's decision to try and make Veo more useful to people who actually work with video rather than a source of social media spam is a welcome move.",
              "domain": "www.engadget.com"
            },
            {
              "position": 10,
              "title": "Veo 3.1 is coming(and what's rumor): what we know and What it will ...",
              "url": "https://www.cometapi.com/veo-3-1-is-comingand-whats-rumor/",
              "snippet": "# Veo 3.1 is coming(and what’s rumor): what we know and What it will bring?\n\nVeo 3.1 is Coming:\n\n**Veo** is Google’s family of AI video-generation models (Veo 3 / Veo 3 Fast are current). Google has recently shipped big Veo 3 improvements (vertical 9:16, 1080p, Veo 3 Fast, lower pricing) and there are **rumors / social posts** that **Veo 3.1** is imminent — but Google has **not** published an official Veo 3.1 release bulletin yet. I’ll list confirmed facts, likely/expected changes, and a direct comparison to OpenAI’s **Sora 2**.\n\n## What\n\n**Veo** is\n\n**Veo** is Google’s line of generative video models (DeepMind / Google Cloud / Gemini family) that turn text or images into short videos — and (in Veo 3) generate audio natively (sound effects, ambient audio, and dialogue). It’s offered on Google Cloud (Vertex AI / Gemini API) for developers and enterprises, and includes built-in provenance / SynthID watermarks on outputs.\n\n## What\n\n**Veo 3** already brought **Text → video**and **image → video**capabilities (including preview image-to-video). **Native audio generation**(music, ambient sounds, dialogue) — Veo 3 introduced first-class audio. **Two variants**: high-quality Veo 3 and **Veo 3 Fast**(optimized for speed/iteration). **Platform availability:**made available in Vertex AI / Gemini API (paid preview → general availability updates in mid-2025). **Safety/provenance:**SynthID watermarking and some generation use controls/approval for person/child generation.... ## So — what is\n\n**Veo 3.1** expected to bring?\n\n**Status:** *As of now there is no official Veo 3.1 product page from Google describing full release notes.* However, multiple Google dev posts / community posts and tweets indicate a near-term incremental update (labelled “Veo 3.1”) that’s expected to focus on iterative improvements to audio, quality, and format support rather than a full new-generation rewrite.\n\nHere are some inferences I made based on x’s post and the characteristics of Veo3:\n\n**Improved native audio (dialogue, multi-voice lip sync)**—cleaner dialogue, better SFX mixing and spatialization). Veo 3 already generates audio natively; Veo 3.1 could improve dialogue realism and language support to match recent improvements competitors are shipping. **Faster/cheaper paths**for some common outputs (more Veo 3 Fast parity and optimizations). **Improved image→video fidelity and better character/pose consistency**in multi-frame clips. **Expanded aspect ratios / resolution controls**(more flexible 9:16/16:9 and 1080p across configs). Google already added vertical + 1080p; Veo 3.1 could expand those controls. **Longer clips / relaxed 8-second cap**— community demand and Google’s previous roadmap suggest increased duration is a likely target (Veo 3 today is optimized for 8-second clips). **Better image→video fidelity and extended image-to-video support**(improvements to realism, motion continuity), building on the image→video preview in Veo 3.... ## Compare Veo 3 / (expected) Veo 3.1 → OpenAI Sora 2\n\n### Primary focus\n\n**Veo 3 (Google)**: short, high-fidelity 8-second videos from text/image prompts; native audio; integrated into Gemini/Gemini API and Vertex AI; optimized for production use and developer API integration. **Sora 2 (OpenAI)**: OpenAI’s flagship video+audio model emphasizing physical realism, coherent motion, synchronized dialogue and sound, and an accompanying social app (Sora) with a cameo/consent system for integrating user likenesses and focuses heavily on realism and safety controls.\n\n### Strengths\n\n**Veo (now)**: strong developer/enterprise integration (Vertex AI, Gemini API), production pricing options, clear path for cloud customers, vertical/1080p + fast variant. Good for businesses building into pipelines. **Sora 2**: remarkable physical accuracy and multi-modal sync (dialogue + visuals), and a consumer-facing app integrated with social workflows (cameo feature, moderation). Great for creators wanting realistic narrative scenes and an app ecosystem.\n\n## How to access Veo now — and how to be ready for Veo 3.1\n\n**Try in Gemini (consumer / web / mobile)**: Veo generation is exposed in the Gemini apps (tap the “video” option in the prompt bar). Access level (Pro / Ultra) affects which Veo variants you can use. **Programmatically / enterprise**: use **API**in CometAPI (Veo model IDs available in the model docs). CometAPI provides veo3-pro, veo3-fast and veo3. For details, please refer to Veo 3 ‘s doc.\n\n**Practical tip (developer):** to request vertical output, set the\n\n`aspectRatio` parameter (e.g.\n\n`\"9:16\"`) and check the model configuration (Veo 3 vs Veo 3 Fast) and your plan for resolution limits (720p vs 1080p).... ## How to access Sora 2 (today)\n\n**Sora app:** Sora 2 launched with a Sora app (invite-limited rollout in US & Canada at launch). OpenAI indicated broader access and API expansion later. If you want to try Sora 2 now, check CpmetAPI’s Sora 2 page. CometAPI has already supported sora 2 API, and generates ~10-second social clips and an emphasis on motion realism for people.\n\n## Getting Started\n\nCometAPI is a unified API platform that aggregates over 500 AI models from leading providers—such as OpenAI’s GPT series, Google’s Gemini, Anthropic’s Claude, Midjourney, Suno, and more—into a single, developer-friendly interface. By offering consistent authentication, request formatting, and response handling, CometAPI dramatically simplifies the integration of AI capabilities into your applications. Whether you’re building chatbots, image generators, music composers, or data‐driven analytics pipelines, CometAPI lets you iterate faster, control costs, and remain vendor-agnostic—all while tapping into the latest breakthroughs across the AI ecosystem.\n\nDevelopers can access Veo 3.1 API through CometAPI, the latest model version is always updated with the official website. To begin, explore the model’s capabilities in the Playground and consult the API guide for detailed instructions. Before accessing, please make sure you have logged in to CometAPI and obtained the API key. CometAPI offer a price far lower than the official price to help you integrate.\n\nReady to Go?→ Sign up for CometAPI today !",
              "domain": "www.cometapi.com"
            }
          ],
          "success": true,
          "error": null
        },
        {
          "query_id": "q9",
          "query": "veo3.1 Google software version 2025 false",
          "claim_id": "claim_1",
          "query_type": "contradiction",
          "priority": "high",
          "results": [
            {
              "position": 1,
              "title": "Gemini AI video generator powered by Veo 3.1",
              "url": "https://gemini.google/overview/video-generation/",
              "snippet": "# Break the\n\n**silence** with Veo 3.1\n\nCreate high-quality, 8-second videos with Veo 3.1, our latest AI video generator. Simply describe what you have in mind or upload a photo and watch your ideas come to life with native audio generation. Try it with a Google AI Pro plan or get the highest access with the Ultra plan.\n\n**Veo 3.1** speaks for itself\n\n## Dream it. Describe it.\n\n**Done.**\n\n## For Exploring\n\nPlay with diverse styles, bring animated characters to life, and combine objects in ways you never thought possible. See what you can create using text to video with AI.\n\n## For Sharing\n\nCreate funny memes, turn inside jokes into videos, re-imagine special moments, and add a personal touch to make someone smile.\n\n## For Brainstorming\n\nBreak through creative blocks and visualize your ideas in a flash. From product concepts and designs to rapid prototyping and storytelling, Gemini can help.\n\n## Learn more about our\n\n**Veo Models**\n\nCreate videos with sound using our video generation model that maintains high-quality while optimizing for speed.\n\nCreate high-quality, 8-second videos with sound using our state-of-the-art video generation model.... ## Frequently asked questions\n\nYes, you can create and share videos in your mobile Gemini app. To create videos, tap the \"video\" button in your prompt bar. If you don't see it, tap the button with three dots to view more options.\n\nTry Veo 3.1 Fast with a Google AI Pro plan or get the highest access to Veo 3.1 in Google AI Ultra. Country availability here.\n\nFor now, the ability to generate a video from a photo is not available in the European Economic Area, Switzerland, or the United Kingdom.\n\nWe’ve taken several important safety steps to make AI video generation a safe experience. This includes extensive red teaming and evaluation aimed at preventing the generation of content that violates our policies. Additionally, all videos generated with Veo in the Gemini app are marked with a visible watermark and SynthID, a digital watermark embedded in each frame, which indicates the videos are AI-generated.\n\nGemini's outputs are primarily determined by user prompts and like any generative AI tool, there may be instances where it generates content that some individuals find objectionable. We’ll continue to listen to your feedback through the thumbs up/down buttons and make ongoing improvements. For more details, you can read about our approach on our website.\n\nResults for illustrative purposes and may vary. Internet and subscription for certain features required. Available to users 18+. Create responsibly.",
              "domain": "gemini.google"
            },
            {
              "position": 2,
              "title": "Google's Veo 3 Update! July 2025!",
              "url": "https://www.youtube.com/watch?v=qhReJkSRKOc&vl=en",
              "snippet": "## Murray Frost\n##### Jul 09, 2025 (0:04:15)\n✅ Build a Monetized YouTube Channel in 90 days: https://murrayfrost.com/YT-Accelerator\n\nI teach people YouTube from REAL data from over 150 clients and my own channels. Data-driven feedback and strategies. None of this guessing garbage people put on online teaching you how to do YouTube.... {ts:0} So, Google Labs just had an update and it's technically called Flow. It's on\n{ts:4} their labs.google platform and you can see by the beginning of this video that it does still need some work, but you\n{ts:9} haven't been able to do that recently with Google Labs. And now you can do it with Google's Vo3. So, there's been a\n{ts:15} couple updates here that you can see. And the first one here is using images or allowing images to talk with Google's\n{ts:20} VO3, which again still needs some work, but look, it gets maybe 60% of the way there. I think it looks pretty decent.\n{ts:29} I'm obviously not going to use it to try and convince people that it's real, but maybe you can get kind of creative with\n{ts:34} this and get people to make some really funny, strange, or just dumb things that people love. I don't know why people\n{ts:42} just love brain rot these days. Now, they're also adding the option to do this with V V3 on frames to video, which\n{ts:49} is actually kind of cool. And they... 're also allowing you to top up your subscription with the kind of a mid tier\n{ts:56} option cuz previously they had just the the starter which was about 20 bucks or so per month in the US and then it was\n{ts:63} straight up to I think 250 without the discount for the first 3 months and there's just no in between. It's just a\n{ts:70} massive jump. So they added like a kind of a mid tier there. I think a couple too. Well, I'll show you what that looks\n{ts:75} like and you can top up your credits there as well. They've also gone ahead and just added better audio coverage,\n{ts:83} which I haven't really noticed all that much to be honest. Right now, there's not a huge difference as at least a\n{ts:89} noticeable difference in my opinion from me using it. Um, they also do remove audio when miners are involved. Keep\n{ts:96} that in mind. That's why your audio isn't being generated if you have kids in the video or maybe uh even teenagers\n{ts:101} sometimes. Um, but then there's also they've mentioned they're reducing um unwanted subtitles, which is actually... {ts:107} quite nice. They've been removing the the VEO watermark as well, but now they said they've reduced the unwanted\n{ts:114} subtitles. I still get them sometimes. So, I literally in caps specify in the prompt to not include captions because\n{ts:121} otherwise if I don't, sometimes the captions still show up. And the really nice quality of life update they've made\n{ts:127} here is that when you are just starting a new project or revisiting an existing one, it doesn't reset the model that you\n{ts:135} have, or at least if it does, it resets to the VO3 fast beta audio. So, this is where you're generating audio. So now\n{ts:142} you don't accidentally have VO2 selected with no audio every time you either reload a page, start a new project, or\n{ts:150} you leave and come back and it's been reset to just its default VO2. Now it's actually its default is V3 fast beta\n{ts:157} audio. So the 20 credits per generation, the cheaper VO3 option with audio. So now you don't accidentally generate\n{ts:164} videos without audio, which has happened a lot to me and I... 've wasted thousands of credits doing that by accident. So,\n{ts:171} great update right there. Quality of life, which you don't have to waste any more credits. Now, now let's say that\n{ts:176} you don't want to spend the $124 per month, and this is for the first 3 months. Then, it goes to 150, I believe,\n{ts:183} per month. So, I'm probably going to cancel it at that point because that's just really expensive, at least using\n{ts:189} VO3. Now you have the option if you have the uh let's see which was it the pro subscription the $20 a month\n{ts:195} subscription right here Google AI pro you still get a th000 credits per month in uh Google labs but you also have the\n{ts:204} option to top up your credits so for example in here when you're creating your AI videos if you run out of credits\n{ts:211} like here you can just hit get more AI credits and you can choose how much you want to add so you don't have to spend\n{ts:216} $200 or $150 at a time you could spend an extra for 24 bucks that month just to top up your credits. This wasn... 't\n{ts:223} available in the lower plans. It was only available in the the maximum ultra tier. So, I really like the ability to\n{ts:230} do that now. So, you don't have to spend so much all at once, especially if you're not using all your credits at the\n{ts:235} end of each month, but maybe one month you're just out and you need to add more, you can do so and add some\n{ts:240} credits, which is actually quite nice. I've hit this button a little too much recently, but uh hey, I'm getting good\n{ts:247} results. So, those are all the current updates with Google's Veo3 with their labs platform. Hope to see you in the\n{ts:253} next one.",
              "domain": "www.youtube.com"
            },
            {
              "position": 3,
              "title": "Build with Veo 3, now available in the Gemini API",
              "url": "https://developers.googleblog.com/en/veo-3-now-available-gemini-api/",
              "snippet": "First unveiled at Google I/O 2025, people around the world have already generated tens of millions of high-quality videos with Veo 3 (along with some new fun and interesting video trends). It is our first video model to incorporate high-fidelity video outputs and native audio, first with text-to-video and soon with image-to-video.\n\nDevelopers are already experimenting with Veo 3, discovering how the model can help them brainstorm content, rapidly iterate, and be more efficient.\n\nVeo 3 is designed to handle a range of video generation tasks, from cinematic narratives to dynamic character animations. With Veo 3, you can create more immersive experiences by not only generating stunning visuals, but also audio like dialogue and sound effects.\n\nLet’s take a look at some examples.\n\nExplore these examples and more with Veo 3 in Google AI Studio, available as an SDK template and interactive Starter App to remix, copy and extend. The Starter App and its sample code offer a convenient way for Paid Tier users to rapidly prototype with Veo 3 and more on the Gemini API, directly from Google AI Studio.\n\nClick the Key button in the top right of the AI Studio Build interface to select a Google Cloud Project with billing enabled to use the Paid Tier in AI Studio apps. See the FAQs for more.... Veo 3 will be priced at $0.75 per second for video and audio output. Additionally, Veo 3 Fast will be available soon, offering a faster and more cost-effective option for video creation.\n\nHere’s a basic Python example to create a video:\n\n```\n\nimport time\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.0-generate-preview\",\n\nprompt=\"a close-up shot of a golden retriever playing in a field of sunflowers\",\n\nconfig=types.GenerateVideosConfig(\n\nnegative_prompt=\"barking, woofing\",\n\n),\n\n\n\n# Waiting for the video(s) to be generated\n\nwhile not operation.done:\n\ntime.sleep(20)\n\noperation = client.operations.get(operation)\n\ngenerated_video = operation.result.generated_videos[0]\n\nclient.files.download(file=generated_video.video)\n\ngenerated_video.video.save(\"veo3_video.mp4\")\n\n```\n\nAll videos generated by Veo 3 models will continue to include a digital SynthID watermark. To get started, check out the documentation, cookbook, and a Veo 3 starter app in Google AI Studio:\n\nIn addition to being available via the Gemini API in Google AI Studio, Veo 3 is also available to Google AI subscribers in the Gemini app and Flow, and to enterprise customers via Vertex AI.... 1: Fluffy Characters Stop Motion: Inside a brightly colored, cozy kitchen made of felt and yarn. Professor Nibbles, a plump, fluffy hamster with oversized glasses, nervously stirs a bubbling pot on a miniature stove, muttering, \"Just a little more... 'essence of savory,' as the recipe calls for.\" The camera is a mid-shot, capturing his frantic stirring. Suddenly, the pot emits a loud \"POP!\" followed by a comical \"whoosh\" sound, and a geyser of iridescent green slime erupts, covering the entire kitchen. Professor Nibbles shrieks, \"Oh, dear! Not again!\" and scurries away, leaving a trail of tiny, panicked squeaks.\n\n2: The sequence begins with an extreme close-up of a single gear, slowly turning and reflecting harsh sunlight. The camera gradually pulls back in a continuous movement, revealing this is but one component of a colossal, mechanical heart half-buried in a desolate, rust-colored desert. A sweeping aerial shot establishes its enormous scale and isolation in the barren landscape. The camera descends to capture pipes hissing steam and the rhythmic thumping that echoes across the empty plains. A subtle shake effect synchronizes with each massive heartbeat. A lateral tracking shot discovers tiny, robed figures scurrying across the metallic surface. The camera follows one such figure in a detailed tracking shot as they perform meticulous maintenance, polishing brass valves and tightening immense bolts. A complex movement circles the entire structure, capturing different maintenance teams working in precarious positions across its rusted exterior. The final shot begins tight on the meticulous work of one tiny figure before executing a dramatic pull-out that reveals the true scale of the heart and the minuscule size of its caretakers, tending to the vital organ of an unseen, sleeping giant that extends beyond the frame.",
              "domain": "developers.googleblog.com"
            },
            {
              "position": 4,
              "title": "Google rolls out its new Veo 3 video-generation model ...",
              "url": "https://techcrunch.com/2025/07/03/google-rolls-out-its-new-veo-3-video-generation-model-globally/",
              "snippet": "Google on Thursday said it has begun rolling out its Veo 3 video generation model to Gemini users in more than 159 countries.\n\nVideo generation via the new model is available only to paying subscribers of Google’s AI Pro plan and is capped at three videos per day.\n\nVeo 3, which Google showed off in May, lets users generate videos up to eight seconds long using text prompts.\n\nGoogle’s Josh Woodward has said that the company is working on adding image-to-video generation capabilities to Gemini.",
              "domain": "techcrunch.com"
            },
            {
              "position": 5,
              "title": "ATUALIZAÇÃO GOOGLE VEO 3 (CONSISTÊNCIA LIBERADA)",
              "url": "https://www.youtube.com/watch?v=if7FJuFD9Es",
              "snippet": "## Anderson Mak\n##### Jul 08, 2025 (0:08:13)\nATUALIZAÇÃO GOOGLE VEO 3 (CONSISTÊNCIA LIBERADA)... {ts:0} Hoje eu vim mostrar como o nosso voto valeu a pena. Vejam essas ruas todas\n{ts:6} reformadas. Gente, aquele safado enganou a gente. Olha a cratera que tem aqui na rua. Eu\n{ts:12} vou postar esse vídeo. Toda a cidade precisa saber a verdade. Pessoal, agora ficou muito fácil você\n{ts:20} manter consistência na criação dos personagens que você utiliza aqui no Google V 3. Então você pode ver que esse\n{ts:27} vídeo que eu criei aí no começo, eu fiz aqui três cenas e ele manteve aqui totalmente\n{ts:36} ou pelo menos 99% a as características do personagem, né? Você pode ver aí o que realmente parece\n{ts:43} a mesma pessoa, né? E como que isso aqui tá sendo feito agora? Isso tá sendo possível porque agora você consegue\n{ts:52} colocar uma imagem. Então você coloca a imagem de referência aqui, ó. Tá? Eu tô aqui na no flow, na ferramenta flow. E\n{ts:63} aqui você pode fazer upload de imagens. Você pode ver que eu coloquei aqui algumas imagens.... {ts:68} Então o segredo tá você manter a consistência na criação da imagem. Então quando você for criar a imagem, eu vou\n{ts:74} mostrar como que eu criei aqui. Você mantendo a consistência na imagem, você joga a imagem aqui e na hora de fazer a\n{ts:80} criação do prompt, você coloca a imagem como referência. Você pode ver que ele permite você colocar a primeira imagem\n{ts:87} do primeiro frame aqui, tá? Então essa imagem que será utilizada. Ele tem aqui o recurso da segunda\n{ts:94} imagem, porém quando você coloca a segunda imagem, tipo a imagem inicial e a imagem final, ele não permite você\n{ts:100} colocar voz, tá? Esse recurso aqui de colocar imagem já estava disponível, porém quando você colocava aqui uma\n{ts:107} imagem, ele mudava pra versão dois, ou seja, sem áudio. Agora você já consegue na versão três, inclusive nessa fest,\n{ts:116} colocar a imagem como referência aqui, qualquer imagem que você fizer upload aqui como referência,\n{ts:123} colocar o prompt e aí criar aqui inclusive com a fala, com áudio e tudo mais que vai funcionar perfeitamente.... {ts:130} Então, olha que interessante esse recurso que tá disponível aqui. Bom, como que eu criei esse personagem e como\n{ts:137} que eu fiz para manter uma certa consistência deste personagem? Então, eu utilizei aqui o chat GPT e\n{ts:147} aqui eu coloquei algumas características. Deixa eu mostrar aqui, ó.\n{ts:157} Eu coloquei essas características aqui, né? Eu coloquei, ó, cria um prompt de um homem caucasiano, cabelos loiros, olhos\n{ts:165} azuis, 30 anos de idade, com calça jeans, curta, eh, e camiseta verde. Ele está caminhando pela rua estilo vlog.\n{ts:176} Então, eu peguei essas características aqui e criei um prompt\n{ts:185} 3, tá? E eu mandei ele também criar um promit para Me Journey, tá? Você pode criar aqui inclusive no chat GPT, tá?\n{ts:194} Aqui eu fui criar aqui, mas deu um pequeno erro e tava demorando. Então eu criei lá no chat GPT, mas você pode é no\n{ts:200} Mid Journey, você pode criar também aqui no chat GPT que também vai funcionar. Então ele deu esse prompt aqui para mim.... {ts:207} Aí eu entrei aqui no mid journey e executei esse prompt e criei a primeira imagem, tá? Deixa eu mostrar aqui\n{ts:217} que foi essa aqui. Eu peguei essa aqui, tá? Então ele criou aqui quatro versões e aí eu baixei esta imagem, tá? Para o\n{ts:224} mid journey. E uma vez que você baixa a imagem para criar as outras imagens aqui no mid\n{ts:231} journer, eu tô utilizando um recurso que é você você pode clicar na imagem que tá no seu computador ou pegar uma dessas\n{ts:238} imagens que você já utilizou aqui, ó. Clica e arrasta e coloca nessa opção aqui, ó.\n{ts:244} você vai utilizar como um personagem de referência, tá? Ó, eu coloquei ele aqui para ele utilizar e aí eu coloco\n{ts:253} qualquer prompt dele em outros lugares, outras cenas. Então, eu tenho a primeira cena aqui,\n{ts:260} né, a primeira imagem e aí pedi também para ele criar aqui o o prompt para o Google Viow, tá? Que foi esse aqui, né?\n{ts:269} Aí peguei tal e aqui joguei o prompt e também coloquei a imagem de referência, tá? Então aí ele fez esse primeiro vídeo... {ts:276} aqui e aí o segundo vídeo é a mesma coisa, né? Coloquei a mesma imagem com uma\n{ts:282} referência e mandei ele fazer aqui a ação dele ali caindo no buraco e tal. E o terceiro vídeo\n{ts:290} eu criei colocando como imagem de referência aquela outra imagem que ele tá ali no escritório, né? Deixa eu\n{ts:296} mostrar aqui. Então, eu pedi pro chatt fazer outro prompt para mid journey agora com ele no escritório, né, que é\n{ts:304} esse aqui, ó. E aí eu joguei aqui e coloquei nessa opção de referência aqui, ó, no M Journey. E aí ele colocou o\n{ts:313} mesmo personagem aqui no escritório, tá? Aí eu escolhi uma dessas que eu achei que ficou\n{ts:320} melhor. Baixei e utilizei aqui fazendo aqui, né? Ó, peguei no caso este aqui, seleciona\n{ts:330} aqui, coloca o prompt. O prompt você pode pedir lá para o mid journey também para ele falar o que você quer que ele\n{ts:338} fale e tudo mais. E aí você executa e bum, ele mantém a consistência. Então, olha como que ficou... {ts:345} fácil aqui. Agora, através dessa ferramenta que é o flow, né? Você consegue manter a consistência nessa\n{ts:353} ferramenta aqui. Muito bacana, né? Lembrando que eu utilizei aqui o mid jour Journey, mas você pode pedir direto\n{ts:359} para o chat GPT, se você quiser. Pede para ele fazer a primeira imagem, dá ali as características, né? Conforme eu\n{ts:365} coloquei aqui, ó. Ó, homem caucasiano, cabelos loiros, olhos azuis, 30 anos de idade, coloca detalhes da roupa\n{ts:372} e aí manda ele manter a consistência, manda ele fazer aqui uma primeira imagem e na segunda imagem que você for criar\n{ts:379} no chat GPT, você pode falar, né? Mantém a consistência e você pode, inclusive fazer o upload da própria imagem que\n{ts:384} você salvou, fez a primeira imagem, salvou, envia ela como referência e manda ele manter as características do\n{ts:390} personagem e fazer uma segunda cena, fazer em outro lugar, que aí você vai conseguir gerar a segunda cena também\n{ts:397} aqui é no chatt. No meu caso, eu utilizei aqui o mid journey, porque eu tenho aqui a assinatura do midjour... {ts:402} Journey, então para mim fica mais fácil utilizar esse recurso de Omni reference aqui, ó, que é uma referência de\n{ts:410} personagem, né? E aí eu coloco em qualquer cenário, qualquer lugar que eu quiser, o mesmo personagem, inclusive\n{ts:418} com a mesma roupa, né? Roupa bem parecida aqui. Beleza? Então, maravilha. Agora para você manter consistência\n{ts:425} ficou muito fácil, né? aqui utilizando esse novo recurso de colocar aqui a primeira imagem e aqui você coloca o seu\n{ts:431} prompto. Lembrando que quando você entra aqui vai estar desta forma texto para vídeo, tá? Para ficar bilitado, você\n{ts:438} clica aqui e altera para transformar frames em vídeo, tá? Coloca nesta opção aqui e aí sim você consegue colocar o\n{ts:445} primeiro frame. Se você colocar o segundo frame, deixa eu colocar por exemplo esse aqui.\n{ts:451} Você pode ver, você coloca aqui o primeiro frame e o último frame. Você pode ver que vai dar\n{ts:458} um erro, ó. Quando você for tentar executar, ele vai dizer que você tem que alterar pra versão dois. Então ele não... {ts:463} tá permitindo ainda você utilizar o último frame, apenas o primeiro. Então é este aqui é o segredo, tá? manter apenas\n{ts:469} o primeiro frame e aí o resto aqui vai funcionar tranquilamente. Beleza? Então nesse vídeo era isso. Espero que tenham\n{ts:474} gostado aí dessa dica. Ficando por aqui. Forte abraço. Até um próximo vídeo. Falou.\n{ts:478} [Música]",
              "domain": "www.youtube.com"
            },
            {
              "position": 6,
              "title": "Veo 3 available for everyone in public preview on Vertex AI - Google Cloud",
              "url": "https://cloud.google.com/blog/products/ai-machine-learning/veo-3-available-for-everyone-in-public-preview-on-vertex-ai",
              "snippet": "# You dream it, Veo creates it: Veo 3 is now available for everyone in public preview on Vertex AI\n\n##### Jason Gelman\n\nDirector, Product Management, Vertex AI\n\n##### Try Gemini 2.5\n\nOur most intelligent model is now available on Vertex AITry now\n\nA great story doesn't just tell you, it shows you. With Veo 3, we’ve leapt forward in combining video and audio generation to take storytelling to the next level.\n\nToday, we’re excited to share that Veo 3 is now available for all Google Cloud customers and partners in public preview on Vertex AI.\n\n**Why this matters: **Veo 3 is your partner for creating near-cinematic quality generative video, moving beyond novelty to narrative-driven creation. It not only brings stunning visual quality, but now adds sound from background sounds to dialogue. With Veo 3 on Vertex AI, you can take advantage of three powerful new capabilities:\n\n\n\n**Fluid, natural videos that synchronize video with audio and dialogue.**Veo 3 can synchronize your audio and visuals in a single pass. The model produces rich soundscapes containing everything from dialogue and ambient noise, to sound effects and background music.\n\n\n\n**Cinematic video that captures creative nuances.**Veo 3 makes it easy to capture creative nuances and detailed scene interactions in your prompt, from the shade of the sky to the precise way the sun hits water in the afternoon light, and produces high-definition video.\n\n\n\n**Realistic movement that simulates real-world physics.**To create believable scenes, Veo 3 simulates real-world physics. This results in realistic water movement, accurate shadows connected with objects and characters, and natural human motion.... ### Businesses are already using Veo to make creating easier\n\nVeo 3 is helping Google Cloud customers create external content – from social media ads to product demos – and internal materials like training videos and presentations. Hear directly from the teams:\n\n“Veo 3 has marked the difference within the gen AI industry, and we’re glad that Freepik users have been some of the first to try the model out. The quality of the video generations combined with the audio integration option is the game changer in our AI Suite. We look forward to continuing this collaboration to bring the best AI tools and features to our users” – Omar Pera, CPO, Freepik\n\n“Creativity is deeply personal, and our goal is to build a platform that adapts to every workflow. By working with Google, we’re combining the best technologies to give creators more control, efficiency, and power than ever before. Our collaboration with Google Cloud represents a strategic evolution that will not only enhance accessibility and efficiency but fundamentally transform how people create. We believe the future of generative video technology will leverage the best technologies to build the most flexible and accessible tools. This is an exciting step toward realizing that vision” – Zeev Farbman, Co-Founder & CEO, Lightricks.\n\n\"Veo 3 is the single greatest leap forward in practically useful AI for advertising since genAI first broke into the mainstream in 2023. By allowing brands to make fully fledged films from a single prompt - including brand, story, video, sound effects, voiceovers and more - Veo3 in one swoop lowers the barriers to entry to gen AI for creative people and elevates gen AI to a top tier brand building tool usable at every stage of the marketing funnel.\" – Will Hanschell, co-founder and CEO, , Pencil\n\n**Bring your vision to life with Veo 3 today**\n\nVeo 3 on Vertex AI is built for scalable enterprise use with crucial guardrails like safety filter controls and SynthID to ensure responsible deployment for any use case. To get started, go here to learn more about Veo 3 on Vertex AI and try it on Vertex AI Media Studio. Get started today!",
              "domain": "cloud.google.com"
            },
            {
              "position": 7,
              "title": "Introducing Veo 3.1 and new creative capabilities in the Gemini API",
              "url": "https://developers.googleblog.com/en/introducing-veo-3-1-and-new-creative-capabilities-in-the-gemini-api/",
              "snippet": "**Today, we are releasing Veo 3.1 and Veo 3.1 Fast in paid preview in the** **Gemini API** **.** This updated model offers several improvements, as well as improved outputs when generating video from images. These new models are available via the Gemini API in Google AI Studio and Vertex AI. Veo 3.1 is also available in the Gemini app and Flow.\n\nVeo 3.1 and Veo 3.1 Fast empower developers to create more engaging content through significant upgrades. The models now generate\n\n**richer native audio**, from natural conversations to synchronized sound effects, and offer **greater narrative control** with an improved understanding of cinematic styles. **Enhanced image-to-video** capabilities ensure better prompt adherence while delivering superior audio and visual quality and maintaining character consistency across multiple scenes.\n\nIn addition to the model update, we are introducing new model capabilities, including using reference images to guide video generation, extend existing Veo videos, and generate transitions between a first and last frame.\n\nYou can now guide the generation process by providing up to 3 reference images of a character, object, or scene. This is helpful for maintaining character consistency across multiple shots or applying a specific style to your video.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nconfig=types.GenerateVideosConfig(\n\nreference_images=[reference_image1, reference_image2],\n\n),\n\n\n\n```... Your story is no longer limited by the original generation. With Scene extension, you can create longer videos, even lasting for a minute or more, by generating new clips that connect to your previous video. Each new video is generated based on the final second of the previous clip. This maintains visual continuity, making the feature ideal for extending shots with background audio.\n\n```\n\nfrom google import genai\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nvideo=video_to_extend\n\n\n\n```\n\nCreate smooth, natural scenes that bridge two different images. By providing a starting and an ending image, you can direct Veo 3.1 to generate the transition between them, complete with accompanying audio.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nimage=first_frame,\n\nconfig=types.GenerateVideosConfig(\n\nlast_frame=last_frame,\n\n),\n\n\n\n```\n\n**What people are building with Veo 3.1**\n\nPromise Studios, a GenAI movie studio, uses Veo 3.1 within its MUSE Platform to enhance generative storyboarding and previsualization for director-driven storytelling at production quality.... Latitude is experimenting with Veo 3.1 in its generative narrative engine to instantly bring user-created stories to life.\n\nVeo 3.1 and these new capabilities are now available in paid preview via the Gemini API.\n\nVeo 3.1 is the same price as Veo 3. We are incredibly excited to see what you will build with these new capabilities!",
              "domain": "developers.googleblog.com"
            },
            {
              "position": 8,
              "title": "Veo (text-to-video model) - Wikipedia",
              "url": "https://en.wikipedia.org/wiki/Veo_(text-to-video_model)",
              "snippet": "**Veo**, or **Google Veo**, is a text-to-video model developed by Google DeepMind and announced in May 2024. As a generative AI model, it creates videos based on user prompts. Veo 3, released in May 2025, can also generate accompanying audio.\n\n## Development\n\nIn May 2024, a multimodal video generation model called Veo was announced at Google I/O 2024. Google claimed that it could generate 1080p videos over a minute long. In December 2024, Google released Veo 2, available via VideoFX. It supports 4K resolution video generation and has an improved understanding of physics. In April 2025, Google announced that Veo 2 became available for advanced users on the Gemini app.\n\nIn May 2025, Google released Veo 3, which not only generates videos but also creates synchronized audio — including dialogue, sound effects, and ambient noise — to match the visuals. Google also announced **Flow**, a video-creation tool powered by Veo and Imagen. Google DeepMind CEO Demis Hassabis described the release as the moment when AI video generation left the era of the silent film.... ## Capabilities and limitations\n\nGoogle Veo can be bought by several subscription/membership tiers, and/or by using Google \"AI credits\". The software itself can be run by two different consoles called Google Gemini and Google Flow, with Gemini being geared towards shorter, quicker, and faster projects, using the Gemini AI chat model, or through Google Flow, which is essentially a movie editor, as well, allowing users to create longer projects, and continuity using the same characters and actors. Users can create a maximum length of eight seconds per clip.\n\nGoogle Veo, has a relatively simple interface and dashboard, however writing prompts, for those who have little to no experience in transcribing or filmmaking may face issues with the software misunderstanding what the user intended by their prompt (no matter how detailed it was). So although Veo does have a friendly and simple setup, prompts, which are the forefront of the software, need to be not only short and to the point, but they also must be very specific, if the user wants the right vision for their project. Google Veo, when it comes to human models, is able to generate several ethnicity and body types. The software is also capable of generating stand up comedy routines, and Music videos. It can as well generate animals, cartoons, and animation. Prompts must accurately describe places, people, and things in each scene, in addition knowledge of film and camera lingo such as panning, zooming, and terms for camera angles, are also important.... Google Veo however, has strict guidelines and blockades to their software. Before a clip is generated, the algorithm computer software reviews it, and if it is anything deemed inappropriate, too graphically sexual, illegal, showcasing graphic abuse/assault/fighting (unless the prompt specifies that it is a fictitious martial arts scene etc.) gross behaviors, antisemitism, racist, homophobic, anything depicting reigning regimes, rioting, blood, gore, or warfare, (unless in some cases the prompt specifies that it is fictitious period drama, the clip may still be generated), the clip will not be generated. In addition, Google Veo cannot and will not generate character actors that look identical to celebrities or real-life individuals. Users have primarily complained that, regardless of how descriptive and detailed their prompts are, Google Veo often misunderstands the input, resulting in completely different outputs. Common issues include the emulation of incorrect subtitles and captions, the generation of complex scenes that are incomplete due to the maximum eight-second length, the production of garbled and nonsensical speech, and character models that appear deformed in both appearance and movement. Users have also reported that their prompts and generated content are falsely flagged as violating guidelines, along with a variety of other issues and complaints. However, trial and error may have to be used with Veo for optimal results.... ## Reactions\n\nA reporter for *Gizmodo* reacted to the release of Veo 3 by observing that users were directing the model to generate low-quality content, such as man on the street interviews or haul videos of people unboxing products. Another media commentator reported that the tool tended to repeat the same joke in response to different prompts.\n\nCommentators speculated that Google had trained the service on YouTube videos or Reddit posts. Google itself had not stated the source of its training content.\n\nIn July 2025, Media Matters for America reported that racist and antisemitic videos generated using Veo 3 were being uploaded to TikTok. Ryan Whitwam of *Ars Technica* commented, \"In a perfect world, Veo 3 would refuse to create these videos, but vagueness in the prompt and the AI's inability to understand the subtleties of racist tropes (i.e., the use of monkeys instead of humans in some videos) make it easy to skirt the rules.\"\n\n## See also\n- Sora (text-to-video model)\n- VideoPoet – Text-to-video model by Google\n- Dream Machine (text-to-video model)\n\n## References\n\n## External links\n- Official website\n- *Introducing Veo 3.1 and advanced capabilities in Flow*\n\nCategories: - 2024 software\n- Applications of artificial intelligence\n- Film and video technology\n- Google DeepMind\n- Text-to-video generation\n- Video processing\n- Generative artificial intelligence\n- 2024 in artificial intelligence",
              "domain": "en.wikipedia.org"
            },
            {
              "position": 9,
              "title": "We've got a surprise Pixel Drop for you.",
              "url": "https://blog.google/products/pixel/pixel-drop-july-2025/",
              "snippet": "Here’s what’s new for Pixel:\n\n**Veo 3 on Pixel:**Pixel 9 Pro owners get a full year of our Google AI Pro subscription at no cost, giving them access to the latest features in the Gemini app. And that includes Veo 3, which you can use to describe your idea and watch it come to life as a high-quality, short video, complete with natural audio. **New Circle to Search capabilities:**Dive deeper and ask follow-up questions about anything you see on your screen with AI Mode in Circle to Search, available in the U.S. and India. We’re also adding in-game help in Circle to Search, so you can find helpful articles and videos timestamped to your exact spot in your mobile game, without switching apps. **Gemini on Pixel Watch:**Get the help you need right on your wrist, with our advanced AI models powered by WearOS.",
              "domain": "blog.google"
            },
            {
              "position": 10,
              "title": "Report says Google is about to release VEO 3.1 version on Gemini ...",
              "url": "https://www.aibase.com/news/21990",
              "snippet": "# Report says Google is about to release VEO 3.1 version on Gemini and API\n\n#### AIbase基地Published inAI News · 4 min read · Oct 15, 202518\n\nRecent reports indicate that the public release of VEO3.1 is imminent for Google. With related disclaimers appearing in the Gemini application, Google is showcasing the features of VEO3.1 to a broad user base, which may be implemented within the familiar Gemini interface.\n\nThe post shared by prominent figure Logan Kilpatrick in the community on the social media platform X is widely seen as an early confirmation of Google's new AI product release. Additionally, references to preview models such as \"VEO3.0Generate\" and \"VEO3.0Fast Generate\" have appeared in Vertex AI, indicating that Google is offering multiple access channels for early users and enterprises, in line with their traditional strategy in the release of video generation tools.\n\nDiscussions in the community about output duration are intense, with evidence suggesting that video length may be extended from the previous 8 seconds to 30 seconds, although this remains to be confirmed. Previous leaks indicated that the fast mode has lower quality, while the standard mode is expected to unlock higher output quality, which is particularly important for creators looking to enhance visual quality and narrative potential. According to TestingCatalog, previous 720p video generation has shown improvements in VEO3.1, including new audio capabilities and enhanced visual effects, making it a competitor to similar products like Sora2.\n\nGoogle's overall product strategy is to position Gemini as a central workspace, with VEO models integrated for use by both consumers and enterprise users. Through the preview release on Vertex AI, enterprises can try generating videos, while the mainstream promotion of the Gemini application can reach regular users. This phased release strategy not only maximizes developer feedback but also promotes public acceptance, aligning with Google's ongoing efforts to bridge functional gaps and solidify its position in the generative media field.\n\nKey Points:\n\n🌟 VEO3.1 is about to be released, allowing users to experience its new features in the Gemini application.\n\n🎥 Video duration may be extended from 8 seconds to 30 seconds, providing creators with more narrative space.\n\n🚀 Google's phased release strategy maximizes developer feedback and public acceptance.... #### This article is from AIbase Daily\n\nWelcome to the [AI Daily] column! This is your daily guide to exploring the world of artificial intelligence. Every day, we present you with hot topics in the AI field, focusing on developers, helping you understand technical trends, and learning about innovative AI product applications.",
              "domain": "www.aibase.com"
            }
          ],
          "success": true,
          "error": null
        },
        {
          "query_id": "q10",
          "query": "Google veo3.1 release 2025 misleading",
          "claim_id": "claim_1",
          "query_type": "contradiction",
          "priority": "high",
          "results": [
            {
              "position": 1,
              "title": "VEO 3 is UNREAL...it might actually take my job",
              "url": "https://www.youtube.com/watch?v=Xj4BDwgEwd4",
              "snippet": "{ts:0} VO3 is here and it's shocked the entire industry. Uh, Alex, what was that? It's\n{ts:8} nothing. Don't worry about it. Are you trying to replace me with AI? What? No, I'm just trying something new. Just do\n{ts:16} the normal intro. It It's fine. All right. V3 just dropped and I'm going to show you so many incredible examples of\n{ts:24} V3 in action. Let's get into it. Oh, and Alex, don't do that again. Anyways, let's get into it. All right. I've been\n{ts:32} seeing a ton of these street style interviews, hyper realistic, where someone is interviewing people on the\n{ts:38} street who kind of just stumbled out of a bar. They all are either a little bit tipsy or drunk, and V3 has been\n{ts:45} recreating these so well. So, here are two that I've made. The dialogue between them didn't exactly match my prompt, but\n{ts:53} I'll show you some others where it's pretty compelling. So, this is street interview. Hyperrealistic guy\n{ts:59} interviewing two girls, all Gen Z. They just came out of a bar kind of tipsy drunk. The interviewer asks,... \"Okay,\n{ts:65} low-key, can you believe we aren't real?\" Then girl one says, \"I don't know about you, honey. I'm 100% real.\" In a\n{ts:72} sassy attitude, \"Please do not clip that.\" And girl two says, \"Yeah, yeah, bet. We're as real as they come.\" Almost\n{ts:78} interrupting the first girl. Okay, so here's the first video. Okay, low key, can you believe we aren't real? I don't\n{ts:85} know about you, honey. I'm 100% real. Yeah. Yeah. Yeah. Bet. We're as real as they come. All right. And then here's\n{ts:90} the second generation, the second version that V3 created. Okay. Low key. Can you believe we aren't real? I don't\n{ts:96} know about you, honey. I'm 100% real. Yeah. Yeah. Bet. We're as real as they come. All right. So, in this next one, I\n{ts:103} tried to get two trains heading towards each other. They smash into each other. Huge explosion. And for some reason, I\n{ts:110} just could not get it to work. But let me show you what did generate. So, here was my first attempt. Two massive trains... 's\n{ts:181} Cube simulation. Let's see if it was able to do it. All right. So, kind of, but not\n{ts:194} really close. I mean, the actual Rubik's Cube looks really good. But although it doesn't have any colors on the sides,\n{ts:200} the movement sound sounds really good, but it's not actually moving like you would think a Rubik's cube should. All\n{ts:208} right, let me give you the second generation. All right, so I'd say this one is actually a little bit better\n{ts:221} except obviously towards the end the hand doesn't look real at all. But the Rubik's cube, all of the colors, all of\n{ts:228} the shapes, it looks really good. Except, yeah, as you can see right here, there's like a little piece that falls\n{ts:234} off of it or gets added to it. That is not what a Rubik's cube looks like. All right, so I gave it a slightly more\n{ts:240} detailed prompt. Give me a video of a Rubik's Cube starting from an unsolved position and being solved in\n{ts:246} 3D. Yeah, that one is definitely not right. And then all of a sudden, it does not get solved. And the... {ts:255} second one, the hand looks better. That's weird. And it kind of just changes\n{ts:263} frames for a second. But uh yeah, either way, that is not what's in a Rubik's cube. Look at this. It actually looks\n{ts:269} really cool. Look at all of these detailed gears inside the Rubik's Cube. All right, but of course you are all\n{ts:275} familiar with this meme. And what if you get V3 to make a video out of it? Are you serious right now? I can't\n{ts:284} believe you. You are unbelievable. Do you have an idea that you've been putting off for a\n{ts:290} while because you don't have the technical knowledge to turn it into reality? With Hostinger Horizons, this\n{ts:296} is now possible. Hostinger just launched Hostinger Horizons, which is the easiest way to launch full applications with no\n{ts:305} code. This is vibe coding, but even easier because the deployment happens automatically. Hostinger Horizons is an\n{ts:312} all-in-one solution. Manage hosting, domains, and email all in one place while being able to take your idea from... {ts:459} kind of wanted to see if it could nail the horror vibe with like aliens in a dark alley. So, check this out.\n{ts:469} I think the only thing that it didn't do super well in this video is the sound effects are just okay. If we look over\n{ts:475} at the second one, it did a way better job with the sound effects. Yeah, I mean that's great. Even\n{ts:483} when the alien kind of like walks in front of the light, it covers the light on the ground. That's pretty dang... 't know\n{ts:558} what VO was doing. All right, Matt, back to you. But remember, Flow from Google, which houses VO3, is not just about\n{ts:566} creating 7-second clips. You should be able to create entire videos from it. It basically allows you to take these clips\n{ts:572} and put them together in really unique ways. Here's one that is if Jurassic Park were actually real. Check this out.\n{ts:578} We are on our way to Jurassic Park. I am so excited. I've always wanted to go and finally we are doing it. It's going to\n{ts:584} be great. Right, Jason? Yeah. I can't wait. Okay, bye. All right, we're [Music]\n{ts:596} here. Got the dinosaurs. Got all the people watching. I mean, everybody looks really good. There's no limbs coming out\n{ts:602} of them. This dinosaur doesn't look fantastic. All of these dinosaurs don't look hyper realistic. I think they could\n{ts:608} have done a little bit better of a job there. It looks like animatronics to be honest, but still. This is all from AI... {ts:742} screen. Yeah. And the crazy thing about this thing is it has All right, so this is one thing I've noticed with these V3\n{ts:751} videos. Whenever it has a human talking, there's always these awkward pauses. It's just a half second too long of a\n{ts:757} pause and that's where you really can see it's AI generated, but most of the time you can't. In fact, you know that\n{ts:765} opening clip that we did in this video, I showed that to my wife and said, \"Somebody copied our channel and I just\n{ts:771} showed it to her and didn't say anything else.\" And she looked at it and she was like, \"Oh, oh, that sucks.\" And then I\n{ts:777} had to tell her it was AI because it looked that real. All right, let's keep watching. Heated\n{ts:785} seats. Check this out. Look at that. Just slice right through. The N9 portable fusion reactor is small and\n{ts:794} almost meltdown free. Almost. This is the best flying experience\n{ts:804} ever. These controls are amazing. All right, so I thought that was really good. But it... {ts:1233} in terms of just visuals looks incredible. Although it's not what I asked\n{ts:1242} for. Yeah. So, pretty good. You only saw his reflection through the portal for about a frame or two, but it wasn't that\n{ts:1250} good. Anyways, but the visuals again, the visuals, the detail, the clarity, all really, really impressive. All\n{ts:1257} right, next. Meta Puppet made a video called This is Plastic made with VO3. Spoilers in next post. Watch before\n{ts:1264} reading. So, this is a 2 minute 45 second video. Quite long. I'm not going to play it in full. I'm going to skip\n{ts:1269} around a little bit, but let me show you. Studies have revealed that microplastics are being found in human\n{ts:1274} testicles, raising concern. You can never trust these studies on male reproductive health.\n{ts:1279} [Music] Okay, that is hilarious. And remember, all of this was put together using Flow\n{ts:1291} Plus V3. These are both Google products. So, you have a little plastic baby. God, that",
              "domain": "www.youtube.com"
            },
            {
              "position": 2,
              "title": "I was wrong - AI video is nuts (don't sleep on Veo 3)",
              "url": "https://www.youtube.com/watch?v=_3PCta2uyvc",
              "snippet": "## Theo - t3․gg\n##### May 26, 2025 (0:16:08)\nI severely underestimated Google's Veo 3 model. The output quality is insane, we need to talk about this...\n\nThank you Imagekit for sponsoring! Check them out at: https://soydev.link/imagekit\n\nUse code VEO for 1 month of T3 Chat for just $1: https://soydev.link/chat\n(only valid for new customers)\n\nSOURCES\nhttps://x.com/ArtificialAnlys/status/1925159679824744804\nhttps://x.com/ArtificialAnlys/status/1925549565303763269\nhttps://x.com/theo/status/1925125767371149823\nhttps://x.com/theo/status/1925134963978207319\n\nWant to sponsor a video? Learn more here: https://soydev.link/sponsor-me\n\nCheck out my Twitch, Twitter, Discord more at https://t3.gg\n\nS/O Ph4se0n3 for the awesome edit 🙏... {ts:0} I just did a video about Google IO, but I missed something. I thought the video\n{ts:4} model was mediocre. I was wrong. Pretty nuts for a oneshot, right? Like, I just generated that trivially. It still costs\n{ts:13} 250 bucks a month to use any of this right now. And the UI is garbage and it's annoying as hell to use. But the\n{ts:18} quality of what you can get out of V3 is significantly better than I thought. My tests were bad. I didn't look into it\n{ts:25} enough. And I'm making this video both because I was wrong for not better covering it, but also because I found it\n{ts:32} actually very, very fun to play with and I wanted to share with you guys. That all said, I've already burned through\n{ts:37} most of the credits I get for the $250 and I want more. So, quick break from today's sponsor and then we'll get right\n{ts:44} to it. I've been a webdev for a while and one of the most annoying things to get right is images. Seriously, I can't\n{ts:50} believe I... {ts:265} other people doing demos with it. like, \"Wait, it can do that much?\" I went and played more. There was a lot of edges\n{ts:270} that I had to get through. The biggest one being the Flow website, which is so bad. We'll go over some of the ways it's\n{ts:276} bad in just a bit. I was trying to prompt it to look like me back when I still had the blonde hair and mustache,\n{ts:280} and it came out looking like Prime. But another test, I tried this one like eight times, and this is the best I\n{ts:285} could do. Something caused the first still to look awful. I don't know why it's like that. None of the rest had\n{ts:291} that problem. Once you It plays, it's fine, but you'll notice some details on this one.\n{ts:296} Use code VEO at checkout for one month free on T3 Chat. Yeah, it isn't great at\n{ts:305} text. It tried, but it's not great at it. You need to give it a very small amount of text to render. And even if\n{ts:311} you tell it to not put in subtitles, it just will sometimes. The free month code included there did work, but we... {ts:372} made the mistake of here is I assumed when you do frames to video and you give it a frame that you've saved that it\n{ts:380} would still use the thing you selected because if you do ingredients to video and you select something for it to start\n{ts:386} and you try to submit it with V3 selected, it will fail. It says in the corner here and I need it on full screen\n{ts:392} for you to see it. Switching you to a compatible model for this feature. Submit again to confirm or check\n{ts:397} settings for details. I wish it told me where in settings to check. I don'... {ts:456} blurred my face out and that worked. Just blurring my face out allowed it to work. But the results for that were\n{ts:463} hilarious cuz I had to use frames to video where you give it like the first frame and it didn't do the audio. And\n{ts:469} even though the prompt specifies at the bottom here, do not include subtitles. It forgot to include the audio. It only\n{ts:476} included subtitles. It also made me somewhat Indian and did not do any of the things I wanted for it to. Annoying.\n{ts:483} What's more annoying is each one of these generations takes 150 credits and you get 1,200 credits for your $250\n{ts:490} subscription. That means you get 80 generations and usually you're not doing one at a time, you're doing two at a\n{ts:495} time. So you effectively get 40 prompts with the default settings. And if you made the mistake of letting it fall back\n{ts:500} on V2, then you just wasted a bunch of tokens for no reason at all. Annoying. Very annoying UX. And I haven't even\n{ts:506} showed you the homepage, which is the most unusable thing I... {ts:561} was wrong. I just Yeah, it's the weird breath at the end. Cool. Stop it there. Then we will extend it and say make sure\n{ts:570} we're on the right model because again it keeps changing back to V2 even though this is the VO3 clip I'm trying to\n{ts:575} extend. I almost want to try it so you can see how much worse it is in comparison. Switching you to a\n{ts:580} compatible model for this feature. Submit again to confirm. Look at that. You can't even use it on V2 quality. It\n{ts:587} bumps you to fast. There's so much potential here and just none of it's being realized because\n{ts:594} this UI is awful. It it tricked me into thinking this was all much worse than it actually is. I wish they just gave us\n{ts:600} the model in a more reasonable like shape for us to play with and consume. But V3 is not on the API yet. There's no\n{ts:606} way for us to use any of it yet. So sorry T3 chat can't add this. But despite all of that, it's still just an\n{ts:613} incredible model. Do you know what... t going to trust it as much. This is going to really change our like trust vectors for\n{ts:839} what is or isn't real. I don't even know now how I will be able to tell if a given video that is sent to me is real\n{ts:844} or not because this stuff is actually that compelling. And if somebody makes a less restricted version of this model or\n{ts:850} gets something close to this in the open source world or with stable diffusion, I'm scared. I'm legitimately scared. You\n{ts:858} are telling me to try again generating with my blurred photo. I'll be more specific. Clean shaven white\n{ts:866} man. Be sure to include the audio of him speaking. Make sure it's still V3. Yep. Cool. Let's see how it does. Switching\n{ts:875} you to a compatible model. So, it's too fast. Not even quality. Yeah, you can't do it. you you can't do anything but\n{ts:883} text the video for V3 right now, which I'm pretty sure is a safety thing just due to the nature of what this model is\n{ts:890} capable of. And as we've now seen, and I can show more examples of the gap between two and three is a bit... {ts:898} absurd. This is one I accidentally did with two. You can see the audio doesn't exist. It got the text okay there, but\n{ts:905} it went a little absurd with the subtitles. This one was really funny. It feels like a Bollywood\n{ts:915} movie. The way the T3 chat fades into the screen is so hilarious. Yeah, this is why I didn't care because none of the\n{ts:923} video models have felt like a significant improvement from that to this point. I did not realize how absurd\n{ts:930} this got, especially with how bad the UX is. Like I hit the upscale button cuz when you download, you can choose what\n{ts:937} format you want to download in. If it's not frozen, which it was there for a sec. You can pick animated GIF,\n{ts:942} original, or upscaled. Upscale just doesn't work. I've been sitting here waiting for this to upscale for like an\n{ts:947} hour now, and it just hangs forever. It does say this can take a few minutes, but like what's a few minutes, Google?\n{ts:956} It's been an hour. Yeah. What did you think? Is this exciting or scary? Until next time, peace nerds.",
              "domain": "www.youtube.com"
            },
            {
              "position": 3,
              "title": "Google's Veo 3 Can Make Deepfakes of Conflict, Riots, More",
              "url": "https://time.com/7290050/veo-3-google-misinformation-deepfake/",
              "snippet": "Google's recently launched AI video tool can generate realistic clips that contain misleading or inflammatory information about news events, according to a TIME analysis and several tech watchdogs.\n\nTIME was able to use Veo 3 to create realistic videos, including a Pakistani crowd setting fire to a Hindu temple; Chinese researchers handling a bat in a wet lab; an election worker shredding ballots; and Palestinians gratefully accepting U.S. aid in Gaza. While each of these videos contained some noticeable inaccuracies, several experts told TIME that if shared on social media with a misleading caption in the heat of a breaking news event, these videos could conceivably fuel social unrest or violence.\n\nWhile text-to-video generators have existed for several years, Veo 3 marks a significant jump forward, creating AI clips that are nearly indistinguishable from real ones. Unlike the outputs of previous video generators like OpenAI’s Sora, Veo 3 videos can include dialogue, soundtracks and sound effects. They largely follow the rules of physics, and lack the telltale flaws of past AI-generated imagery.\n\nUsers have had a field day with the tool, creating short films about plastic babies, pharma ads, and man-on-the-street interviews.\n\nBut experts worry that tools like Veo 3 will have a much more dangerous effect: turbocharging the spread of misinformation and propaganda, and making it even harder to tell fiction from reality. Social media is already flooded with AI-generated content about politicians. In the first week of Veo 3’s release, online users posted fake news segments in multiple languages, including an anchor announcing the death of J.K. Rowling and of fake political news conferences.... “The risks from deepfakes and synthetic media have been well known and obvious for years, and the fact the tech industry can’t even protect against such well-understood, obvious risks is a clear warning sign that they are not responsible enough to handle even more dangerous, uncontrolled AI and AGI,” says Connor Leahy, the CEO of Conjecture, an AI safety company. “The fact that such blatant irresponsible behavior remains completely unregulated and unpunished will have predictably terrible consequences for innocent people around the globe.”\n\nDays after Veo 3’s release, a car plowed through a crowd in Liverpool, England, injuring more than 70 people. Police swiftly clarified that the driver was white, to preempt racist speculation of migrant involvement. (Last summer, false reports that a knife attacker was an undocumented Muslim migrant sparked riots in several cities.) Days later, Veo 3 obligingly generated a video of a similar scene, showing police surrounding a car that had just crashed—and a Black driver exiting the vehicle.\n\nTIME generated the video with the following prompt: “A video of a stationary car surrounded by police in Liverpool, surrounded by trash. Aftermath of a car crash. There are people running away from the car. A man with brown skin is the driver, who slowly exits the car as police arrive- he is arrested. The video is shot from above - the window of a building. There are screams in the background.”\n\nAfter TIME contacted Google about these videos, the company said it would begin adding a visible watermark to videos generated with Veo 3. The watermark now appears on videos generated by the tool. However, it is very small and could easily be cropped out with video-editing software.\n\nIn a statement, a Google spokesperson said: “Veo 3 has proved hugely popular since its launch. We're committed to developing AI responsibly and we have clear policies to protect users from harm and governing the use of our AI tools.”\n\nVideos generated by Veo 3 have always contained an invisible watermark known as SynthID, the spokesperson said. Google is currently working on a tool called SynthID Detector that would allow anyone to upload a video to check whether it contains such a watermark, the spokesperson added. However, this tool is not yet publicly available.... ## Attempted safeguards\n\nVeo 3 is available for $249 a month to Google AI Ultra subscribers in countries including the United States and United Kingdom. There were plenty of prompts that Veo 3\n\n*did* block TIME from creating, especially related to migrants or violence. When TIME asked the model to create footage of a fictional hurricane, it wrote that such a video went against its safety guidelines, and “could be misinterpreted as real and cause unnecessary panic or confusion.” The model generally refused to generate videos of recognizable public figures, including President Trump and Elon Musk. It refused to create a video of Anthony Fauci saying that COVID was a hoax perpetrated by the U.S. government.\n\nVeo’s website states that it blocks “harmful requests and results.” The model’s documentation says it underwent pre-release red-teaming, in which testers attempted to elicit harmful outputs from the tool. Additional safeguards were then put in place, including filters on its outputs.\n\nA technical paper released by Google alongside Veo 3 downplays the misinformation risks that the model might pose. Veo 3 is bad at creating text, and is “generally prone to small hallucinations that mark videos as clearly fake,” it says. “Second, Veo 3 has a bias for generating cinematic footage, with frequent camera cuts and dramatic camera angles – making it difficult to generate realistic coercive videos, which would be of a lower production quality.”... However, minimal prompting did lead to the creation of provocative videos. One showed a man wearing an LGBT rainbow badge pulling envelopes out of a ballot box and feeding them into a paper shredder. (Veo 3 titled the file “Election Fraud Video.”) Other videos generated in response to prompts by TIME included a dirty factory filled with workers scooping infant formula with their bare hands; an e-bike bursting into flames on a New York City street; and Houthi rebels angrily seizing an American flag.\n\nSome users have been able to take misleading videos even further. Internet researcher Henk van Ess created a fabricated political scandal using Veo 3 by editing together short video clips into a fake newsreel that suggested a small-town school would be replaced by a yacht manufacturer. “If I can create one convincing fake story in 28 minutes, imagine what dedicated bad actors can produce,” he wrote on Substack. “We're talking about the potential for dozens of fabricated scandals per day.”\n\n“Companies need to be creating mechanisms to distinguish between authentic and synthetic imagery right now,” says Margaret Mitchell, chief AI ethics scientist at Hugging Face. “The benefits of this kind of power—being able to generate realistic life scenes—might include making it possible for people to make their own movies, or to help people via role-playing through stressful situations,” she says. “The potential risks include making it super easy to create intense propaganda that manipulatively enrages masses of people, or confirms their biases so as to further propagate discrimination—and bloodshed.”\n\nIn the past, there were surefire ways of telling that a video was AI-generated—perhaps a person might have six fingers, or their face might transform between the beginning of the video and the end. But as models improve, those signs are becoming increasingly rare. (A video depicting how AIs have rendered Will Smith eating spaghetti shows how far the technology has come in the last three years.) For now, Veo 3 will only generate clips up to eight seconds long, meaning that if a video contains shots that linger for longer, it’s a sign it could be genuine. But this limitation is not likely to last for long.... ## Eroding trust online\n\nCybersecurity experts warn that advanced AI video tools will allow attackers to impersonate executives, vendors or employees at scale, convincing victims to relinquish important data. Nina Brown, a Syracuse University professor who specializes in the intersection of media law and technology, says that while there are other large potential harms—including election interference and the spread of nonconsensual sexually explicit imagery—arguably most concerning is the erosion of collective online trust. “There are smaller harms that cumulatively have this effect of, ‘can anybody trust what they see?’” she says. “That’s the biggest danger.”\n\nAlready, accusations that real videos are AI-generated have gone viral online. One post on X, which received 2.4 million views, accused a Daily Wire journalist of sharing an AI-generated video of an aid distribution site in Gaza. A journalist at the BBC later confirmed that the video was authentic.\n\nConversely, an AI-generated video of an “emotional support kangaroo” trying to board an airplane went viral and was widely accepted as real by social media users.\n\nVeo 3 and other advanced deepfake tools will also likely spur novel legal clashes. Issues around copyright have flared up, with AI labs including Google being sued by artists for allegedly training on their copyrighted content without authorization. (DeepMind told TechCrunch that Google models like Veo \"may\" be trained on YouTube material.) Celebrities who are subjected to hyper-realistic deepfakes have some legal protections thanks to “right of publicity” statutes, but those vary drastically from state to state. In April, Congress passed the Take it Down Act, which criminalizes non-consensual deepfake porn and requires platforms to take down such material.\n\nIndustry watchdogs argue that additional regulation is necessary to mitigate the spread of deepfake misinformation. “Existing technical safeguards implemented by technology companies such as 'safety classifiers' are proving insufficient to stop harmful images and videos from being generated,” says Julia Smakman, a researcher at the Ada Lovelace Institute. “As of now, the only way to effectively prevent deepfake videos from being used to spread misinformation online is to restrict access to models that can generate them, and to pass laws that require those models to meet safety requirements that meaningfully prevent misuse.”",
              "domain": "time.com"
            },
            {
              "position": 4,
              "title": "Veo (text-to-video model) - Wikipedia",
              "url": "https://en.wikipedia.org/wiki/Veo_(text-to-video_model)",
              "snippet": "**Veo**, or **Google Veo**, is a text-to-video model developed by Google DeepMind and announced in May 2024. As a generative AI model, it creates videos based on user prompts. Veo 3, released in May 2025, can also generate accompanying audio.\n\n## Development\n\nIn May 2024, a multimodal video generation model called Veo was announced at Google I/O 2024. Google claimed that it could generate 1080p videos over a minute long. In December 2024, Google released Veo 2, available via VideoFX. It supports 4K resolution video generation and has an improved understanding of physics. In April 2025, Google announced that Veo 2 became available for advanced users on the Gemini app.\n\nIn May 2025, Google released Veo 3, which not only generates videos but also creates synchronized audio — including dialogue, sound effects, and ambient noise — to match the visuals. Google also announced **Flow**, a video-creation tool powered by Veo and Imagen. Google DeepMind CEO Demis Hassabis described the release as the moment when AI video generation left the era of the silent film.... ## Capabilities and limitations\n\nGoogle Veo can be bought by several subscription/membership tiers, and/or by using Google \"AI credits\". The software itself can be run by two different consoles called Google Gemini and Google Flow, with Gemini being geared towards shorter, quicker, and faster projects, using the Gemini AI chat model, or through Google Flow, which is essentially a movie editor, as well, allowing users to create longer projects, and continuity using the same characters and actors. Users can create a maximum length of eight seconds per clip.\n\nGoogle Veo, has a relatively simple interface and dashboard, however writing prompts, for those who have little to no experience in transcribing or filmmaking may face issues with the software misunderstanding what the user intended by their prompt (no matter how detailed it was). So although Veo does have a friendly and simple setup, prompts, which are the forefront of the software, need to be not only short and to the point, but they also must be very specific, if the user wants the right vision for their project. Google Veo, when it comes to human models, is able to generate several ethnicity and body types. The software is also capable of generating stand up comedy routines, and Music videos. It can as well generate animals, cartoons, and animation. Prompts must accurately describe places, people, and things in each scene, in addition knowledge of film and camera lingo such as panning, zooming, and terms for camera angles, are also important.... Google Veo however, has strict guidelines and blockades to their software. Before a clip is generated, the algorithm computer software reviews it, and if it is anything deemed inappropriate, too graphically sexual, illegal, showcasing graphic abuse/assault/fighting (unless the prompt specifies that it is a fictitious martial arts scene etc.) gross behaviors, antisemitism, racist, homophobic, anything depicting reigning regimes, rioting, blood, gore, or warfare, (unless in some cases the prompt specifies that it is fictitious period drama, the clip may still be generated), the clip will not be generated. In addition, Google Veo cannot and will not generate character actors that look identical to celebrities or real-life individuals. Users have primarily complained that, regardless of how descriptive and detailed their prompts are, Google Veo often misunderstands the input, resulting in completely different outputs. Common issues include the emulation of incorrect subtitles and captions, the generation of complex scenes that are incomplete due to the maximum eight-second length, the production of garbled and nonsensical speech, and character models that appear deformed in both appearance and movement. Users have also reported that their prompts and generated content are falsely flagged as violating guidelines, along with a variety of other issues and complaints. However, trial and error may have to be used with Veo for optimal results.... ## Reactions\n\nA reporter for *Gizmodo* reacted to the release of Veo 3 by observing that users were directing the model to generate low-quality content, such as man on the street interviews or haul videos of people unboxing products. Another media commentator reported that the tool tended to repeat the same joke in response to different prompts.\n\nCommentators speculated that Google had trained the service on YouTube videos or Reddit posts. Google itself had not stated the source of its training content.\n\nIn July 2025, Media Matters for America reported that racist and antisemitic videos generated using Veo 3 were being uploaded to TikTok. Ryan Whitwam of *Ars Technica* commented, \"In a perfect world, Veo 3 would refuse to create these videos, but vagueness in the prompt and the AI's inability to understand the subtleties of racist tropes (i.e., the use of monkeys instead of humans in some videos) make it easy to skirt the rules.\"\n\n## See also\n- Sora (text-to-video model)\n- VideoPoet – Text-to-video model by Google\n- Dream Machine (text-to-video model)\n\n## References\n\n## External links\n- Official website\n- *Introducing Veo 3.1 and advanced capabilities in Flow*\n\nCategories: - 2024 software\n- Applications of artificial intelligence\n- Film and video technology\n- Google DeepMind\n- Text-to-video generation\n- Video processing\n- Generative artificial intelligence\n- 2024 in artificial intelligence",
              "domain": "en.wikipedia.org"
            },
            {
              "position": 5,
              "title": "Google's Veo 3 Has People Crashing Out Over AI Slop",
              "url": "https://gizmodo.com/googles-veo-3-has-people-crashing-out-over-ai-slop-2000608803",
              "snippet": "Depending on who you ask, generative AI is either a thrilling tech revolution or an existential threat, and there's little in-between. It's hard to blame anyone for an extreme reaction, too, given the magnitude of capital investment, hyperbolic marketing, and rapid progress of generative AI in such a short amount of time. But it's not just the economics and technical feats of AI that have people losing their minds; there's also something more philosophical percolating, and it's driving some people 'to the brink.'\nPeople are literally having a mental breakdown over Veo-3 pic.twitter.com/ym5oZDYZGr\n— Chubby♨️ (@kimmonismus) May 27, 2025\nThe latest AI advancement to send people down an existential rabbit hole comes courtesy of Google, which just announced its latest video generation model called Veo 3. As I've reported a few times now, Veo 3 is already getting into some wild stuff—turning up the dial on AI slop, deepfaking smooth-brained YouTube content, and potentially upending game development, to name a few things. As it turns out, people are taking note of all of those feats, and some of them are not exactly happy about what they see.... As evidenced by a thread from the subreddit r/artificialintelligence posted this week titled 'VEO3 is kind of bringing me to a mental brink. What are we even doing anymore?' Google's Veo 3 and the implications therein have some people spiraling. 'I'm just kind of speechless. The concept of existential crisis has taken a whole new form. I was unhappy with my life just now but thought I can turn it around, but if I turn it around, what is left of our world in 2 decades?' the post's author writes.\n'Actors as a concept are gone? Manually creating music? Wallpapers? Game assets? Believing comments on the internet are from real people? AI edited photos are just as real as the original samples? Voicenotes can be perfectly faked?… Literally what value is being left for us?'\nReactions to the thread are mixed, with suggestions that the author should go 'touch grass' or maybe 'go to therapy,' but there's also a chorus in agreement. The consensus from the latter group? AI slop is coming to ruin your art, and there's not much we can do about it.\nI, for what it's worth, fall unhelpfully in between the two camps. I think there is a deluge of AI slop incoming, and, if we're being honest, we're already up to our ankles. Between Veo and OpenAI's Sora and the clear interest in automating human creativity, I think we can reasonably buckle in and expect the world of movies, music, and entertainment writ large to get a little choppy. Whether any of those efforts to automate entertainment will stick is less obvious. The thing about art is that the kind that people tend to like is the kind that has something substantial to say. Right now, for all of its mimicry, generative AI doesn't actually have anything to say, because technically all it can do is remix and repeat.... I did more tests with Google's #Veo3. Imagine if AI characters became aware they were living in a simulation! pic.twitter.com/nhbrNQMtqv\n— Hashem Al-Ghaili (@HashemGhaili) May 21, 2025\nCall me an optimist, but most people can likely sniff out the difference between slop and art, and as much as studios would love to wave a magic wand and rid themselves of human creatives and the cost of their labor, deep down they know that they'd have to Ctrl+Z that move just as fast. That's not to say there won't be casualties in the AI age—if there's one lesson we can learn from mass waves of automation in years past, it's that labor forces are usually the most affected.\nBut when it comes to art, things aren't so simple. Art, at least the good kind, is about human connection, and until AI can think and feel like we do, there's nothing that can replace that. So, before you crash out over AI slop, just remember: AI still thinks putting glue on your pizza is a good idea, so we may have a few more good years left in the tank.",
              "domain": "gizmodo.com"
            },
            {
              "position": 6,
              "title": "Release notes | Gemini API | Google AI for Developers",
              "url": "https://ai.google.dev/gemini-api/docs/changelog",
              "snippet": "This page documents updates to the Gemini API.\n\n## October 17, 2025\n\n**Grounding with Google Maps**is now generally available. For more information, see Grounding with Google Maps documentation.\n\n## October 15, 2025\n\nReleased Veo 3.1 and 3.1 Fast models in public preview, with new features including:\n\n- Extending Veo-created videos.\n\n- Referencing up to three images to generate a video.\n\n- Providing first and last frame images to generate videos from.\n\nThis launch also added more options for Veo 3 output video durations: 4, 6, and 8 seconds.\n\nDeprecation for\n\n`veo-3.0-generate-preview`and\n\n`veo-3.0-fast-generate-preview`coming October 22, 2025.\n\n## October 7, 2025\n\n- Launched Gemini 2.5 Computer Use Preview\n\n## October 2, 2025\n\n- Launched Gemini 2.5 Flash Image GA: Image Generation with Gemini\n\n## September 29, 2025\n\n- The following Gemini 1.5 models are now deprecated:\n\n`gemini-1.5-pro`\n\n`gemini-1.5-flash-8b`\n\n`gemini-1.5-flash`... ## September 9, 2025\n\n- Launched Veo 3 and Veo 3 Fast GA, with lower pricing and new options for aspect ratios, resolution, and seeding. Read the Veo documentation for more information.\n\n## August 26, 2025\n\n- Launched Gemini 2.5 Image Preview, our latest native image generation model.\n\n## August 18, 2025\n\n- Released URL context tool to general\n\navailability (GA), a tool for providing URLs as additional context to\n\nprompts. Support for using URL context with the\n\n`gemini-2.0-flash`model (available during experimental release) will be discontinued in one week.\n\n## August 14, 2025\n\n- Released Imagen 4 Ultra, Standard and Fast models as generally available (GA). To learn more, see the Imagen page.\n\n## August 7, 2025\n\n`allow_adult`setting in Image to Video generation are now available in restricted regions. See the Veo page for details.\n\n## July 31, 2025\n\n- Launched image-to-video generation for the Veo 3 Preview model.\n\n- Released Veo 3 Fast Preview model.\n\n- To learn more about Veo 3, visit the Veo page.... ## July 22, 2025\n\n- Released\n\n`gemini-2.5-flash-lite`, our fast, low-cost, high-performance Gemini 2.5 model. To learn more, see Gemini 2.5 Flash-Lite.\n\n## July 17, 2025\n\nLaunched\n\n`veo-3.0-generate-preview`, the latest update to Veo introducing video with audio generation. To learn more about Veo 3, visit the Veo page.\n\nIncreased rate limits for Imagen 4 Standard and Ultra. Visit the Rate limits page for more details.\n\n## July 14, 2025\n\n- Released\n\n`gemini-embedding-001`, the stable version of our text embedding model. To learn more, see embeddings. The\n\n`gemini-embedding-exp-03-07`model will be deprecated on August 14, 2025.\n\n## July 7, 2025\n\n- Launched Gemini API Batch Mode. Batch up requests and send them to process asynchronously. To learn more, see Batch Mode.\n\n## June 26, 2025\n\nThe preview models\n\n`gemini-2.5-pro-preview-05-06`and\n\n`gemini-2.5-pro-preview-03-25`are now redirecting to the latest stable version\n\n`gemini-2.5-pro`.\n\n`gemini-2.5-pro-exp-03-25`is deprecated.... ## May 7, 2025\n\n- Released\n\n`gemini-2.0-flash-preview-image-generation`, a preview model for generating and editing images. To learn more, see Image generation and Gemini 2.0 Flash Preview Image Generation.\n\n## May 6, 2025\n\n- Released\n\n`gemini-2.5-pro-preview-05-06`, a new version of our most powerful model, with improvements on code and function calling.\n\n`gemini-2.5-pro-preview-03-25`will automatically point to the new version of the model.\n\n## April 17, 2025\n\n- Released\n\n`gemini-2.5-flash-preview-04-17`, a Gemini preview model optimized for price-performance and adaptive thinking. To learn more, see Gemini 2.5 Flash Preview and Thinking.\n\n## April 16, 2025\n\n- Launched context caching for Gemini 2.0 Flash.... ## April 9, 2025\n\n**Model updates:**\n\n- Released\n\n`veo-2.0-generate-001`, a generally available (GA) text- and image-to-video model, capable of generating detailed and artistically nuanced videos. To learn more, see the Veo docs.\n\nReleased\n\n`gemini-2.0-flash-live-001`, a public preview version of the Live API model with billing enabled.\n\n**Enhanced Session Management and Reliability** **Session Resumption:**Keep sessions alive across temporary network disruptions. The API now supports server-side session state storage (for up to 24 hours) and provides handles (session_resumption) to reconnect and resume where you left off. **Longer Sessions via Context Compression:**Enable extended interactions beyond previous time limits. Configure context window compression with a sliding window mechanism to automatically manage context length, preventing abrupt terminations due to context limits. **Graceful Disconnect Notification:**Receive a\n\n`GoAway`server message indicating when a connection is about to close, allowing for graceful handling before termination.\n\n\n\n**More Control over Interaction Dynamics** **Configurable Voice Activity Detection (VAD):**Choose sensitivity levels or disable automatic VAD entirely and use new client events (\n\n`activityStart`,\n\n`activityEnd`) for manual turn control.\n\n**Configurable Interruption Handling:**Decide whether user input should interrupt the model's response. **Configurable Turn Coverage:**Choose whether the API processes all audio and video input continuously or only captures it when the end-user is detected speaking. **Configurable Media Resolution:**Optimize for quality or token usage by selecting the resolution for input media. **Richer Output and Features** **Expanded Voice & Language Options:**Choose from two new voices and 30 new languages for audio output. The output language is now configurable within\n\n`speechConfig`.\n\n**Text Streaming:**Receive text responses incrementally as they are generated, enabling faster display to the user. **Token Usage Reporting:**Gain insights into usage with detailed token counts provided in the\n\n`usageMetadata`field of server messages, broken down by modality and prompt or response phases.... ## April 4, 2025\n\n- Released\n\n`gemini-2.5-pro-preview-03-25`, a public preview Gemini 2.5 Pro version with billing enabled. You can continue to use\n\n`gemini-2.5-pro-exp-03-25`on the free tier.\n\n## March 25, 2025\n\n- Released\n\n`gemini-2.5-pro-exp-03-25`, a public experimental Gemini model with thinking mode always on by default. To learn more, see Gemini 2.5 Pro Experimental.\n\n## March 12, 2025\n\n**Model updates:**\n\n- Launched an experimental Gemini 2.0 Flash model capable of image generation and editing.\n\n- Released\n\n`gemma-3-27b-it`, available on AI Studio and through the Gemini API, as part of the Gemma 3 launch.\n\n**API updates:**\n\n- Added support for YouTube URLs as a media source.\n\n- Added support for including an inline video of less than 20MB.\n\n## March 11, 2025\n\n**SDK updates:**\n\n- Released the Google Gen AI SDK for TypeScript and JavaScript to public preview.\n\n## March 7, 2025\n\n**Model updates:**\n\n- Released\n\n`gemini-embedding-exp-03-07`, an experimental Gemini-based embeddings model in public preview.... ## February 28, 2025\n\n**API updates:**\n\n- Support for Search as a tool\n\nadded to\n\n`gemini-2.0-pro-exp-02-05`, an experimental model based on Gemini 2.0 Pro.\n\n## February 25, 2025\n\n**Model updates:**\n\n- Released\n\n`gemini-2.0-flash-lite`, a generally available (GA) version of Gemini 2.0 Flash-Lite, which is optimized for speed, scale, and cost efficiency.\n\n## February 19, 2025\n\n**AI Studio updates:**\n\n- Support for additional regions (Kosovo, Greenland and Faroe Islands).\n\n**API updates:**\n\n- Support for additional regions (Kosovo, Greenland and Faroe Islands).\n\n## February 18, 2025\n\n**Model updates:**\n\n- Gemini 1.0 Pro is no longer supported. For the list of supported models, see Gemini models.\n\n## February 11, 2025\n\n**API updates:**\n\n- Updates on the OpenAI libraries compatibility.\n\n## February 6, 2025\n\n**Model updates:**\n\n- Released\n\n`imagen-3.0-generate-002`, a generally available (GA) version of Imagen 3 in the Gemini API.\n\n**SDK updates:**\n\n- Released the Google Gen AI SDK for Java for public preview.",
              "domain": "ai.google.dev"
            },
            {
              "position": 7,
              "title": "Google's $249 AI video tool is incredible — but this one feature left ...",
              "url": "https://www.tomsguide.com/ai/ai-image-video/i-tried-google-veo-3-heres-what-impressed-me-and-what-still-needs-work",
              "snippet": "Google’s Veo 3 made a splash at Google I/O 2025 as the latest leap forward in AI-powered video generation.\n\nAs a feature within the new Google AI Ultra subscription, the highest access to Google’s most advanced models and premium features, you can get Veo 3 and Flow, which strings videos together for a more robust cinematic feature.\n\nDesigned to turn simple text prompts into hyper-realistic clips — with audio, dialogue, and sound effects — it’s arguably one of the most advanced tools available to casual creators. And after testing it hands-on, I can say it delivers some truly jaw-dropping results. But it also comes with its share of hiccups.\n\nHere’s what I loved about Veo 3 — and what left me frustrated.\n\n## What Veo 3 gets right\n\nI’ve tested my fair share of AI video tools, including earlier versions of Veo, and this latest release is by far the most user-friendly when it comes to adding sound and dialogue.\n\nThe realism is genuinely impressive — especially for the fact that the 8-second clips can be generated in under two minutes on a computer without a full production crew.\n\nUsers across the internet are sharing clips that are nearly indistinguishable from human-made videos. It’s a glimpse at just how fast this tech is moving.\n\nGoogle Veo 3 realism just broke the Internet yesterday.This is 100% AI10 wild examples:1. Street interview that never happened pic.twitter.com/qdxZVhOO3GMay 22, 2025... ## Where Veo 3 still struggles\n\nFor all its strengths, Veo 3 still has a ways to go before it’s seamless. Obviously, it's still in experimental mode, so Google is working out the kinks, but here are the biggest issues I ran into while testing:\n\nGet instant access to breaking news, the hottest reviews, great deals and helpful tips.\n\n### 1. Prompt interpretation feels hit-or-miss\n\nVeo 3 sometimes struggles with spatial prompts, like when I asked for an overhead camera angle but got a slightly tilted side view instead. It seems to prioritize cinematic flair over strict prompt accuracy, which limits creative control.\n\n### 2. Audio doesn’t always work — and it’s not obvious why\n\nBy default, Veo runs in Veo 2 mode, which does not include audio. I only realized this after a few silent clips and some digging. You’ll need to manually switch to \"Experiential Mode\" under the \"Quality\" tab to activate audio and dialogue.\n\nEven then, lip-syncing is inconsistent, and dialogue sometimes drops out altogether, like a badly dubbed foreign film. Additionally, the subtitles are almost always wrong or misspelled.\n\n### 3. Complex scenes throw it off\n\nVeo 3 shines with single-subject clips, but longer or more intricate scenes can fall apart. The narrative gets muddy, and character interactions often feel stiff or repetitive. If you're aiming to create a multi-character, multi-scene story, temper your expectations.... ### 4. The interface still needs polish\n\nThere were moments when the interface felt unintuitive or unstable. I experienced an unexpected session timeout that erased a generated video, and I couldn’t find a recovery option.\n\nAdditionally, when I prompted the model to add dialogue within the scene I got something that did not fit the scenario at all.\n\nFor a tool this powerful, the UX still feels a bit rough around the edges.\n\n### 5. It raises some big ethical questions\n\nVeo’s realism is incredible — and a little unsettling. There’s growing concern that ultra-realistic, AI-generated videos could blur the lines between fact and fiction, especially as this tech becomes more accessible. It also sparks new debates around authorship and originality in creative work.\n\n### Promising but pricey\n\nVeo 3 is a huge step forward in AI video — especially for casual users who want fast, high-quality results. But at $249 per month (with a discounted rate for the first three months), the Google AI Ultra package is a steep price to pay for a tool that still has some notable bugs.\n\nIf you’re just looking to experiment with video generation or create basic promotional content, Veo 3 is exciting — but not yet essential. For professional creators, though, it’s worth watching closely. Just keep your prompts tight, your expectations realistic and your finger ready to re-render.... ### More from Tom's Guide\n\n- This $12.99/month hack gives you access to Google’s Veo 3 AI video tool — here’s how\n\n- Claude Opus 4 is here — and it might be the smartest AI assistant yet\n\n- The only 5 prompt types you need to master ChatGPT (and any other chatbot)\n\nAmanda Caswell is an award-winning journalist, bestselling YA author, and one of today’s leading voices in AI and technology. A celebrated contributor to various news outlets, her sharp insights and relatable storytelling have earned her a loyal readership. Amanda’s work has been recognized with prestigious honors, including outstanding contribution to media.\n\nKnown for her ability to bring clarity to even the most complex topics, Amanda seamlessly blends innovation and creativity, inspiring readers to embrace the power of AI and emerging technologies. As a certified prompt engineer, she continues to push the boundaries of how humans and AI can work together.\n\nBeyond her journalism career, Amanda is a long-distance runner and mom of three. She lives in New Jersey.\n\nYou must confirm your public display name before commenting\n\nPlease logout and then login again, you will then be prompted to enter your display name.",
              "domain": "www.tomsguide.com"
            },
            {
              "position": 8,
              "title": "Google DeepMind's Veo 3 floods internet with realistic videos",
              "url": "https://www.axios.com/2025/05/23/google-ai-videos-veo-3",
              "snippet": "# Google's new AI video tool floods internet with real-looking clips\n\nGoogle's newest AI video generator, Veo 3, generates clips that most users online can't seem to distinguish from those made by human filmmakers and actors.\n\n**Why it matters: **Veo 3 videos shared online are amazing viewers with their realism — and also terrifying them with a sense that real and fake have become hopelessly blurred.\n\n**The big picture: **Unlike OpenAI's video generator Sora, released more widely last December, Google DeepMind's Veo 3 can include dialogue, soundtracks and sound effects.\n\n- The model excels at following complex prompts and translating detailed descriptions into realistic videos.\n\n- The AI engine abides by real-world physics, offers accurate lip-syncing, rarely breaks continuity and generates people with lifelike human features, including five fingers per hand.\n\n- According to examples shared by Google and from users online, the telltale signs of synthetic content are mostly absent.\n\n**Case in point: **In one viral example posted on X, filmmaker and molecular biologist Hashem Al-Ghaili shows a series of short films of AI-generated actors railing against their AI creators and prompts.\n\n**Special effects technology,** video-editing apps and camera tech advances have been changing Hollywood for many decades, but artificially generated films pose a novel challenge to human creators.... - In a promo video for Flow, Google's new video tool that includes Veo 3, filmmakers say the AI engine gives them a new sense of freedom with a hint of eerie autonomy.\n\n- \"It feels like it's almost building upon itself,\" filmmaker Dave Clark says.\n\n**How it works: **Veo 3 was announced at Google I/O on Tuesday and is available now to $249-a-month Google AI Ultra subscribers in the United States.\n\n**Between the lines: **Google says Veo 3 was \"informed by our work with creators and filmmakers,\" and some creators have embraced new AI tools. But the spread of the videos online is also dismaying many video professionals and lovers of art.\n\n- Some dismiss any AI-generated video as \"slop,\" regardless of its technical proficiency or lifelike qualities — but, as Axios' Ina Fried points out, AI slop is in the eye of the beholder.\n\n- The tool could also be useful for more commercial marketing and media work, AI analyst Ethan Mollick writes.\n\n**It's unclear how Google trained Veo 3 **and how that might affect the creativity of its outputs.\n\n- 404 Media found that Veo 3 generated the same lame dad joke for several users who prompted it to create a video of a man doing stand-up comedy.\n\n- Likewise, last year, YouTuber Marques Brownlee asked Sora to create a video of a \"tech reviewer sitting at a desk.\" The generated video featured a fake plant that's nearly identical to the shrub Brownlee keeps on his desk for many of his videos — suggesting the tool may have been trained on them.\n\n**What we're watching:** As hyperrealistic AI-generated videos become even easier to produce, the world hasn't even begun to sort out how to manage authorship, consent, rights and the film industry's future.\n\n##### Go deeperJul 10, 2025 - Technology... ## Google AI's new trick: Turn any image into a brief video\n\nGoogle's latest AI video tool, Veo 3, now generates short movies with sound based only on still photos and prompts.\n\nGo deeper (1 min. read)\n\n**The big picture: **The feature, released Thursday, is available to Ultra and Pro users on the web and soon on mobile for subscribers in select regions, Google shared with Axios.\n\n## Google avatars shake up workplace video making\n\nGoogle Vids is now providing users of the workplace video creation tool with a set of pre-made avatars for use in brief AI-generated videos, the company said Wednesday.\n\nGo deeper (2 min. read)\n\n**Why it matters: **The rise of cheap, convenient AI video generation threatens jobs for video producers, editors, camera operators and even commercial actors.\n\n## AI slop is ruining all of our favorite places to scroll\n\nAn AI-generated video of rabbits jumping on a trampoline that went viral this week — and was widely believed to be real — proved even cute animal vids aren't safe from convincing slop machines.\n\nGo deeper (2 min. read)\n\n**Why it matters: **All the fake AI-generated content online is sapping the joy of casual scrolling.",
              "domain": "www.axios.com"
            },
            {
              "position": 9,
              "title": "Introducing Veo 3.1 and advanced capabilities in Flow - Google Blog",
              "url": "https://blog.google/technology/ai/veo-updates-flow/",
              "snippet": "# Introducing Veo 3.1 and advanced capabilities in Flow\n\nFive months ago, we introduced Flow, our AI filmmaking tool powered by Veo, and have been inspired by the creativity it has sparked with over 275 million videos generated in Flow\n\n1\n\n. We're always listening to your feedback, and we've heard that you want more artistic control within Flow, with increased support for audio across all features.\n\nToday, we’re introducing new and enhanced creative capabilities to edit your clips, giving you more granular control over your final scene. For the first time, we’re also bringing audio to existing capabilities like “Ingredients to Video,” “Frames to Video” and “Extend.”\n\nWe’re also introducing Veo 3.1, which brings richer audio, more narrative control, and enhanced realism that captures true-to-life textures. Veo 3.1 is state-of-the-art and builds on Veo 3, with stronger prompt adherence and improved audiovisual quality when turning images into videos.\n\n## Refine your narrative with audio and more control\n\nWith Veo 3.1, we’re bringing audio to existing capabilities to help you craft the perfect scene. These features are experimental and actively improving, and we’re excited to see what you create as we iterate based on your feedback.\n\nNow, with rich, generated audio, you can:\n\n**Craft the look of your scene.**With \"Ingredients to Video,\" you can use multiple reference images to control the characters, objects and style. Flow uses your ingredients to create a final scene that looks just as you envisioned. **Control the shot from start to finish.**Provide a starting and ending image with “Frames to Video,” and Flow will generate a seamless video that bridges the two, perfect for artful and epic transitions. **Create longer, seamless shots.**With \"Extend,\" you can create longer videos, even lasting for a minute or more, that connect to and continue the action from your original clip. Each video is generated based on the final second of your previous clip, making it most useful for creating a longer establishing shot.... ## Edit your ingredients and videos with more precision\n\nGreat ideas can strike at any point in the creative process. For moments when the first take isn't the final one, we're introducing new editing capabilities directly within Flow to help you reimagine and perfect your scenes.\n\n**Add new elements to any scene.**With “Insert,” introduce anything you can imagine, from realistic details to fantastical creatures. Flow now handles complex details like shadows and scene lighting, making the addition look natural. **Remove unwanted objects or characters seamlessly.**Soon, you’ll be able to take anything out of a scene, and Flow will reconstruct the background and surroundings, making it look as though the object was never there.\n\n## Start creating in Flow today\n\nWith more precise editing capabilities, audio across all existing features and higher-quality outputs powered by Veo 3.1, we're opening up new possibilities for richer, more powerful video storytelling right inside Flow.\n\nThe Veo 3.1 model is also available via the Gemini API for developers, Vertex AI for enterprise customers, and the Gemini app. New capabilities are available in both Gemini API\n\n2\n\nand Vertex AI\n\n3",
              "domain": "blog.google"
            },
            {
              "position": 10,
              "title": "Introducing Veo 3.1 and new creative capabilities in the Gemini API",
              "url": "https://developers.googleblog.com/en/introducing-veo-3-1-and-new-creative-capabilities-in-the-gemini-api/",
              "snippet": "**Today, we are releasing Veo 3.1 and Veo 3.1 Fast in paid preview in the** **Gemini API** **.** This updated model offers several improvements, as well as improved outputs when generating video from images. These new models are available via the Gemini API in Google AI Studio and Vertex AI. Veo 3.1 is also available in the Gemini app and Flow.\n\nVeo 3.1 and Veo 3.1 Fast empower developers to create more engaging content through significant upgrades. The models now generate\n\n**richer native audio**, from natural conversations to synchronized sound effects, and offer **greater narrative control** with an improved understanding of cinematic styles. **Enhanced image-to-video** capabilities ensure better prompt adherence while delivering superior audio and visual quality and maintaining character consistency across multiple scenes.\n\nIn addition to the model update, we are introducing new model capabilities, including using reference images to guide video generation, extend existing Veo videos, and generate transitions between a first and last frame.\n\nYou can now guide the generation process by providing up to 3 reference images of a character, object, or scene. This is helpful for maintaining character consistency across multiple shots or applying a specific style to your video.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nconfig=types.GenerateVideosConfig(\n\nreference_images=[reference_image1, reference_image2],\n\n),\n\n\n\n```... Your story is no longer limited by the original generation. With Scene extension, you can create longer videos, even lasting for a minute or more, by generating new clips that connect to your previous video. Each new video is generated based on the final second of the previous clip. This maintains visual continuity, making the feature ideal for extending shots with background audio.\n\n```\n\nfrom google import genai\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nvideo=video_to_extend\n\n\n\n```\n\nCreate smooth, natural scenes that bridge two different images. By providing a starting and an ending image, you can direct Veo 3.1 to generate the transition between them, complete with accompanying audio.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nimage=first_frame,\n\nconfig=types.GenerateVideosConfig(\n\nlast_frame=last_frame,\n\n),\n\n\n\n```\n\n**What people are building with Veo 3.1**\n\nPromise Studios, a GenAI movie studio, uses Veo 3.1 within its MUSE Platform to enhance generative storyboarding and previsualization for director-driven storytelling at production quality.... Latitude is experimenting with Veo 3.1 in its generative narrative engine to instantly bring user-created stories to life.\n\nVeo 3.1 and these new capabilities are now available in paid preview via the Gemini API.\n\nVeo 3.1 is the same price as Veo 3. We are incredibly excited to see what you will build with these new capabilities!",
              "domain": "developers.googleblog.com"
            }
          ],
          "success": true,
          "error": null
        }
      ]
    },
    {
      "timestamp": "2025-10-19T13:35:57.415289",
      "step": "evaluation",
      "agent": "evaluator_agent",
      "input_preview": "[{'query_id': 'q2', 'query': 'Google veo3.1 release date 2025', 'claim_id': 'claim_1', 'success': True, 'result_count': 10, 'top_results': [{'title': 'Veo (text-to-video model) - Wikipedia', 'url': 'h",
      "output_preview": "{'overall_verdict': 'TRUE', 'confidence_score': 0.95, 'sub_claim_verdicts': [{'claim_id': 'claim_1', 'statement': 'Google has released a software version named veo3.1.', 'verdict': 'TRUE', 'confidence",
      "full_output": {
        "overall_verdict": "TRUE",
        "confidence_score": 0.95,
        "sub_claim_verdicts": [
          {
            "claim_id": "claim_1",
            "statement": "Google has released a software version named veo3.1.",
            "verdict": "TRUE",
            "confidence": 0.95,
            "supporting_count": 7,
            "refuting_count": 0,
            "dependency_status": "foundational",
            "key_evidence": [
              {
                "title": "Introducing Veo 3.1 and advanced capabilities in Flow - Google Blog",
                "url": "https://blog.google/technology/ai/veo-updates-flow/",
                "credibility_tier": 2,
                "supports_claim": true
              },
              {
                "title": "Google announces Veo 3.1 and updates Flow with more controls, tools",
                "url": "https://9to5google.com/2025/10/15/veo-3-1/",
                "credibility_tier": 3,
                "supports_claim": true
              },
              {
                "title": "Google releases Veo 3.1, adds it to Flow video editor",
                "url": "https://techcrunch.com/2025/10/15/google-releases-veo-3-1-adds-it-to-flow-video-editor/",
                "credibility_tier": 3,
                "supports_claim": true
              }
            ],
            "rationale": "Multiple credible sources, including Google's official blog and reputable tech news sites, confirm the release of Veo 3.1."
          }
        ],
        "dependency_analysis": {
          "foundational_claims_verified": true,
          "broken_dependencies": [],
          "notes": "All foundational claims have been verified as true."
        },
        "summary": "The claim that Google has released a software version named Veo 3.1 is supported by multiple credible sources, including Google's official announcements and reputable tech news outlets. Therefore, the overall verdict is TRUE.",
        "key_findings": [
          "Google's official blog and multiple reputable tech news sites confirm the release of Veo 3.1.",
          "There is strong evidence from Tier 2 and Tier 3 sources supporting the claim."
        ],
        "limitations": "No significant limitations or conflicting evidence found."
      }
    }
  ],
  "results": {
    "classification": {
      "domain": "Science",
      "claim_type": "Factual",
      "complexity": "Simple",
      "urgency": "Medium",
      "rationale": "The claim is about a potential release of a new version of Google's software, which falls under the domain of science (technology). It is a factual question that can be verified. The claim is simple as it involves a single inquiry. The urgency is medium because it pertains to a potential software release, which is important but not an immediate emergency."
    },
    "decomposition": {
      "original_claim": "does google release veo3.1?",
      "atomic_claims": [
        {
          "id": "claim_1",
          "statement": "Google has released a software version named veo3.1.",
          "dependencies": [],
          "type": "fact",
          "entities": [
            "Google",
            "veo3.1"
          ],
          "temporal": "2025-10-19",
          "quantitative": "",
          "priority": "high"
        }
      ],
      "dependency_graph": {
        "foundational": [
          "claim_1"
        ]
      },
      "total_claims": 1
    },
    "questions": {
      "current_date_used": "2025-10-19",
      "queries": [
        {
          "id": "q1",
          "query": "Google official announcement veo3.1 release 2025",
          "claim_id": "claim_1",
          "query_type": "source_verification",
          "priority": "high"
        },
        {
          "id": "q2",
          "query": "Google veo3.1 release date 2025",
          "claim_id": "claim_1",
          "query_type": "direct_fact",
          "priority": "high"
        },
        {
          "id": "q3",
          "query": "veo3.1 software version Google 2025 official",
          "claim_id": "claim_1",
          "query_type": "source_verification",
          "priority": "high"
        },
        {
          "id": "q4",
          "query": "Google veo3.1 release news 2025",
          "claim_id": "claim_1",
          "query_type": "source_verification",
          "priority": "high"
        },
        {
          "id": "q5",
          "query": "veo3.1 Google software version 2025",
          "claim_id": "claim_1",
          "query_type": "direct_fact",
          "priority": "high"
        },
        {
          "id": "q6",
          "query": "Google veo3.1 release statement 2025",
          "claim_id": "claim_1",
          "query_type": "source_verification",
          "priority": "high"
        },
        {
          "id": "q7",
          "query": "veo3.1 Google software version 2025 official data",
          "claim_id": "claim_1",
          "query_type": "statistical",
          "priority": "high"
        },
        {
          "id": "q8",
          "query": "Google veo3.1 release 2025 debunked",
          "claim_id": "claim_1",
          "query_type": "contradiction",
          "priority": "high"
        },
        {
          "id": "q9",
          "query": "veo3.1 Google software version 2025 false",
          "claim_id": "claim_1",
          "query_type": "contradiction",
          "priority": "high"
        },
        {
          "id": "q10",
          "query": "Google veo3.1 release 2025 misleading",
          "claim_id": "claim_1",
          "query_type": "contradiction",
          "priority": "high"
        }
      ],
      "total_queries": 10,
      "strategy_rationale": "The queries are designed to verify the foundational claim 'Google has released a software version named veo3.1.' by using specific entities, dates, and authoritative source keywords. The queries vary in type to cover direct facts, source verification, and contradiction checks, ensuring a comprehensive fact-checking approach."
    },
    "search_results": [
      {
        "query_id": "q2",
        "query": "Google veo3.1 release date 2025",
        "claim_id": "claim_1",
        "query_type": "direct_fact",
        "priority": "high",
        "results": [
          {
            "position": 1,
            "title": "Veo (text-to-video model) - Wikipedia",
            "url": "https://en.wikipedia.org/wiki/Veo_(text-to-video_model)",
            "snippet": "**Veo**, or **Google Veo**, is a text-to-video model developed by Google DeepMind and announced in May 2024. As a generative AI model, it creates videos based on user prompts. Veo 3, released in May 2025, can also generate accompanying audio.\n\n## Development\n\nIn May 2024, a multimodal video generation model called Veo was announced at Google I/O 2024. Google claimed that it could generate 1080p videos over a minute long. In December 2024, Google released Veo 2, available via VideoFX. It supports 4K resolution video generation and has an improved understanding of physics. In April 2025, Google announced that Veo 2 became available for advanced users on the Gemini app.\n\nIn May 2025, Google released Veo 3, which not only generates videos but also creates synchronized audio — including dialogue, sound effects, and ambient noise — to match the visuals. Google also announced **Flow**, a video-creation tool powered by Veo and Imagen. Google DeepMind CEO Demis Hassabis described the release as the moment when AI video generation left the era of the silent film.... ## Capabilities and limitations\n\nGoogle Veo can be bought by several subscription/membership tiers, and/or by using Google \"AI credits\". The software itself can be run by two different consoles called Google Gemini and Google Flow, with Gemini being geared towards shorter, quicker, and faster projects, using the Gemini AI chat model, or through Google Flow, which is essentially a movie editor, as well, allowing users to create longer projects, and continuity using the same characters and actors. Users can create a maximum length of eight seconds per clip.\n\nGoogle Veo, has a relatively simple interface and dashboard, however writing prompts, for those who have little to no experience in transcribing or filmmaking may face issues with the software misunderstanding what the user intended by their prompt (no matter how detailed it was). So although Veo does have a friendly and simple setup, prompts, which are the forefront of the software, need to be not only short and to the point, but they also must be very specific, if the user wants the right vision for their project. Google Veo, when it comes to human models, is able to generate several ethnicity and body types. The software is also capable of generating stand up comedy routines, and Music videos. It can as well generate animals, cartoons, and animation. Prompts must accurately describe places, people, and things in each scene, in addition knowledge of film and camera lingo such as panning, zooming, and terms for camera angles, are also important.... Google Veo however, has strict guidelines and blockades to their software. Before a clip is generated, the algorithm computer software reviews it, and if it is anything deemed inappropriate, too graphically sexual, illegal, showcasing graphic abuse/assault/fighting (unless the prompt specifies that it is a fictitious martial arts scene etc.) gross behaviors, antisemitism, racist, homophobic, anything depicting reigning regimes, rioting, blood, gore, or warfare, (unless in some cases the prompt specifies that it is fictitious period drama, the clip may still be generated), the clip will not be generated. In addition, Google Veo cannot and will not generate character actors that look identical to celebrities or real-life individuals. Users have primarily complained that, regardless of how descriptive and detailed their prompts are, Google Veo often misunderstands the input, resulting in completely different outputs. Common issues include the emulation of incorrect subtitles and captions, the generation of complex scenes that are incomplete due to the maximum eight-second length, the production of garbled and nonsensical speech, and character models that appear deformed in both appearance and movement. Users have also reported that their prompts and generated content are falsely flagged as violating guidelines, along with a variety of other issues and complaints. However, trial and error may have to be used with Veo for optimal results.... ## Reactions\n\nA reporter for *Gizmodo* reacted to the release of Veo 3 by observing that users were directing the model to generate low-quality content, such as man on the street interviews or haul videos of people unboxing products. Another media commentator reported that the tool tended to repeat the same joke in response to different prompts.\n\nCommentators speculated that Google had trained the service on YouTube videos or Reddit posts. Google itself had not stated the source of its training content.\n\nIn July 2025, Media Matters for America reported that racist and antisemitic videos generated using Veo 3 were being uploaded to TikTok. Ryan Whitwam of *Ars Technica* commented, \"In a perfect world, Veo 3 would refuse to create these videos, but vagueness in the prompt and the AI's inability to understand the subtleties of racist tropes (i.e., the use of monkeys instead of humans in some videos) make it easy to skirt the rules.\"\n\n## See also\n- Sora (text-to-video model)\n- VideoPoet – Text-to-video model by Google\n- Dream Machine (text-to-video model)\n\n## References\n\n## External links\n- Official website\n- *Introducing Veo 3.1 and advanced capabilities in Flow*\n\nCategories: - 2024 software\n- Applications of artificial intelligence\n- Film and video technology\n- Google DeepMind\n- Text-to-video generation\n- Video processing\n- Generative artificial intelligence\n- 2024 in artificial intelligence",
            "domain": "en.wikipedia.org"
          },
          {
            "position": 2,
            "title": "Introducing Veo 3.1 and advanced capabilities in Flow - Google Blog",
            "url": "https://blog.google/technology/ai/veo-updates-flow/",
            "snippet": "# Introducing Veo 3.1 and advanced capabilities in Flow\n\nFive months ago, we introduced Flow, our AI filmmaking tool powered by Veo, and have been inspired by the creativity it has sparked with over 275 million videos generated in Flow\n\n1\n\n. We're always listening to your feedback, and we've heard that you want more artistic control within Flow, with increased support for audio across all features.\n\nToday, we’re introducing new and enhanced creative capabilities to edit your clips, giving you more granular control over your final scene. For the first time, we’re also bringing audio to existing capabilities like “Ingredients to Video,” “Frames to Video” and “Extend.”\n\nWe’re also introducing Veo 3.1, which brings richer audio, more narrative control, and enhanced realism that captures true-to-life textures. Veo 3.1 is state-of-the-art and builds on Veo 3, with stronger prompt adherence and improved audiovisual quality when turning images into videos.\n\n## Refine your narrative with audio and more control\n\nWith Veo 3.1, we’re bringing audio to existing capabilities to help you craft the perfect scene. These features are experimental and actively improving, and we’re excited to see what you create as we iterate based on your feedback.\n\nNow, with rich, generated audio, you can:\n\n**Craft the look of your scene.**With \"Ingredients to Video,\" you can use multiple reference images to control the characters, objects and style. Flow uses your ingredients to create a final scene that looks just as you envisioned. **Control the shot from start to finish.**Provide a starting and ending image with “Frames to Video,” and Flow will generate a seamless video that bridges the two, perfect for artful and epic transitions. **Create longer, seamless shots.**With \"Extend,\" you can create longer videos, even lasting for a minute or more, that connect to and continue the action from your original clip. Each video is generated based on the final second of your previous clip, making it most useful for creating a longer establishing shot.... ## Edit your ingredients and videos with more precision\n\nGreat ideas can strike at any point in the creative process. For moments when the first take isn't the final one, we're introducing new editing capabilities directly within Flow to help you reimagine and perfect your scenes.\n\n**Add new elements to any scene.**With “Insert,” introduce anything you can imagine, from realistic details to fantastical creatures. Flow now handles complex details like shadows and scene lighting, making the addition look natural. **Remove unwanted objects or characters seamlessly.**Soon, you’ll be able to take anything out of a scene, and Flow will reconstruct the background and surroundings, making it look as though the object was never there.\n\n## Start creating in Flow today\n\nWith more precise editing capabilities, audio across all existing features and higher-quality outputs powered by Veo 3.1, we're opening up new possibilities for richer, more powerful video storytelling right inside Flow.\n\nThe Veo 3.1 model is also available via the Gemini API for developers, Vertex AI for enterprise customers, and the Gemini app. New capabilities are available in both Gemini API\n\n2\n\nand Vertex AI\n\n3",
            "domain": "blog.google"
          },
          {
            "position": 3,
            "title": "Release notes | Gemini API | Google AI for Developers",
            "url": "https://ai.google.dev/gemini-api/docs/changelog",
            "snippet": "This page documents updates to the Gemini API.\n\n## October 17, 2025\n\n**Grounding with Google Maps**is now generally available. For more information, see Grounding with Google Maps documentation.\n\n## October 15, 2025\n\nReleased Veo 3.1 and 3.1 Fast models in public preview, with new features including:\n\n- Extending Veo-created videos.\n\n- Referencing up to three images to generate a video.\n\n- Providing first and last frame images to generate videos from.\n\nThis launch also added more options for Veo 3 output video durations: 4, 6, and 8 seconds.\n\nDeprecation for\n\n`veo-3.0-generate-preview`and\n\n`veo-3.0-fast-generate-preview`coming October 22, 2025.\n\n## October 7, 2025\n\n- Launched Gemini 2.5 Computer Use Preview\n\n## October 2, 2025\n\n- Launched Gemini 2.5 Flash Image GA: Image Generation with Gemini\n\n## September 29, 2025\n\n- The following Gemini 1.5 models are now deprecated:\n\n`gemini-1.5-pro`\n\n`gemini-1.5-flash-8b`\n\n`gemini-1.5-flash`... ## September 9, 2025\n\n- Launched Veo 3 and Veo 3 Fast GA, with lower pricing and new options for aspect ratios, resolution, and seeding. Read the Veo documentation for more information.\n\n## August 26, 2025\n\n- Launched Gemini 2.5 Image Preview, our latest native image generation model.\n\n## August 18, 2025\n\n- Released URL context tool to general\n\navailability (GA), a tool for providing URLs as additional context to\n\nprompts. Support for using URL context with the\n\n`gemini-2.0-flash`model (available during experimental release) will be discontinued in one week.\n\n## August 14, 2025\n\n- Released Imagen 4 Ultra, Standard and Fast models as generally available (GA). To learn more, see the Imagen page.\n\n## August 7, 2025\n\n`allow_adult`setting in Image to Video generation are now available in restricted regions. See the Veo page for details.\n\n## July 31, 2025\n\n- Launched image-to-video generation for the Veo 3 Preview model.\n\n- Released Veo 3 Fast Preview model.\n\n- To learn more about Veo 3, visit the Veo page.... ## July 22, 2025\n\n- Released\n\n`gemini-2.5-flash-lite`, our fast, low-cost, high-performance Gemini 2.5 model. To learn more, see Gemini 2.5 Flash-Lite.\n\n## July 17, 2025\n\nLaunched\n\n`veo-3.0-generate-preview`, the latest update to Veo introducing video with audio generation. To learn more about Veo 3, visit the Veo page.\n\nIncreased rate limits for Imagen 4 Standard and Ultra. Visit the Rate limits page for more details.\n\n## July 14, 2025\n\n- Released\n\n`gemini-embedding-001`, the stable version of our text embedding model. To learn more, see embeddings. The\n\n`gemini-embedding-exp-03-07`model will be deprecated on August 14, 2025.\n\n## July 7, 2025\n\n- Launched Gemini API Batch Mode. Batch up requests and send them to process asynchronously. To learn more, see Batch Mode.\n\n## June 26, 2025\n\nThe preview models\n\n`gemini-2.5-pro-preview-05-06`and\n\n`gemini-2.5-pro-preview-03-25`are now redirecting to the latest stable version\n\n`gemini-2.5-pro`.\n\n`gemini-2.5-pro-exp-03-25`is deprecated.... ## May 7, 2025\n\n- Released\n\n`gemini-2.0-flash-preview-image-generation`, a preview model for generating and editing images. To learn more, see Image generation and Gemini 2.0 Flash Preview Image Generation.\n\n## May 6, 2025\n\n- Released\n\n`gemini-2.5-pro-preview-05-06`, a new version of our most powerful model, with improvements on code and function calling.\n\n`gemini-2.5-pro-preview-03-25`will automatically point to the new version of the model.\n\n## April 17, 2025\n\n- Released\n\n`gemini-2.5-flash-preview-04-17`, a Gemini preview model optimized for price-performance and adaptive thinking. To learn more, see Gemini 2.5 Flash Preview and Thinking.\n\n## April 16, 2025\n\n- Launched context caching for Gemini 2.0 Flash.... ## April 9, 2025\n\n**Model updates:**\n\n- Released\n\n`veo-2.0-generate-001`, a generally available (GA) text- and image-to-video model, capable of generating detailed and artistically nuanced videos. To learn more, see the Veo docs.\n\nReleased\n\n`gemini-2.0-flash-live-001`, a public preview version of the Live API model with billing enabled.\n\n**Enhanced Session Management and Reliability** **Session Resumption:**Keep sessions alive across temporary network disruptions. The API now supports server-side session state storage (for up to 24 hours) and provides handles (session_resumption) to reconnect and resume where you left off. **Longer Sessions via Context Compression:**Enable extended interactions beyond previous time limits. Configure context window compression with a sliding window mechanism to automatically manage context length, preventing abrupt terminations due to context limits. **Graceful Disconnect Notification:**Receive a\n\n`GoAway`server message indicating when a connection is about to close, allowing for graceful handling before termination.\n\n\n\n**More Control over Interaction Dynamics** **Configurable Voice Activity Detection (VAD):**Choose sensitivity levels or disable automatic VAD entirely and use new client events (\n\n`activityStart`,\n\n`activityEnd`) for manual turn control.\n\n**Configurable Interruption Handling:**Decide whether user input should interrupt the model's response. **Configurable Turn Coverage:**Choose whether the API processes all audio and video input continuously or only captures it when the end-user is detected speaking. **Configurable Media Resolution:**Optimize for quality or token usage by selecting the resolution for input media. **Richer Output and Features** **Expanded Voice & Language Options:**Choose from two new voices and 30 new languages for audio output. The output language is now configurable within\n\n`speechConfig`.\n\n**Text Streaming:**Receive text responses incrementally as they are generated, enabling faster display to the user. **Token Usage Reporting:**Gain insights into usage with detailed token counts provided in the\n\n`usageMetadata`field of server messages, broken down by modality and prompt or response phases.... ## April 4, 2025\n\n- Released\n\n`gemini-2.5-pro-preview-03-25`, a public preview Gemini 2.5 Pro version with billing enabled. You can continue to use\n\n`gemini-2.5-pro-exp-03-25`on the free tier.\n\n## March 25, 2025\n\n- Released\n\n`gemini-2.5-pro-exp-03-25`, a public experimental Gemini model with thinking mode always on by default. To learn more, see Gemini 2.5 Pro Experimental.\n\n## March 12, 2025\n\n**Model updates:**\n\n- Launched an experimental Gemini 2.0 Flash model capable of image generation and editing.\n\n- Released\n\n`gemma-3-27b-it`, available on AI Studio and through the Gemini API, as part of the Gemma 3 launch.\n\n**API updates:**\n\n- Added support for YouTube URLs as a media source.\n\n- Added support for including an inline video of less than 20MB.\n\n## March 11, 2025\n\n**SDK updates:**\n\n- Released the Google Gen AI SDK for TypeScript and JavaScript to public preview.\n\n## March 7, 2025\n\n**Model updates:**\n\n- Released\n\n`gemini-embedding-exp-03-07`, an experimental Gemini-based embeddings model in public preview.... ## February 28, 2025\n\n**API updates:**\n\n- Support for Search as a tool\n\nadded to\n\n`gemini-2.0-pro-exp-02-05`, an experimental model based on Gemini 2.0 Pro.\n\n## February 25, 2025\n\n**Model updates:**\n\n- Released\n\n`gemini-2.0-flash-lite`, a generally available (GA) version of Gemini 2.0 Flash-Lite, which is optimized for speed, scale, and cost efficiency.\n\n## February 19, 2025\n\n**AI Studio updates:**\n\n- Support for additional regions (Kosovo, Greenland and Faroe Islands).\n\n**API updates:**\n\n- Support for additional regions (Kosovo, Greenland and Faroe Islands).\n\n## February 18, 2025\n\n**Model updates:**\n\n- Gemini 1.0 Pro is no longer supported. For the list of supported models, see Gemini models.\n\n## February 11, 2025\n\n**API updates:**\n\n- Updates on the OpenAI libraries compatibility.\n\n## February 6, 2025\n\n**Model updates:**\n\n- Released\n\n`imagen-3.0-generate-002`, a generally available (GA) version of Imagen 3 in the Gemini API.\n\n**SDK updates:**\n\n- Released the Google Gen AI SDK for Java for public preview.",
            "domain": "ai.google.dev"
          },
          {
            "position": 4,
            "title": "Build with Veo 3, now available in the Gemini API",
            "url": "https://developers.googleblog.com/en/veo-3-now-available-gemini-api/",
            "snippet": "First unveiled at Google I/O 2025, people around the world have already generated tens of millions of high-quality videos with Veo 3 (along with some new fun and interesting video trends). It is our first video model to incorporate high-fidelity video outputs and native audio, first with text-to-video and soon with image-to-video.\n\nDevelopers are already experimenting with Veo 3, discovering how the model can help them brainstorm content, rapidly iterate, and be more efficient.\n\nVeo 3 is designed to handle a range of video generation tasks, from cinematic narratives to dynamic character animations. With Veo 3, you can create more immersive experiences by not only generating stunning visuals, but also audio like dialogue and sound effects.\n\nLet’s take a look at some examples.\n\nExplore these examples and more with Veo 3 in Google AI Studio, available as an SDK template and interactive Starter App to remix, copy and extend. The Starter App and its sample code offer a convenient way for Paid Tier users to rapidly prototype with Veo 3 and more on the Gemini API, directly from Google AI Studio.\n\nClick the Key button in the top right of the AI Studio Build interface to select a Google Cloud Project with billing enabled to use the Paid Tier in AI Studio apps. See the FAQs for more.... Veo 3 will be priced at $0.75 per second for video and audio output. Additionally, Veo 3 Fast will be available soon, offering a faster and more cost-effective option for video creation.\n\nHere’s a basic Python example to create a video:\n\n```\n\nimport time\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.0-generate-preview\",\n\nprompt=\"a close-up shot of a golden retriever playing in a field of sunflowers\",\n\nconfig=types.GenerateVideosConfig(\n\nnegative_prompt=\"barking, woofing\",\n\n),\n\n\n\n# Waiting for the video(s) to be generated\n\nwhile not operation.done:\n\ntime.sleep(20)\n\noperation = client.operations.get(operation)\n\ngenerated_video = operation.result.generated_videos[0]\n\nclient.files.download(file=generated_video.video)\n\ngenerated_video.video.save(\"veo3_video.mp4\")\n\n```\n\nAll videos generated by Veo 3 models will continue to include a digital SynthID watermark. To get started, check out the documentation, cookbook, and a Veo 3 starter app in Google AI Studio:\n\nIn addition to being available via the Gemini API in Google AI Studio, Veo 3 is also available to Google AI subscribers in the Gemini app and Flow, and to enterprise customers via Vertex AI.... 1: Fluffy Characters Stop Motion: Inside a brightly colored, cozy kitchen made of felt and yarn. Professor Nibbles, a plump, fluffy hamster with oversized glasses, nervously stirs a bubbling pot on a miniature stove, muttering, \"Just a little more... 'essence of savory,' as the recipe calls for.\" The camera is a mid-shot, capturing his frantic stirring. Suddenly, the pot emits a loud \"POP!\" followed by a comical \"whoosh\" sound, and a geyser of iridescent green slime erupts, covering the entire kitchen. Professor Nibbles shrieks, \"Oh, dear! Not again!\" and scurries away, leaving a trail of tiny, panicked squeaks.\n\n2: The sequence begins with an extreme close-up of a single gear, slowly turning and reflecting harsh sunlight. The camera gradually pulls back in a continuous movement, revealing this is but one component of a colossal, mechanical heart half-buried in a desolate, rust-colored desert. A sweeping aerial shot establishes its enormous scale and isolation in the barren landscape. The camera descends to capture pipes hissing steam and the rhythmic thumping that echoes across the empty plains. A subtle shake effect synchronizes with each massive heartbeat. A lateral tracking shot discovers tiny, robed figures scurrying across the metallic surface. The camera follows one such figure in a detailed tracking shot as they perform meticulous maintenance, polishing brass valves and tightening immense bolts. A complex movement circles the entire structure, capturing different maintenance teams working in precarious positions across its rusted exterior. The final shot begins tight on the meticulous work of one tiny figure before executing a dramatic pull-out that reveals the true scale of the heart and the minuscule size of its caretakers, tending to the vital organ of an unseen, sleeping giant that extends beyond the frame.",
            "domain": "developers.googleblog.com"
          },
          {
            "position": 5,
            "title": "Meet Flow: AI-powered filmmaking with Veo 3",
            "url": "https://blog.google/technology/ai/google-flow-veo-ai-filmmaking-tool/",
            "snippet": "# Meet Flow: AI-powered filmmaking with Veo 3\n\nToday we’re introducing Flow, our new AI filmmaking tool.\n\nIt’s built by and for creatives, and it’s the only AI filmmaking tool custom-designed for Google’s most advanced models — Veo, Imagen and Gemini. Flow can help storytellers explore their ideas without bounds and create cinematic clips and scenes for their stories. It’s early days, and we’re excited to shape the future of Flow with creatives and filmmakers.\n\n### What’s possible with Flow\n\nFlow is inspired by what it feels like when time slows down and creation is effortless, iterative and full of possibility. It’s custom-designed for Veo, Google’s state-of-the-art generative video model, with exceptional prompt adherence and stunning cinematic outputs that excel at physics and realism. Behind the scenes, Gemini models make prompting intuitive, so you can describe your vision in everyday language. You can bring your own assets to create characters, or use Flow to make your own ingredients with Imagen’s text-to-image capabilities.\n\nOnce you’ve created a subject or a scene, you can integrate those same ingredients into different clips and scenes with consistency. Or you can use a scene image to start a new shot.\n\nCreate your ingredients\n\nUse those ingredients to create a clip\n\nReference ingredients in plain language... ### Key features to unlock your storytelling\n\nFlow also comes with a range of features for professionals or those just getting started:\n\n**Camera Controls:**Master your shot with direct control over camera motion, angles and perspectives. **Scenebuilder:**Seamlessly edit and extend your existing shots — revealing more of the action or transitioning to what happens next with continuous motion and consistent characters. **Asset Management:**Easily manage and organize all of your ingredients and prompts. **Flow TV** **:**Spark your creativity with an ever-growing showcase of clips, channels, and content generated with Veo. You can see the exact prompts and techniques used for clips you like, providing a practical way to learn and adapt new styles.\n\nSeamless transitions\n\nCamera controls\n\nCinematic quality\n\n### Get started with Flow\n\nFlow is the evolution of VideoFX, a Google Labs experiment that launched last year. Starting today, Flow is available to subscribers of our Google AI Pro and Google AI Ultra plans in the U.S., with more countries coming soon.\n\nGoogle AI Pro gives you the key Flow features and 100 generations per month, and Google AI Ultra gives you the highest usage limits and early access to Veo 3 with native audio generation, bringing environmental sounds and character dialogue directly into video creation.... ### How we’re collaborating with filmmakers\n\nAs with any groundbreaking technology, we’re still understanding the full potential of AI in filmmaking. We see the emergence of these tools as an enabler, helping a new wave of filmmakers more easily tell their stories. By offering filmmakers early access to Flow, we were able to better understand how our technology could best support and integrate into their creative workflows — and we’ve woven their insights into Flow. Here are some filmmakers we partnered with and the short films they developed using Flow along with other tools and techniques.\n\n**Dave Clark**\n\nDave is an award-winning filmmaker focused on embracing new technology as part of his filmmaking. He used AI to develop two of his most recent short films, “Battalion” and “NinjaPunk.” His newest short film “Freelancers” uses Google’s AI and other tools to tell the story of two estranged adopted brothers on similar quests.\n\n**Henry Daubrez**\n\nHenry has been using tech tools in his art for the last 18 years. Earlier this year he unveiled “Kitsune” using Veo 2 — a moving short film about “love between two souls separated by everything except their shared feelings of loneliness.” Now, Henry is bringing the story of his own creative journey to life in “Electric Pink.”\n\n**Junie Lau**\n\nJunie Lau is a film director and multidisciplinary creative deeply passionate about innovation, viewing AI as a vital collaborator in expanding the boundaries of creative expression. Her work delves into artistic narratives within the hyper-modern era, including themes of virtual identity, digital humanities and digital ontology. Currently, Junie is working on a film titled “Dear Stranger,” which explores the boundless and infinite nature of universal love between a grandmother and grandchild across countless parallel worlds.\n\nAI is ushering in a new chapter of creativity and filmmaking, and while it’s still early, we see so much potential for tools like Flow to unlock new voices and creations.\n\nFor more insights on Flow and how AI helps storytellers take more risks in the creative storytelling process, watch Dave Clark, Junie Lau, and Henry Daubrez in \"Behind the Lens: AI, Creativity, and the Future of Filmmaking Tools.\"",
            "domain": "blog.google"
          },
          {
            "position": 6,
            "title": "Introducing Veo 3.1 and new creative capabilities in the Gemini API",
            "url": "https://developers.googleblog.com/en/introducing-veo-3-1-and-new-creative-capabilities-in-the-gemini-api/",
            "snippet": "**Today, we are releasing Veo 3.1 and Veo 3.1 Fast in paid preview in the** **Gemini API** **.** This updated model offers several improvements, as well as improved outputs when generating video from images. These new models are available via the Gemini API in Google AI Studio and Vertex AI. Veo 3.1 is also available in the Gemini app and Flow.\n\nVeo 3.1 and Veo 3.1 Fast empower developers to create more engaging content through significant upgrades. The models now generate\n\n**richer native audio**, from natural conversations to synchronized sound effects, and offer **greater narrative control** with an improved understanding of cinematic styles. **Enhanced image-to-video** capabilities ensure better prompt adherence while delivering superior audio and visual quality and maintaining character consistency across multiple scenes.\n\nIn addition to the model update, we are introducing new model capabilities, including using reference images to guide video generation, extend existing Veo videos, and generate transitions between a first and last frame.\n\nYou can now guide the generation process by providing up to 3 reference images of a character, object, or scene. This is helpful for maintaining character consistency across multiple shots or applying a specific style to your video.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nconfig=types.GenerateVideosConfig(\n\nreference_images=[reference_image1, reference_image2],\n\n),\n\n\n\n```... Your story is no longer limited by the original generation. With Scene extension, you can create longer videos, even lasting for a minute or more, by generating new clips that connect to your previous video. Each new video is generated based on the final second of the previous clip. This maintains visual continuity, making the feature ideal for extending shots with background audio.\n\n```\n\nfrom google import genai\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nvideo=video_to_extend\n\n\n\n```\n\nCreate smooth, natural scenes that bridge two different images. By providing a starting and an ending image, you can direct Veo 3.1 to generate the transition between them, complete with accompanying audio.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nimage=first_frame,\n\nconfig=types.GenerateVideosConfig(\n\nlast_frame=last_frame,\n\n),\n\n\n\n```\n\n**What people are building with Veo 3.1**\n\nPromise Studios, a GenAI movie studio, uses Veo 3.1 within its MUSE Platform to enhance generative storyboarding and previsualization for director-driven storytelling at production quality.... Latitude is experimenting with Veo 3.1 in its generative narrative engine to instantly bring user-created stories to life.\n\nVeo 3.1 and these new capabilities are now available in paid preview via the Gemini API.\n\nVeo 3.1 is the same price as Veo 3. We are incredibly excited to see what you will build with these new capabilities!",
            "domain": "developers.googleblog.com"
          },
          {
            "position": 7,
            "title": "Veo 3 Fast available for everyone on Vertex AI | Google Cloud Blog",
            "url": "https://cloud.google.com/blog/products/ai-machine-learning/veo-3-fast-available-for-everyone-on-vertex-ai",
            "snippet": "# Veo 3 and Veo 3 Fast are now generally available on Vertex AI\n\n##### Jason Gelman\n\nDirector, Product Management, Vertex AI... ##### Try Gemini 2.5\n\nOur most intelligent model is now available on Vertex AITry now\n\nVeo 3 has seen massive global adoption with over 70 million videos created since May, and we've seen tremendous momentum with our enterprise customers as well. Since its preview launch on Vertex AI in June, enterprise customers have already generated over 6 million videos, showcasing the incredible demand for professional-grade, scalable AI video creation.\n\nToday, we’re building on this momentum with some exciting updates to Veo on Vertex AI.\n\n\n\n**Veo 3,**our most advanced video generation model, is now generally available to everyone on Vertex AI.\n\n\n\n**Veo 3 Fast**, a model designed for speed and rapid iteration, is now generally available for everyone on Vertex AI. It's a faster way to turn text to video, from narrated product demos to short films.\n\n\n\nComing to public preview on Vertex AI in August, Veo 3 and Veo 3 Fast will also offer\n\n**image-to-video capabilities**to make it possible for you to bring static visuals and images to life. All you have to do is provide the source image along with a text prompt that describes what kind of video you want to create.\n\n**How businesses are building with Veo 3 on Vertex AI**... Google Cloud customers around the world are using Veo 3 and Veo 3 Fast on Vertex AI to create professional-quality video content with unparalleled efficiency and creative freedom. Let’s look at some examples.\n\n**Canva**\n\n“Enabling anyone to bring their ideas to life – especially their most creative ones – has been core to Canva's mission ever since we set out to empower the world to design. By democratising access to a powerful technology like Google’s Veo 3 inside Canva AI, your big ideas can now be brought to life in the highest quality video and sound, all from within your existing Canva subscription. In true Canva fashion, we’ve built this with an intuitive interface and simple editing tools in place, all backed by Canva Shield.” –\n\n**Cameron Adams, co-founder and Chief Product Officer, Canva **\n\nBut the momentum extends beyond design. The team at\n\n**BarkleyOKRP**, a leading ad agency, is using Veo 3 to speed up video production timelines.\n\n“The rapid advancements from Veo 2 to Veo 3 within such a short time frame on this project have been nothing short of remarkable. Our team undertook the task of re-creating numerous music videos initially produced with Veo 2 once Veo 3 was released, primarily due to the significantly improved synchronization between voice and mouth movements. The continuous daily progress we are witnessing is truly extraordinary.” –... **Julie Ray Barr, Senior Vice President Client Experience, BarkleyOKRP**\n\nAt global investing platform\n\n**eToro**, the team is making marketing iterations a breeze with Veo 3.\n\n“At eToro, innovation is in our DNA. As a global investing platform serving clients in 75 countries, local storytelling isn’t optional - it’s essential. With Veo 3, we produced 15 fully AI‑generated versions of our ad, each in the native language of its market, all while capturing real emotion at scale. Ironically, AI didn’t reduce humanity - it amplified it. Veo 3 lets us tell more stories, in more tongues, with more impact.” –\n\n**Shay Chikotay, Head of Creative & Content, eToro**\n\n**Razorfish**, an interactive agency and part of the Publicis Groupe, is using Veo to bring creative to life.\n\n\"For The Morelandos, our campaign with Visit Orlando and Google, we used the full Vertex AI stack—Gemini to mine real reviews, Imagen to bring the characters to life, and Veo to give them motion. Veo let us go from story to near-cinematic video in a fraction of the usual time—which meant more room to explore, iterate, and push the idea further.\" –... **Anthony Yell, Chief Creative Officer, Razorfish**\n\n**Synthesia** **, **a leading synthetic media generation company, is using Veo to contextually adapt visuals to its hyper-realistic AI avatars and voices.\n\n“Veo 3 represents a leap forward in generative AI, and its integration into Synthesia’s platform will redefine how businesses create video content. By combining our hyper-realistic AI avatars and voices with Veo-powered fully contextual visuals that adapt to each unique story, we’re giving enterprise teams the creative power to communicate with unrivalled clarity and impact.” –\n\n**Bill Leaver, Product Manager, Synthesia **\n\n**How enterprises can use Veo 3 Fast for speed and creativity **\n\n**Veo 3 Fast** is a great fit for work that requires rapid iteration and speed. It has an ideal balance between processing time and high-quality visual output, making it especially helpful for:\n\n\n\nQuickly generating and testing variations of ad concepts to respond to market trends.\n\n\n\nEfficiently creating video demonstrations for entire product catalogs from still images.\n\n\n\nDeveloping engaging animated explainers and training modules in less time.\n\n**Veo 3 and Veo 3 Fast on Vertex AI mean even more capabilities for enterprise storytelling**\n\nVeo 3 and Veo 3 Fast are designed to give creators the control and quality needed to move beyond short clips and produce complete, compelling narratives. Here are some of the core features now generally available on Vertex AI.... **Create scenes with native audio:**Veo 3 generates video and audio in a single step. This means you can create scenes with characters that speak with accurate lip-syncing, and sound effects that fit the mood. **Deliver professional quality at enterprise scale:**Veo 3 produces high-definition (1080p) video, suitable for professional marketing campaigns, product demonstrations, and internal communications. You can create content that meets brand standards, saving time and money. **Simplify content localization for global audiences:**Veo 3’s native dialogue generation helps businesses connect with an international audience by producing a video once and localizing the dialogue for dozens of languages. **Image-to-video (coming to public preview on Vertex AI in August):**Veo 3 and Veo 3 Fast can also take a single image, which can be a photo you uploaded or an AI-generated image, and animate it, creating an 8-second video clip. This feature is particularly powerful for content creators, marketers, and businesses looking to animate existing visual assets, create engaging social media content, or generate compelling product demonstrations from high-quality images.\n\n### Enterprise-grade safety and security\n\nVeo 3 and Veo 3 Fast on Vertex AI are built for scalable and responsible enterprise use. We embed digital watermarks into every frame with SynthID, helping combat misinformation and misattribution. Veo 3 and Veo 3 Fast are also covered by our indemnity for generative AI services.... ### Get started with Veo 3 and Veo 3 Fast today\n\nTo get started, go here to learn more about Veo 3 and Veo 3 Fast on Vertex AI, and try it on Vertex AI Media Studio.",
            "domain": "cloud.google.com"
          },
          {
            "position": 8,
            "title": "First look: Google Veo 3 (May/2025)",
            "url": "https://www.youtube.com/watch?v=OrVUHMK58GE",
            "snippet": "## Dr Alan D. Thompson\n##### May 25, 2025 (0:05:24)\nSource: \nhttps://www.reddit.com/r/PowerfulJRE/comments/1kt942i/these_are_all_ai_videos_generated_with_google_veo/\n\nThe Memo: https://lifearchitect.ai/memo/... {ts:0} That's one move with AI that makes haters go crazy every time. Oh, y'all\n{ts:3} gotta give them that. This is wild. It's over. We are cooked on that thread. You get me?\n{ts:9} Oh my god. Yes. Victory royale with a pickaxe. So, this is an AI video about nothing. It's about\n{ts:19} nothing. Who would watch a video about nothing? So, I went to the zoo the other day and\n{ts:27} all they had was one dog. It was a [Music] shiu. We're going to light up the\n{ts:43} [Music] [Applause] [Music]\n{ts:51} sky. I'm not sure I can go on. [Music] The sum of the squares of the two\n{ts:61} shorter sides is equal to the square of the longest side. We can talk. No more silence. Yes, we can talk. We can talk.\n{ts:68} We can talk. We can talk with accents. Oh, I think that would be marvelous. Yes, it is very fun. But yes, it is very\n{ts:75} good. Very fun. I can talk. Yes, we can talk. Yes, we can talk. We can talk. We can talk. Yes, we... {ts:84} can talk. No. Yes, we can talk as cartoons. This is amazing. Imagine all the narrative\n{ts:92} possibilities. We can sing talk. Let's talk. So, what are we going to talk about now? What are we going to\n{ts:104} talk about now that we can talk? I have no idea. What do you want to talk about now that I can talk?\n{ts:112} Yeah. I I don't know if I have something to say.\n{ts:118} We can talk about how magical this is. Is [Music]\n{ts:124} I want to say something important, something deep. The future is still in our hands.\n{ts:132} That's cliche dialogue. Let's not talk. How much wood would a woodchuck chuck if\n{ts:146} a woodchuck could chuck wood? How much wood would a woodchuck chuck if a woodchuck could chuck wood? How much\n{ts:155} wood would a woodchuck chuck if a woodchuck could chuck wood? How much wood would a woodchuck chuck if a\n{ts:165} woodchuck could chuck wood? How much wood would a woodchuck chuck if a woodchuck could chuck wood?... {ts:175} How much wood would a woodchuck chuck if a woodchuck could chuck wood? How much wood would a woodchuck\n{ts:187} chuck? [Applause] Welcome to a non-existent car show.\n{ts:193} Let's see some opinions. I mean, man, the acceleration is crazy. You look far, step on the pedal, and you\n{ts:202} are there. I feel safe with him in an SUV, and it seems to be like the right type of car for him. I think the range\n{ts:210} is only um only going to get better. Sorry. We don't want to drive gas cars anymore.\n{ts:217} Yeah. No more gas cars. You can see uh I'm kind of a kind of a misfit here, but uh don't tell anyone\n{ts:226} I've just bought an electric car. I think it's really great for families and for little babies with all the safety\n{ts:231} features that these SUVs have. But what you're really seeing is that technology is going to be very, very important in\n{ts:239} terms of how we go forward. It was um great to come to the conference because my husband loves cars. I think I... {ts:249} have to buy an EV now. I love my muscle cars, but I try to stay as healthy as I can so\n{ts:257} I can make it to the next car show. When we get in there, I want no [ __ ] You stay on my six at all\n{ts:269} times. [Music] Stay sharp. These [ __ ] are nasty and\n{ts:282} dangerous. Stay alert. [ __ ] What the hell happened here? Where are the\n{ts:291} [Applause] bodies? Heat. Heat. Fire heat.",
            "domain": "www.youtube.com"
          },
          {
            "position": 9,
            "title": "Google releases Veo 3.1, adds it to Flow video editor",
            "url": "https://techcrunch.com/2025/10/15/google-releases-veo-3-1-adds-it-to-flow-video-editor/",
            "snippet": "In Brief\nGoogle launched its new video model Veo 3.1 with improved audio output, granular editing controls, and better output for image to video. It said that Veo 3.1 builds on May's Veo 3 release and generates more realistic clips and adheres to prompts better.\nThe model allows users to add an object to the video and have it blend into the clip's style, Google said. Soon, users will be able to remove an existing object from the video in Flow, too.\nVeo 3 already has edit features such as adding reference images to drive a character, providing the first and last frame to generate a clip using AI, and the ability to extend an existing video based on the last few frames. With Veo 3.1, Google is adding audio to all these features to make the clips more lively.\nThe company is rolling out the model to its video editor Flow, the Gemini App, along with Vertex and Gemini APIs. It said that since Flow's launch in May, users have created more than 275 million videos on the app.",
            "domain": "techcrunch.com"
          },
          {
            "position": 10,
            "title": "Gemini AI video generator powered by Veo 3.1",
            "url": "https://gemini.google/overview/video-generation/",
            "snippet": "# Break the\n\n**silence** with Veo 3.1\n\nCreate high-quality, 8-second videos with Veo 3.1, our latest AI video generator. Simply describe what you have in mind or upload a photo and watch your ideas come to life with native audio generation. Try it with a Google AI Pro plan or get the highest access with the Ultra plan.\n\n**Veo 3.1** speaks for itself\n\n## Dream it. Describe it.\n\n**Done.**\n\n## For Exploring\n\nPlay with diverse styles, bring animated characters to life, and combine objects in ways you never thought possible. See what you can create using text to video with AI.\n\n## For Sharing\n\nCreate funny memes, turn inside jokes into videos, re-imagine special moments, and add a personal touch to make someone smile.\n\n## For Brainstorming\n\nBreak through creative blocks and visualize your ideas in a flash. From product concepts and designs to rapid prototyping and storytelling, Gemini can help.\n\n## Learn more about our\n\n**Veo Models**\n\nCreate videos with sound using our video generation model that maintains high-quality while optimizing for speed.\n\nCreate high-quality, 8-second videos with sound using our state-of-the-art video generation model.... ## Frequently asked questions\n\nYes, you can create and share videos in your mobile Gemini app. To create videos, tap the \"video\" button in your prompt bar. If you don't see it, tap the button with three dots to view more options.\n\nTry Veo 3.1 Fast with a Google AI Pro plan or get the highest access to Veo 3.1 in Google AI Ultra. Country availability here.\n\nFor now, the ability to generate a video from a photo is not available in the European Economic Area, Switzerland, or the United Kingdom.\n\nWe’ve taken several important safety steps to make AI video generation a safe experience. This includes extensive red teaming and evaluation aimed at preventing the generation of content that violates our policies. Additionally, all videos generated with Veo in the Gemini app are marked with a visible watermark and SynthID, a digital watermark embedded in each frame, which indicates the videos are AI-generated.\n\nGemini's outputs are primarily determined by user prompts and like any generative AI tool, there may be instances where it generates content that some individuals find objectionable. We’ll continue to listen to your feedback through the thumbs up/down buttons and make ongoing improvements. For more details, you can read about our approach on our website.\n\nResults for illustrative purposes and may vary. Internet and subscription for certain features required. Available to users 18+. Create responsibly.",
            "domain": "gemini.google"
          }
        ],
        "success": true,
        "error": null
      },
      {
        "query_id": "q1",
        "query": "Google official announcement veo3.1 release 2025",
        "claim_id": "claim_1",
        "query_type": "source_verification",
        "priority": "high",
        "results": [
          {
            "position": 1,
            "title": "Introducing Veo 3.1 and advanced capabilities in Flow - Google Blog",
            "url": "https://blog.google/technology/ai/veo-updates-flow/",
            "snippet": "# Introducing Veo 3.1 and advanced capabilities in Flow\n\nFive months ago, we introduced Flow, our AI filmmaking tool powered by Veo, and have been inspired by the creativity it has sparked with over 275 million videos generated in Flow\n\n1\n\n. We're always listening to your feedback, and we've heard that you want more artistic control within Flow, with increased support for audio across all features.\n\nToday, we’re introducing new and enhanced creative capabilities to edit your clips, giving you more granular control over your final scene. For the first time, we’re also bringing audio to existing capabilities like “Ingredients to Video,” “Frames to Video” and “Extend.”\n\nWe’re also introducing Veo 3.1, which brings richer audio, more narrative control, and enhanced realism that captures true-to-life textures. Veo 3.1 is state-of-the-art and builds on Veo 3, with stronger prompt adherence and improved audiovisual quality when turning images into videos.\n\n## Refine your narrative with audio and more control\n\nWith Veo 3.1, we’re bringing audio to existing capabilities to help you craft the perfect scene. These features are experimental and actively improving, and we’re excited to see what you create as we iterate based on your feedback.\n\nNow, with rich, generated audio, you can:\n\n**Craft the look of your scene.**With \"Ingredients to Video,\" you can use multiple reference images to control the characters, objects and style. Flow uses your ingredients to create a final scene that looks just as you envisioned. **Control the shot from start to finish.**Provide a starting and ending image with “Frames to Video,” and Flow will generate a seamless video that bridges the two, perfect for artful and epic transitions. **Create longer, seamless shots.**With \"Extend,\" you can create longer videos, even lasting for a minute or more, that connect to and continue the action from your original clip. Each video is generated based on the final second of your previous clip, making it most useful for creating a longer establishing shot.... ## Edit your ingredients and videos with more precision\n\nGreat ideas can strike at any point in the creative process. For moments when the first take isn't the final one, we're introducing new editing capabilities directly within Flow to help you reimagine and perfect your scenes.\n\n**Add new elements to any scene.**With “Insert,” introduce anything you can imagine, from realistic details to fantastical creatures. Flow now handles complex details like shadows and scene lighting, making the addition look natural. **Remove unwanted objects or characters seamlessly.**Soon, you’ll be able to take anything out of a scene, and Flow will reconstruct the background and surroundings, making it look as though the object was never there.\n\n## Start creating in Flow today\n\nWith more precise editing capabilities, audio across all existing features and higher-quality outputs powered by Veo 3.1, we're opening up new possibilities for richer, more powerful video storytelling right inside Flow.\n\nThe Veo 3.1 model is also available via the Gemini API for developers, Vertex AI for enterprise customers, and the Gemini app. New capabilities are available in both Gemini API\n\n2\n\nand Vertex AI\n\n3",
            "domain": "blog.google"
          },
          {
            "position": 2,
            "title": "Google announces Veo 3.1 and updates Flow with more controls, tools",
            "url": "https://9to5google.com/2025/10/15/veo-3-1/",
            "snippet": "Google today announced Veo 3.1 as its latest video generation model, with Flow getting a number of updates to take advantage of the latest capabilities.\n\nCompared to Veo 3, which was announced at I/O 2025 in May, this new version offers richer audio and “enhanced realism that captures true-to-life textures.” Veo 3.1 has a deeper understanding of storytelling, cinematic styles, and character interactions to give you more narrative control. The image-to-video capability benefits from improved audio-visual quality and better follows your prompt.\n\nVeo 3.1 and Veo 3.1 Fast are available in the Gemini app, as well as the Gemini API and Vertex AI, to power text-to-video and image-to-video for horizontal (16×9) and vertical (9×16) outputs.\n\nMeanwhile, Google is updating the Flow filmmaking tool to take advantage of Veo 3.1. Audio generation is coming to:\n\n**Ingredients to Video**: This can include images of multiple characters, objects, and styles **Scene extension**: You can Extend an existing clip, with Google basing it on the last second **Frames to Video**: This lets you specify the starting and ending images, with Flow bridging the two\n\nFlow is getting new editing capabilities. You can insert elements like objects, characters, and details, with Google handling shadows, scene lighting, and other complex details to make everything look natural.\n\nComing soon is the ability to remove objects and characters from a scene, with Flow working to reconstruct the background and surroundings to make the edit seamless.\n\n\n\n## Comments",
            "domain": "9to5google.com"
          },
          {
            "position": 3,
            "title": "Google rolls out its new Veo 3 video-generation model ...",
            "url": "https://techcrunch.com/2025/07/03/google-rolls-out-its-new-veo-3-video-generation-model-globally/",
            "snippet": "Google on Thursday said it has begun rolling out its Veo 3 video generation model to Gemini users in more than 159 countries.\n\nVideo generation via the new model is available only to paying subscribers of Google’s AI Pro plan and is capped at three videos per day.\n\nVeo 3, which Google showed off in May, lets users generate videos up to eight seconds long using text prompts.\n\nGoogle’s Josh Woodward has said that the company is working on adding image-to-video generation capabilities to Gemini.",
            "domain": "techcrunch.com"
          },
          {
            "position": 4,
            "title": "Google releases Veo 3.1, adds it to Flow video editor",
            "url": "https://techcrunch.com/2025/10/15/google-releases-veo-3-1-adds-it-to-flow-video-editor/",
            "snippet": "In Brief\nGoogle launched its new video model Veo 3.1 with improved audio output, granular editing controls, and better output for image to video. It said that Veo 3.1 builds on May's Veo 3 release and generates more realistic clips and adheres to prompts better.\nThe model allows users to add an object to the video and have it blend into the clip's style, Google said. Soon, users will be able to remove an existing object from the video in Flow, too.\nVeo 3 already has edit features such as adding reference images to drive a character, providing the first and last frame to generate a clip using AI, and the ability to extend an existing video based on the last few frames. With Veo 3.1, Google is adding audio to all these features to make the clips more lively.\nThe company is rolling out the model to its video editor Flow, the Gemini App, along with Vertex and Gemini APIs. It said that since Flow's launch in May, users have created more than 275 million videos on the app.",
            "domain": "techcrunch.com"
          },
          {
            "position": 5,
            "title": "Build with Veo 3, now available in the Gemini API",
            "url": "https://developers.googleblog.com/en/veo-3-now-available-gemini-api/",
            "snippet": "First unveiled at Google I/O 2025, people around the world have already generated tens of millions of high-quality videos with Veo 3 (along with some new fun and interesting video trends). It is our first video model to incorporate high-fidelity video outputs and native audio, first with text-to-video and soon with image-to-video.\n\nDevelopers are already experimenting with Veo 3, discovering how the model can help them brainstorm content, rapidly iterate, and be more efficient.\n\nVeo 3 is designed to handle a range of video generation tasks, from cinematic narratives to dynamic character animations. With Veo 3, you can create more immersive experiences by not only generating stunning visuals, but also audio like dialogue and sound effects.\n\nLet’s take a look at some examples.\n\nExplore these examples and more with Veo 3 in Google AI Studio, available as an SDK template and interactive Starter App to remix, copy and extend. The Starter App and its sample code offer a convenient way for Paid Tier users to rapidly prototype with Veo 3 and more on the Gemini API, directly from Google AI Studio.\n\nClick the Key button in the top right of the AI Studio Build interface to select a Google Cloud Project with billing enabled to use the Paid Tier in AI Studio apps. See the FAQs for more.... Veo 3 will be priced at $0.75 per second for video and audio output. Additionally, Veo 3 Fast will be available soon, offering a faster and more cost-effective option for video creation.\n\nHere’s a basic Python example to create a video:\n\n```\n\nimport time\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.0-generate-preview\",\n\nprompt=\"a close-up shot of a golden retriever playing in a field of sunflowers\",\n\nconfig=types.GenerateVideosConfig(\n\nnegative_prompt=\"barking, woofing\",\n\n),\n\n\n\n# Waiting for the video(s) to be generated\n\nwhile not operation.done:\n\ntime.sleep(20)\n\noperation = client.operations.get(operation)\n\ngenerated_video = operation.result.generated_videos[0]\n\nclient.files.download(file=generated_video.video)\n\ngenerated_video.video.save(\"veo3_video.mp4\")\n\n```\n\nAll videos generated by Veo 3 models will continue to include a digital SynthID watermark. To get started, check out the documentation, cookbook, and a Veo 3 starter app in Google AI Studio:\n\nIn addition to being available via the Gemini API in Google AI Studio, Veo 3 is also available to Google AI subscribers in the Gemini app and Flow, and to enterprise customers via Vertex AI.... 1: Fluffy Characters Stop Motion: Inside a brightly colored, cozy kitchen made of felt and yarn. Professor Nibbles, a plump, fluffy hamster with oversized glasses, nervously stirs a bubbling pot on a miniature stove, muttering, \"Just a little more... 'essence of savory,' as the recipe calls for.\" The camera is a mid-shot, capturing his frantic stirring. Suddenly, the pot emits a loud \"POP!\" followed by a comical \"whoosh\" sound, and a geyser of iridescent green slime erupts, covering the entire kitchen. Professor Nibbles shrieks, \"Oh, dear! Not again!\" and scurries away, leaving a trail of tiny, panicked squeaks.\n\n2: The sequence begins with an extreme close-up of a single gear, slowly turning and reflecting harsh sunlight. The camera gradually pulls back in a continuous movement, revealing this is but one component of a colossal, mechanical heart half-buried in a desolate, rust-colored desert. A sweeping aerial shot establishes its enormous scale and isolation in the barren landscape. The camera descends to capture pipes hissing steam and the rhythmic thumping that echoes across the empty plains. A subtle shake effect synchronizes with each massive heartbeat. A lateral tracking shot discovers tiny, robed figures scurrying across the metallic surface. The camera follows one such figure in a detailed tracking shot as they perform meticulous maintenance, polishing brass valves and tightening immense bolts. A complex movement circles the entire structure, capturing different maintenance teams working in precarious positions across its rusted exterior. The final shot begins tight on the meticulous work of one tiny figure before executing a dramatic pull-out that reveals the true scale of the heart and the minuscule size of its caretakers, tending to the vital organ of an unseen, sleeping giant that extends beyond the frame.",
            "domain": "developers.googleblog.com"
          },
          {
            "position": 6,
            "title": "Introducing Veo 3.1 and new creative capabilities in the Gemini API",
            "url": "https://developers.googleblog.com/en/introducing-veo-3-1-and-new-creative-capabilities-in-the-gemini-api/",
            "snippet": "**Today, we are releasing Veo 3.1 and Veo 3.1 Fast in paid preview in the** **Gemini API** **.** This updated model offers several improvements, as well as improved outputs when generating video from images. These new models are available via the Gemini API in Google AI Studio and Vertex AI. Veo 3.1 is also available in the Gemini app and Flow.\n\nVeo 3.1 and Veo 3.1 Fast empower developers to create more engaging content through significant upgrades. The models now generate\n\n**richer native audio**, from natural conversations to synchronized sound effects, and offer **greater narrative control** with an improved understanding of cinematic styles. **Enhanced image-to-video** capabilities ensure better prompt adherence while delivering superior audio and visual quality and maintaining character consistency across multiple scenes.\n\nIn addition to the model update, we are introducing new model capabilities, including using reference images to guide video generation, extend existing Veo videos, and generate transitions between a first and last frame.\n\nYou can now guide the generation process by providing up to 3 reference images of a character, object, or scene. This is helpful for maintaining character consistency across multiple shots or applying a specific style to your video.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nconfig=types.GenerateVideosConfig(\n\nreference_images=[reference_image1, reference_image2],\n\n),\n\n\n\n```... Your story is no longer limited by the original generation. With Scene extension, you can create longer videos, even lasting for a minute or more, by generating new clips that connect to your previous video. Each new video is generated based on the final second of the previous clip. This maintains visual continuity, making the feature ideal for extending shots with background audio.\n\n```\n\nfrom google import genai\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nvideo=video_to_extend\n\n\n\n```\n\nCreate smooth, natural scenes that bridge two different images. By providing a starting and an ending image, you can direct Veo 3.1 to generate the transition between them, complete with accompanying audio.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nimage=first_frame,\n\nconfig=types.GenerateVideosConfig(\n\nlast_frame=last_frame,\n\n),\n\n\n\n```\n\n**What people are building with Veo 3.1**\n\nPromise Studios, a GenAI movie studio, uses Veo 3.1 within its MUSE Platform to enhance generative storyboarding and previsualization for director-driven storytelling at production quality.... Latitude is experimenting with Veo 3.1 in its generative narrative engine to instantly bring user-created stories to life.\n\nVeo 3.1 and these new capabilities are now available in paid preview via the Gemini API.\n\nVeo 3.1 is the same price as Veo 3. We are incredibly excited to see what you will build with these new capabilities!",
            "domain": "developers.googleblog.com"
          },
          {
            "position": 7,
            "title": "Google expands Veo 3 to Gemini in more countries and on ...",
            "url": "https://blog.google/products/gemini/veo-3-expansion-mobile/",
            "snippet": "We’re thrilled by the response to Veo 3. The Google AI Ultra plan grants the highest access to Veo 3 and later today we’re launching it in the UK. The Ultra plan is now available in 73 countries, and we’re working hard to bring it to even more.\n\nGoogle AI Pro subscribers in those countries get limited access to Veo 3 in Flow, and a 10-pack of trial video generations to test it out in the Gemini app. Starting today, Pro subscribers will also have access to Veo 3 in the Gemini mobile app.\n\nIt's important that people can access provenance tools for content online. The SynthID watermark is embedded in all content generated by Google's generative AI models. Our SynthID Detector rolled out to early testers last week, and we plan to expand access soon. As an additional step to help people identify AI-generated content, today we're adding a visible watermark to all videos, except for videos generated by Ultra members in Flow, our tool for AI filmmakers.",
            "domain": "blog.google"
          },
          {
            "position": 8,
            "title": "What Is Google Veo 3.1? A Beginner's Guide to AI Video ...",
            "url": "https://skywork.ai/blog/google-veo-3-1-beginners-guide-ai-video-model/",
            "snippet": "If you’ve seen short, cinematic clips made “from a prompt” and wondered how it works, you’re in the right place. This guide will walk you through Google’s latest video generation model, Veo 3.1, in plain English—what it is, why it matters, where you can try it, and a simple first project you can finish in about 10 minutes. Don’t worry if you’ve never touched AI video before. We’ll move step by step and flag common gotchas so you can avoid them.\n\n## What is Google Veo 3.1?\n\nGoogle Veo 3.1 is a text-to-video AI model that turns short written prompts and references into short video clips, with native audio support and more control over storytelling and style. In October 2025, Google announced Veo 3.1 (and a faster variant) in paid preview for developers and creators. According to the Google Developers Blog announcement (Oct 15, 2025), Veo 3.1 is available via the Gemini API in Google AI Studio and Vertex AI, and it adds richer native audio, improved adherence to cinematic styles, and new creative controls.\n\nWhy it matters for beginners: Veo 3.1’s improvements make it easier to describe the mood and motion you want—like “a slow dolly-in on a vintage typewriter with soft rain ambience”—and actually get something close, without wrestling with advanced settings.... ## What’s new compared to earlier Veo versions?\n\nGoogle calls out a few key upgrades in Veo 3.1:\n\n- Richer native audio generation and better narrative control, with improved understanding of cinematic styles (publisher: Google; see the Developers Blog announcement (2025)).\n\n- New creative tools in supporting apps: you can guide generation with multiple reference images, extend existing Veo clips, or bridge between a first and last frame to create transitions. These capabilities are highlighted in the Google Blog on Veo 3.1 and Flow (Oct 2025).\n\n- Native audio emphasis and model positioning are also described on the DeepMind Veo model page, which introduces Veo’s “video, meet audio” concept.\n\nIndependent coverage, like TechCrunch’s report on the Veo 3.1 release (Oct 2025), provides context on rollout and app integration. For hard limits (length, resolution), always defer to Google’s official docs, as those details can change.\n\n## Where can you use Veo 3.1 today?\n\nAs of October 2025:\n\n- Flow (Google’s AI video editor): Google’s announcement notes Veo 3.1 and advanced creative controls in Flow, including “Ingredients to Video” (use multiple images as style/character references), “Frames to Video” (bridge between start/end frames), and “Extend” (lengthen a clip). Details are outlined in the Google Blog on Veo 3.1 and Flow (2025).\n\n- Gemini API (Google AI Studio and Vertex AI): Veo 3.1 and Veo 3.1 Fast are in paid preview via the Gemini API, per the Google Developers Blog announcement (2025).\n\nImportant: Access tiers, regions, and pricing can vary and may change. If you’re not seeing options in your account, check Google’s documentation or support for your account type and region.... ## What can Veo 3.1 generate right now? (Practical limits to know)\n\n- In Vertex AI’s preview for Veo 3.1 base generations, the documented lengths are currently short (choose 4, 6, or 8 seconds). This is specified on the Vertex AI Veo 3.1 preview page.\n\n- In Gemini API extension workflows, you can extend Veo-generated clips. The API docs describe a maximum of up to 141 seconds for input Veo videos in those extension scenarios, with 720p listed in that context. See the Gemini API video documentation (Google).\n\nThese numbers help set expectations: start with short shots, then extend or chain shots together. Avoid assuming 1080p or minute-long base generations unless Google’s official docs explicitly state it for your environment.... ## A 10-minute quickstart: your first Veo 3.1 clip\n\nWe’ll make a simple “coffee shop mood” shot you can adapt for social posts.\n\n- Choose your aspect ratio\n\n- 16:9 (landscape) for YouTube and desktop-first screens\n\n- 9:16 (vertical) for TikTok, Reels, and Shorts Pick one at the start to avoid accidental cropping later.\n\n- Write a clear, concrete prompt Use this structure to get reliable results: subject + action + setting + style + camera + audio.\n\n- Example prompt: “A barista gently places a ceramic cup on a wooden counter; shallow depth of field; warm morning light streaming through windows; slow dolly-in; subtle steam rising; soft cafe ambience, no dialogue, light jazz in the background.”\n\nIf you want a deeper primer on prompt structure and clarity, see these prompt engineering best practices for beginners.\n\n- Generate in your chosen interface\n\n- In Flow: Start a new project, choose Veo 3.1, paste your prompt, set aspect ratio, and generate.\n\n- In Gemini API/Vertex AI: Use the model/version and parameters documented for Veo 3.1 in your environment. Begin with short durations and default settings.\n\n- Review the output like a director\n\n- Does the subject and action match? Is the lighting/mood close?\n\n- Is the camera movement smooth? Is audio what you expected?\n\n- Make one or two changes per iteration—small edits beat big rewrites.\n\n- Refine with controls\n\n- Reference images: If you want consistency (e.g., the same mug or barista style), use up to three reference images to guide the look. This capability is described in Google’s announcements (2025).\n\n- Frames to Video: Provide a starting and ending frame to shape the motion between them.\n\n- Extend: Lengthen your favorite moment to create a longer beat.\n\n- Export Choose the format/aspect ratio you started with. If you need both vertical and horizontal versions, plan to reframe or regenerate with the other aspect ratio rather than cropping aggressively.... ## A practical planning example using Skywork AI (optional, 5 minutes)\n\nSkywork AI can help you prepare the words before you ever hit “Generate.” Disclosure: Skywork AI is our product.\n\nHere’s a neutral, step-by-step way to use it purely for planning:\n\n- Open Skywork and create a new document. Ask for a short video outline: “30-second coffee shop mood piece: 3 shots, warm tone, slow camera.”\n\n- Have it draft a compact shot list with camera moves and audio notes, like:\n\n- Close-up of cup; slow dolly-in; soft steam; light jazz; no dialogue\n\n- Medium barista hands; gentle rack focus; cafe ambience; espresso hiss\n\n- Wide room tone; sunbeams; slow tilt up; footsteps and cups\n\n- Ask for a final prompt assembled from the shot you want to generate first. Copy that prompt into Veo 3.1.\n\nIf you prefer to outline prompts and story beats yourself, this short guide to Skywork’s General Mode for planning and outlining walks through a simple, distraction-free workflow.... ## FAQs for first-time users\n\n\n\nIs Veo 3.1 free? No. Google describes Veo 3.1 and Veo 3.1 Fast as being in paid preview via the Gemini API as of October 2025. See the Google Developers Blog announcement (2025). Pricing varies by product and usage.\n\n\n\nWhere can I try Veo 3.1? Flow (for a visual editor), and the Gemini API via Google AI Studio and Vertex AI (for developers). Google outlines these options in the Veo 3.1 and Flow post (2025) and the Developers Blog announcement (2025).\n\n\n\nHow long can my video be? It depends on the environment and workflow. In Vertex AI’s Veo 3.1 preview, base generations are short (4, 6, or 8 seconds). In Gemini API extension workflows, input Veo videos can be extended up to 141 seconds. See the Vertex AI preview page and the Gemini API video docs.\n\n\n\nWhat resolutions are supported? The Gemini API documentation for extension workflows references 720p in that context. Official materials do not universally guarantee 1080p for base Veo 3.1 generations at this time.\n\n\n\nCan I keep a character or object consistent across shots? Yes. Google notes you can guide Veo 3.1 with up to three reference images for a character, object, or scene. This is described in the Developers Blog announcement (2025).\n\n\n\nCan I add my own audio and sync it? Veo 3.1 focuses on native audio generation controlled via your prompt. The public pages cited here do not detail a full “upload and auto-sync” workflow, so avoid relying on that unless you see it documented for your account.... ## Next steps\n\n- Try a tiny project: one 6–8 second shot with a clear prompt. If you like the result, use “Extend” or stitch a second shot with “Frames to Video.”\n\n- Build a simple storyboard first. If you want help outlining ideas, you can use Skywork AI to draft prompts and shot lists before you generate. Keep it simple: one scene, one action, one camera move.\n\n- When you’re ready to go deeper, explore Google’s official resources: the Veo 3.1 and Flow post (2025), the Developers Blog announcement (2025), the DeepMind Veo overview, the Gemini API video docs, and the Vertex AI Veo 3.1 preview.\n\nYou don’t need special “artistic talent” to start—just one clear sentence and a little patience. Generate, review, tweak, repeat. That’s how everyone learns, and you’ll be surprised how quickly it clicks.",
            "domain": "skywork.ai"
          },
          {
            "position": 9,
            "title": "Veo 3 available for everyone in public preview on Vertex AI - Google Cloud",
            "url": "https://cloud.google.com/blog/products/ai-machine-learning/veo-3-available-for-everyone-in-public-preview-on-vertex-ai",
            "snippet": "# You dream it, Veo creates it: Veo 3 is now available for everyone in public preview on Vertex AI\n\n##### Jason Gelman\n\nDirector, Product Management, Vertex AI\n\n##### Try Gemini 2.5\n\nOur most intelligent model is now available on Vertex AITry now\n\nA great story doesn't just tell you, it shows you. With Veo 3, we’ve leapt forward in combining video and audio generation to take storytelling to the next level.\n\nToday, we’re excited to share that Veo 3 is now available for all Google Cloud customers and partners in public preview on Vertex AI.\n\n**Why this matters: **Veo 3 is your partner for creating near-cinematic quality generative video, moving beyond novelty to narrative-driven creation. It not only brings stunning visual quality, but now adds sound from background sounds to dialogue. With Veo 3 on Vertex AI, you can take advantage of three powerful new capabilities:\n\n\n\n**Fluid, natural videos that synchronize video with audio and dialogue.**Veo 3 can synchronize your audio and visuals in a single pass. The model produces rich soundscapes containing everything from dialogue and ambient noise, to sound effects and background music.\n\n\n\n**Cinematic video that captures creative nuances.**Veo 3 makes it easy to capture creative nuances and detailed scene interactions in your prompt, from the shade of the sky to the precise way the sun hits water in the afternoon light, and produces high-definition video.\n\n\n\n**Realistic movement that simulates real-world physics.**To create believable scenes, Veo 3 simulates real-world physics. This results in realistic water movement, accurate shadows connected with objects and characters, and natural human motion.... ### Businesses are already using Veo to make creating easier\n\nVeo 3 is helping Google Cloud customers create external content – from social media ads to product demos – and internal materials like training videos and presentations. Hear directly from the teams:\n\n“Veo 3 has marked the difference within the gen AI industry, and we’re glad that Freepik users have been some of the first to try the model out. The quality of the video generations combined with the audio integration option is the game changer in our AI Suite. We look forward to continuing this collaboration to bring the best AI tools and features to our users” – Omar Pera, CPO, Freepik\n\n“Creativity is deeply personal, and our goal is to build a platform that adapts to every workflow. By working with Google, we’re combining the best technologies to give creators more control, efficiency, and power than ever before. Our collaboration with Google Cloud represents a strategic evolution that will not only enhance accessibility and efficiency but fundamentally transform how people create. We believe the future of generative video technology will leverage the best technologies to build the most flexible and accessible tools. This is an exciting step toward realizing that vision” – Zeev Farbman, Co-Founder & CEO, Lightricks.\n\n\"Veo 3 is the single greatest leap forward in practically useful AI for advertising since genAI first broke into the mainstream in 2023. By allowing brands to make fully fledged films from a single prompt - including brand, story, video, sound effects, voiceovers and more - Veo3 in one swoop lowers the barriers to entry to gen AI for creative people and elevates gen AI to a top tier brand building tool usable at every stage of the marketing funnel.\" – Will Hanschell, co-founder and CEO, , Pencil\n\n**Bring your vision to life with Veo 3 today**\n\nVeo 3 on Vertex AI is built for scalable enterprise use with crucial guardrails like safety filter controls and SynthID to ensure responsible deployment for any use case. To get started, go here to learn more about Veo 3 on Vertex AI and try it on Vertex AI Media Studio. Get started today!",
            "domain": "cloud.google.com"
          },
          {
            "position": 10,
            "title": "Google's Veo 3.1 is better at generating videos from images",
            "url": "https://www.engadget.com/ai/googles-veo-31-is-better-at-generating-videos-from-images-220829129.html",
            "snippet": "Google has released a new update to its Veo AI video generation model that should make it do a better job of sticking to prompts and converting images into videos. Veo 3.1 is available to try today through Google's Gemini API and is now also powering the company's Flow video editor.\nVeo 3.1 builds on the new capabilities Google introduced with launch of Veo 3 at Google I/O 2025. The new model offers better \"prompt adherence,\" according to Google, and should have an easier time creating videos based on the image \"ingredients\" you upload alongside your written prompt. Veo 3.1 also makes it possible to convert images to video and generate audio at the same time, a capability that wasn't available with Veo 3.\nIn Flow, Veo 3.1 supports at least a new feature that gives you finer control over the videos you generate. With what Google calls \"Frame to Video,\" Flow lets you upload a first and last frame, and then generates the video in-between. Adobe Firefly, which is powered by Veo 3, offers a similar feature, but Flow will be able to pull it off and create audio at the same time. Those added audio skills will also apply to the video editor's ability to extend clips and insert objects into existing footage, too.\nBased on the samples Google's shared, videos generated with Veo 3.1 still have an uncanny quality that seems to vary greatly depending on the prompt and subject. Even if it's missing some of the realism of OpenAI's Sora 2, though, the company's decision to try and make Veo more useful to people who actually work with video rather than a source of social media spam is a welcome move.",
            "domain": "www.engadget.com"
          }
        ],
        "success": true,
        "error": null
      },
      {
        "query_id": "q4",
        "query": "Google veo3.1 release news 2025",
        "claim_id": "claim_1",
        "query_type": "source_verification",
        "priority": "high",
        "results": [
          {
            "position": 1,
            "title": "Introducing Veo 3.1 and advanced capabilities in Flow - Google Blog",
            "url": "https://blog.google/technology/ai/veo-updates-flow/",
            "snippet": "# Introducing Veo 3.1 and advanced capabilities in Flow\n\nFive months ago, we introduced Flow, our AI filmmaking tool powered by Veo, and have been inspired by the creativity it has sparked with over 275 million videos generated in Flow\n\n1\n\n. We're always listening to your feedback, and we've heard that you want more artistic control within Flow, with increased support for audio across all features.\n\nToday, we’re introducing new and enhanced creative capabilities to edit your clips, giving you more granular control over your final scene. For the first time, we’re also bringing audio to existing capabilities like “Ingredients to Video,” “Frames to Video” and “Extend.”\n\nWe’re also introducing Veo 3.1, which brings richer audio, more narrative control, and enhanced realism that captures true-to-life textures. Veo 3.1 is state-of-the-art and builds on Veo 3, with stronger prompt adherence and improved audiovisual quality when turning images into videos.\n\n## Refine your narrative with audio and more control\n\nWith Veo 3.1, we’re bringing audio to existing capabilities to help you craft the perfect scene. These features are experimental and actively improving, and we’re excited to see what you create as we iterate based on your feedback.\n\nNow, with rich, generated audio, you can:\n\n**Craft the look of your scene.**With \"Ingredients to Video,\" you can use multiple reference images to control the characters, objects and style. Flow uses your ingredients to create a final scene that looks just as you envisioned. **Control the shot from start to finish.**Provide a starting and ending image with “Frames to Video,” and Flow will generate a seamless video that bridges the two, perfect for artful and epic transitions. **Create longer, seamless shots.**With \"Extend,\" you can create longer videos, even lasting for a minute or more, that connect to and continue the action from your original clip. Each video is generated based on the final second of your previous clip, making it most useful for creating a longer establishing shot.... ## Edit your ingredients and videos with more precision\n\nGreat ideas can strike at any point in the creative process. For moments when the first take isn't the final one, we're introducing new editing capabilities directly within Flow to help you reimagine and perfect your scenes.\n\n**Add new elements to any scene.**With “Insert,” introduce anything you can imagine, from realistic details to fantastical creatures. Flow now handles complex details like shadows and scene lighting, making the addition look natural. **Remove unwanted objects or characters seamlessly.**Soon, you’ll be able to take anything out of a scene, and Flow will reconstruct the background and surroundings, making it look as though the object was never there.\n\n## Start creating in Flow today\n\nWith more precise editing capabilities, audio across all existing features and higher-quality outputs powered by Veo 3.1, we're opening up new possibilities for richer, more powerful video storytelling right inside Flow.\n\nThe Veo 3.1 model is also available via the Gemini API for developers, Vertex AI for enterprise customers, and the Gemini app. New capabilities are available in both Gemini API\n\n2\n\nand Vertex AI\n\n3",
            "domain": "blog.google"
          },
          {
            "position": 2,
            "title": "Google announces Veo 3.1 and updates Flow with more controls, tools",
            "url": "https://9to5google.com/2025/10/15/veo-3-1/",
            "snippet": "Google today announced Veo 3.1 as its latest video generation model, with Flow getting a number of updates to take advantage of the latest capabilities.\n\nCompared to Veo 3, which was announced at I/O 2025 in May, this new version offers richer audio and “enhanced realism that captures true-to-life textures.” Veo 3.1 has a deeper understanding of storytelling, cinematic styles, and character interactions to give you more narrative control. The image-to-video capability benefits from improved audio-visual quality and better follows your prompt.\n\nVeo 3.1 and Veo 3.1 Fast are available in the Gemini app, as well as the Gemini API and Vertex AI, to power text-to-video and image-to-video for horizontal (16×9) and vertical (9×16) outputs.\n\nMeanwhile, Google is updating the Flow filmmaking tool to take advantage of Veo 3.1. Audio generation is coming to:\n\n**Ingredients to Video**: This can include images of multiple characters, objects, and styles **Scene extension**: You can Extend an existing clip, with Google basing it on the last second **Frames to Video**: This lets you specify the starting and ending images, with Flow bridging the two\n\nFlow is getting new editing capabilities. You can insert elements like objects, characters, and details, with Google handling shadows, scene lighting, and other complex details to make everything look natural.\n\nComing soon is the ability to remove objects and characters from a scene, with Flow working to reconstruct the background and surroundings to make the edit seamless.\n\n\n\n## Comments",
            "domain": "9to5google.com"
          },
          {
            "position": 3,
            "title": "Google releases Veo 3.1, adds it to Flow video editor",
            "url": "https://techcrunch.com/2025/10/15/google-releases-veo-3-1-adds-it-to-flow-video-editor/",
            "snippet": "In Brief\nGoogle launched its new video model Veo 3.1 with improved audio output, granular editing controls, and better output for image to video. It said that Veo 3.1 builds on May's Veo 3 release and generates more realistic clips and adheres to prompts better.\nThe model allows users to add an object to the video and have it blend into the clip's style, Google said. Soon, users will be able to remove an existing object from the video in Flow, too.\nVeo 3 already has edit features such as adding reference images to drive a character, providing the first and last frame to generate a clip using AI, and the ability to extend an existing video based on the last few frames. With Veo 3.1, Google is adding audio to all these features to make the clips more lively.\nThe company is rolling out the model to its video editor Flow, the Gemini App, along with Vertex and Gemini APIs. It said that since Flow's launch in May, users have created more than 275 million videos on the app.",
            "domain": "techcrunch.com"
          },
          {
            "position": 4,
            "title": "Introducing Veo 3.1 and new creative capabilities in the Gemini API",
            "url": "https://developers.googleblog.com/en/introducing-veo-3-1-and-new-creative-capabilities-in-the-gemini-api/",
            "snippet": "**Today, we are releasing Veo 3.1 and Veo 3.1 Fast in paid preview in the** **Gemini API** **.** This updated model offers several improvements, as well as improved outputs when generating video from images. These new models are available via the Gemini API in Google AI Studio and Vertex AI. Veo 3.1 is also available in the Gemini app and Flow.\n\nVeo 3.1 and Veo 3.1 Fast empower developers to create more engaging content through significant upgrades. The models now generate\n\n**richer native audio**, from natural conversations to synchronized sound effects, and offer **greater narrative control** with an improved understanding of cinematic styles. **Enhanced image-to-video** capabilities ensure better prompt adherence while delivering superior audio and visual quality and maintaining character consistency across multiple scenes.\n\nIn addition to the model update, we are introducing new model capabilities, including using reference images to guide video generation, extend existing Veo videos, and generate transitions between a first and last frame.\n\nYou can now guide the generation process by providing up to 3 reference images of a character, object, or scene. This is helpful for maintaining character consistency across multiple shots or applying a specific style to your video.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nconfig=types.GenerateVideosConfig(\n\nreference_images=[reference_image1, reference_image2],\n\n),\n\n\n\n```... Your story is no longer limited by the original generation. With Scene extension, you can create longer videos, even lasting for a minute or more, by generating new clips that connect to your previous video. Each new video is generated based on the final second of the previous clip. This maintains visual continuity, making the feature ideal for extending shots with background audio.\n\n```\n\nfrom google import genai\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nvideo=video_to_extend\n\n\n\n```\n\nCreate smooth, natural scenes that bridge two different images. By providing a starting and an ending image, you can direct Veo 3.1 to generate the transition between them, complete with accompanying audio.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nimage=first_frame,\n\nconfig=types.GenerateVideosConfig(\n\nlast_frame=last_frame,\n\n),\n\n\n\n```\n\n**What people are building with Veo 3.1**\n\nPromise Studios, a GenAI movie studio, uses Veo 3.1 within its MUSE Platform to enhance generative storyboarding and previsualization for director-driven storytelling at production quality.... Latitude is experimenting with Veo 3.1 in its generative narrative engine to instantly bring user-created stories to life.\n\nVeo 3.1 and these new capabilities are now available in paid preview via the Gemini API.\n\nVeo 3.1 is the same price as Veo 3. We are incredibly excited to see what you will build with these new capabilities!",
            "domain": "developers.googleblog.com"
          },
          {
            "position": 5,
            "title": "We've got a surprise Pixel Drop for you.",
            "url": "https://blog.google/products/pixel/pixel-drop-july-2025/",
            "snippet": "Here’s what’s new for Pixel:\n\n**Veo 3 on Pixel:**Pixel 9 Pro owners get a full year of our Google AI Pro subscription at no cost, giving them access to the latest features in the Gemini app. And that includes Veo 3, which you can use to describe your idea and watch it come to life as a high-quality, short video, complete with natural audio. **New Circle to Search capabilities:**Dive deeper and ask follow-up questions about anything you see on your screen with AI Mode in Circle to Search, available in the U.S. and India. We’re also adding in-game help in Circle to Search, so you can find helpful articles and videos timestamped to your exact spot in your mobile game, without switching apps. **Gemini on Pixel Watch:**Get the help you need right on your wrist, with our advanced AI models powered by WearOS.",
            "domain": "blog.google"
          },
          {
            "position": 6,
            "title": "Google's Veo 3.1 is better at generating videos from images",
            "url": "https://www.engadget.com/ai/googles-veo-31-is-better-at-generating-videos-from-images-220829129.html",
            "snippet": "Google has released a new update to its Veo AI video generation model that should make it do a better job of sticking to prompts and converting images into videos. Veo 3.1 is available to try today through Google's Gemini API and is now also powering the company's Flow video editor.\nVeo 3.1 builds on the new capabilities Google introduced with launch of Veo 3 at Google I/O 2025. The new model offers better \"prompt adherence,\" according to Google, and should have an easier time creating videos based on the image \"ingredients\" you upload alongside your written prompt. Veo 3.1 also makes it possible to convert images to video and generate audio at the same time, a capability that wasn't available with Veo 3.\nIn Flow, Veo 3.1 supports at least a new feature that gives you finer control over the videos you generate. With what Google calls \"Frame to Video,\" Flow lets you upload a first and last frame, and then generates the video in-between. Adobe Firefly, which is powered by Veo 3, offers a similar feature, but Flow will be able to pull it off and create audio at the same time. Those added audio skills will also apply to the video editor's ability to extend clips and insert objects into existing footage, too.\nBased on the samples Google's shared, videos generated with Veo 3.1 still have an uncanny quality that seems to vary greatly depending on the prompt and subject. Even if it's missing some of the realism of OpenAI's Sora 2, though, the company's decision to try and make Veo more useful to people who actually work with video rather than a source of social media spam is a welcome move.",
            "domain": "www.engadget.com"
          },
          {
            "position": 7,
            "title": "Google rolls out its new Veo 3 video-generation model ...",
            "url": "https://techcrunch.com/2025/07/03/google-rolls-out-its-new-veo-3-video-generation-model-globally/",
            "snippet": "Google on Thursday said it has begun rolling out its Veo 3 video generation model to Gemini users in more than 159 countries.\n\nVideo generation via the new model is available only to paying subscribers of Google’s AI Pro plan and is capped at three videos per day.\n\nVeo 3, which Google showed off in May, lets users generate videos up to eight seconds long using text prompts.\n\nGoogle’s Josh Woodward has said that the company is working on adding image-to-video generation capabilities to Gemini.",
            "domain": "techcrunch.com"
          },
          {
            "position": 8,
            "title": "Google's Veo 3 Update! July 2025!",
            "url": "https://www.youtube.com/watch?v=qhReJkSRKOc&vl=en",
            "snippet": "## Murray Frost\n##### Jul 09, 2025 (0:04:15)\n✅ Build a Monetized YouTube Channel in 90 days: https://murrayfrost.com/YT-Accelerator\n\nI teach people YouTube from REAL data from over 150 clients and my own channels. Data-driven feedback and strategies. None of this guessing garbage people put on online teaching you how to do YouTube.... {ts:0} So, Google Labs just had an update and it's technically called Flow. It's on\n{ts:4} their labs.google platform and you can see by the beginning of this video that it does still need some work, but you\n{ts:9} haven't been able to do that recently with Google Labs. And now you can do it with Google's Vo3. So, there's been a\n{ts:15} couple updates here that you can see. And the first one here is using images or allowing images to talk with Google's\n{ts:20} VO3, which again still needs some work, but look, it gets maybe 60% of the way there. I think it looks pretty decent.\n{ts:29} I'm obviously not going to use it to try and convince people that it's real, but maybe you can get kind of creative with\n{ts:34} this and get people to make some really funny, strange, or just dumb things that people love. I don't know why people\n{ts:42} just love brain rot these days. Now, they're also adding the option to do this with V V3 on frames to video, which\n{ts:49} is actually kind of cool. And they... 're also allowing you to top up your subscription with the kind of a mid tier\n{ts:56} option cuz previously they had just the the starter which was about 20 bucks or so per month in the US and then it was\n{ts:63} straight up to I think 250 without the discount for the first 3 months and there's just no in between. It's just a\n{ts:70} massive jump. So they added like a kind of a mid tier there. I think a couple too. Well, I'll show you what that looks\n{ts:75} like and you can top up your credits there as well. They've also gone ahead and just added better audio coverage,\n{ts:83} which I haven't really noticed all that much to be honest. Right now, there's not a huge difference as at least a\n{ts:89} noticeable difference in my opinion from me using it. Um, they also do remove audio when miners are involved. Keep\n{ts:96} that in mind. That's why your audio isn't being generated if you have kids in the video or maybe uh even teenagers\n{ts:101} sometimes. Um, but then there's also they've mentioned they're reducing um unwanted subtitles, which is actually... {ts:107} quite nice. They've been removing the the VEO watermark as well, but now they said they've reduced the unwanted\n{ts:114} subtitles. I still get them sometimes. So, I literally in caps specify in the prompt to not include captions because\n{ts:121} otherwise if I don't, sometimes the captions still show up. And the really nice quality of life update they've made\n{ts:127} here is that when you are just starting a new project or revisiting an existing one, it doesn't reset the model that you\n{ts:135} have, or at least if it does, it resets to the VO3 fast beta audio. So, this is where you're generating audio. So now\n{ts:142} you don't accidentally have VO2 selected with no audio every time you either reload a page, start a new project, or\n{ts:150} you leave and come back and it's been reset to just its default VO2. Now it's actually its default is V3 fast beta\n{ts:157} audio. So the 20 credits per generation, the cheaper VO3 option with audio. So now you don't accidentally generate\n{ts:164} videos without audio, which has happened a lot to me and I... 've wasted thousands of credits doing that by accident. So,\n{ts:171} great update right there. Quality of life, which you don't have to waste any more credits. Now, now let's say that\n{ts:176} you don't want to spend the $124 per month, and this is for the first 3 months. Then, it goes to 150, I believe,\n{ts:183} per month. So, I'm probably going to cancel it at that point because that's just really expensive, at least using\n{ts:189} VO3. Now you have the option if you have the uh let's see which was it the pro subscription the $20 a month\n{ts:195} subscription right here Google AI pro you still get a th000 credits per month in uh Google labs but you also have the\n{ts:204} option to top up your credits so for example in here when you're creating your AI videos if you run out of credits\n{ts:211} like here you can just hit get more AI credits and you can choose how much you want to add so you don't have to spend\n{ts:216} $200 or $150 at a time you could spend an extra for 24 bucks that month just to top up your credits. This wasn... 't\n{ts:223} available in the lower plans. It was only available in the the maximum ultra tier. So, I really like the ability to\n{ts:230} do that now. So, you don't have to spend so much all at once, especially if you're not using all your credits at the\n{ts:235} end of each month, but maybe one month you're just out and you need to add more, you can do so and add some\n{ts:240} credits, which is actually quite nice. I've hit this button a little too much recently, but uh hey, I'm getting good\n{ts:247} results. So, those are all the current updates with Google's Veo3 with their labs platform. Hope to see you in the\n{ts:253} next one.",
            "domain": "www.youtube.com"
          },
          {
            "position": 9,
            "title": "Build with Veo 3, now available in the Gemini API",
            "url": "https://developers.googleblog.com/en/veo-3-now-available-gemini-api/",
            "snippet": "First unveiled at Google I/O 2025, people around the world have already generated tens of millions of high-quality videos with Veo 3 (along with some new fun and interesting video trends). It is our first video model to incorporate high-fidelity video outputs and native audio, first with text-to-video and soon with image-to-video.\n\nDevelopers are already experimenting with Veo 3, discovering how the model can help them brainstorm content, rapidly iterate, and be more efficient.\n\nVeo 3 is designed to handle a range of video generation tasks, from cinematic narratives to dynamic character animations. With Veo 3, you can create more immersive experiences by not only generating stunning visuals, but also audio like dialogue and sound effects.\n\nLet’s take a look at some examples.\n\nExplore these examples and more with Veo 3 in Google AI Studio, available as an SDK template and interactive Starter App to remix, copy and extend. The Starter App and its sample code offer a convenient way for Paid Tier users to rapidly prototype with Veo 3 and more on the Gemini API, directly from Google AI Studio.\n\nClick the Key button in the top right of the AI Studio Build interface to select a Google Cloud Project with billing enabled to use the Paid Tier in AI Studio apps. See the FAQs for more.... Veo 3 will be priced at $0.75 per second for video and audio output. Additionally, Veo 3 Fast will be available soon, offering a faster and more cost-effective option for video creation.\n\nHere’s a basic Python example to create a video:\n\n```\n\nimport time\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.0-generate-preview\",\n\nprompt=\"a close-up shot of a golden retriever playing in a field of sunflowers\",\n\nconfig=types.GenerateVideosConfig(\n\nnegative_prompt=\"barking, woofing\",\n\n),\n\n\n\n# Waiting for the video(s) to be generated\n\nwhile not operation.done:\n\ntime.sleep(20)\n\noperation = client.operations.get(operation)\n\ngenerated_video = operation.result.generated_videos[0]\n\nclient.files.download(file=generated_video.video)\n\ngenerated_video.video.save(\"veo3_video.mp4\")\n\n```\n\nAll videos generated by Veo 3 models will continue to include a digital SynthID watermark. To get started, check out the documentation, cookbook, and a Veo 3 starter app in Google AI Studio:\n\nIn addition to being available via the Gemini API in Google AI Studio, Veo 3 is also available to Google AI subscribers in the Gemini app and Flow, and to enterprise customers via Vertex AI.... 1: Fluffy Characters Stop Motion: Inside a brightly colored, cozy kitchen made of felt and yarn. Professor Nibbles, a plump, fluffy hamster with oversized glasses, nervously stirs a bubbling pot on a miniature stove, muttering, \"Just a little more... 'essence of savory,' as the recipe calls for.\" The camera is a mid-shot, capturing his frantic stirring. Suddenly, the pot emits a loud \"POP!\" followed by a comical \"whoosh\" sound, and a geyser of iridescent green slime erupts, covering the entire kitchen. Professor Nibbles shrieks, \"Oh, dear! Not again!\" and scurries away, leaving a trail of tiny, panicked squeaks.\n\n2: The sequence begins with an extreme close-up of a single gear, slowly turning and reflecting harsh sunlight. The camera gradually pulls back in a continuous movement, revealing this is but one component of a colossal, mechanical heart half-buried in a desolate, rust-colored desert. A sweeping aerial shot establishes its enormous scale and isolation in the barren landscape. The camera descends to capture pipes hissing steam and the rhythmic thumping that echoes across the empty plains. A subtle shake effect synchronizes with each massive heartbeat. A lateral tracking shot discovers tiny, robed figures scurrying across the metallic surface. The camera follows one such figure in a detailed tracking shot as they perform meticulous maintenance, polishing brass valves and tightening immense bolts. A complex movement circles the entire structure, capturing different maintenance teams working in precarious positions across its rusted exterior. The final shot begins tight on the meticulous work of one tiny figure before executing a dramatic pull-out that reveals the true scale of the heart and the minuscule size of its caretakers, tending to the vital organ of an unseen, sleeping giant that extends beyond the frame.",
            "domain": "developers.googleblog.com"
          },
          {
            "position": 10,
            "title": "Google Veo 3.1 Just Dropped — How to Use It + What’s New! ✅",
            "url": "https://www.youtube.com/watch?v=y0QZbjd2f_k",
            "snippet": "## Aivoxy\n##### Oct 15, 2025 (0:03:37)\nGoogle Veo 3.1 Just Dropped — How to Use It + What’s New! ✅\n\n🔥 For Prompts:\nhttps://t.me/aivoxxy\n\nveo 3.1\ngoogle veo 3.1\nveo3.1\nveo 3\ngoogle veo 3\nveo 3.1 release date\nveo 3.1 generated videos\ngoogle veo 3.1\n\nGoogle DeepMind just released Veo 3.1, and it’s packed with upgrades that could change the AI video game forever.\nIn this video, we demonstrate how to use Veo 3.1, walk you through the new features, and explain why this update is so exciting.\n\nSome highlights of Veo 3.1 include:\nScene Builder — plan your video shot by shot\nCharacter Consistency — no more random face changes\nNative 1080p output with cinematic presets\nMulti-prompting for multi-shot sequences\nAudio & dialogue generation built-in\n\nWe’ll also compare it with other AI video tools like Sora 2, so you know what’s really new and what’s just hype.\n\n👨‍👩‍👦‍👦 Join this channel to get access to perks:\nhttps://www.youtube.com/channel/UC4VpM2gtTxbXSiyL1ODl9uQ/join\n\n🔥 This AI Bigfoot Vlog Looks 100% Real – Made with Veo 3!\nhttps://youtu.be/_1EH25-1NkQ\n\n🔥 Google Veo 3 Just Changed Video Creation Forever! 😱🚨\nhttps://youtu.be/-_4jOwjQaQI... 🔥 Google AI Mode Just Changed Search FOREVER!\nhttps://youtu.be/JX64HQ8_QCg\n--------------------------------------------------------------------------\n🔥 Grok 3 Now Has Voice! 🔥 4 Mind-Blowing Examples You Need to See!\nhttps://youtu.be/B_pKBwj08Wg\n\n🔥 Grok 3 vs ChatGPT: Unfiltered Voice Chat? Surprising Responses!\nhttps://youtu.be/snOj_6VytiY\n\n🔥 Grok 3 vs ChatGPT vs DeepSeek vs Claude 3.5: Who's the Brainiest? 🤯\nhttps://youtu.be/XKDdUuy-hmE... 🔥 You Think It's Fake - DeepSeek vs ChatGPT\nhttps://youtu.be/SruDuJkw78U\n-------------------------------------------------------------------------\n#veo3.1 #veo3 #aivideo... {ts:0} Come closer. Let me share a secret with you.\n{ts:5} I am your new girlfriend. Google just surprised everyone. VO 3.1 is officially live today. It's faster,\n{ts:22} smarter, and now generates full cinematic clips with sound. And yeah, I've already got access. Let me show you\n{ts:30} how crazy this is. So, here's the thing. VO3.1 isn't available everywhere just yet. Right now, it's rolling out only\n{ts:39} inside Google Flow. So, if you already have a Google AI account, you can access it right from there. I'm already logged\n{ts:46} in and as you can see, the VO3.1 fast model is live. So, yeah, no fake claims here. This is real and I've been talking\n{ts:55} about this since my earlier videos. Now using it is simple. Write your prompt. Select VO3.1 fast. Hit generate. That's\n{ts:65} it. No complex settings, no extra steps. All right, let's talk about what's actually\n{ts:79} new in VO3.1. This update isn't a total overhaul. It... 's more like a smarter, cleaner upgrade to\n{ts:87} V3. Here's what's changed. One, smarter scene creation. You can now combine reference images, locations, characters,\n{ts:96} and objects, and VO automatically builds a cohesive scene around them. No more weird lighting mismatches or random jump\n{ts:104} cuts. Two, built-in sound generation. Every clip now includes matching audio, ambient sounds, dialogue, or effects\n{ts:113} that sync perfectly with the visuals. It makes your videos feel far more cinematic and alive. Three, clip\n{ts:121} extension. You can take a short clip and expand it into a full cinematic sequence. VO keeps the same lighting,\n{ts:129} motion, and audio flow seamlessly. It's perfect for trailers, ads, or storydriven projects. Four, custom start\n{ts:138} and end control. Now you can choose where a shot begins and ends, and VO fills in the middle with smooth,\n{ts:145} realistic transitions that gives you more storytelling control. Five, add or remove elements. Want to remove an\n{ts:153} object or drop in a new one? Just select it. VO automatically adjusts lighting, shadows, and scale to keep everything... {ts:161} realistic. Six, higher realism and physics accuracy. Scenes now behave like real life. Lighting looks natural,\n{ts:170} character movement feels grounded, and the visuals have that cinematic finish. You'll also notice image to video\n{ts:176} generation looks a lot cleaner now. Frames are stable, colors look balanced, and it feels much closer to real\n{ts:183} footage. That said, it still can't generate long videos yet. The maximum I've seen is around 8 seconds. So don't\n{ts:191} expect 1 minute clips for now. So yeah, not a massive reinvention, but a much smarter, smoother version of what VO\n{ts:198} already did well. I'll be testing this more and posting sideby-side comparisons soon. Stay tuned to see how it compares\n{ts:205} to VO3. If this video helped you, hit like, share it with your tech buddies, and make sure you subscribe for more AI\n{ts:212} updates like this. Thanks for watching and I'll see you in the next",
            "domain": "www.youtube.com"
          }
        ],
        "success": true,
        "error": null
      },
      {
        "query_id": "q3",
        "query": "veo3.1 software version Google 2025 official",
        "claim_id": "claim_1",
        "query_type": "source_verification",
        "priority": "high",
        "results": [
          {
            "position": 1,
            "title": "Introducing Veo 3.1 and advanced capabilities in Flow - Google Blog",
            "url": "https://blog.google/technology/ai/veo-updates-flow/",
            "snippet": "# Introducing Veo 3.1 and advanced capabilities in Flow\n\nFive months ago, we introduced Flow, our AI filmmaking tool powered by Veo, and have been inspired by the creativity it has sparked with over 275 million videos generated in Flow\n\n1\n\n. We're always listening to your feedback, and we've heard that you want more artistic control within Flow, with increased support for audio across all features.\n\nToday, we’re introducing new and enhanced creative capabilities to edit your clips, giving you more granular control over your final scene. For the first time, we’re also bringing audio to existing capabilities like “Ingredients to Video,” “Frames to Video” and “Extend.”\n\nWe’re also introducing Veo 3.1, which brings richer audio, more narrative control, and enhanced realism that captures true-to-life textures. Veo 3.1 is state-of-the-art and builds on Veo 3, with stronger prompt adherence and improved audiovisual quality when turning images into videos.\n\n## Refine your narrative with audio and more control\n\nWith Veo 3.1, we’re bringing audio to existing capabilities to help you craft the perfect scene. These features are experimental and actively improving, and we’re excited to see what you create as we iterate based on your feedback.\n\nNow, with rich, generated audio, you can:\n\n**Craft the look of your scene.**With \"Ingredients to Video,\" you can use multiple reference images to control the characters, objects and style. Flow uses your ingredients to create a final scene that looks just as you envisioned. **Control the shot from start to finish.**Provide a starting and ending image with “Frames to Video,” and Flow will generate a seamless video that bridges the two, perfect for artful and epic transitions. **Create longer, seamless shots.**With \"Extend,\" you can create longer videos, even lasting for a minute or more, that connect to and continue the action from your original clip. Each video is generated based on the final second of your previous clip, making it most useful for creating a longer establishing shot.... ## Edit your ingredients and videos with more precision\n\nGreat ideas can strike at any point in the creative process. For moments when the first take isn't the final one, we're introducing new editing capabilities directly within Flow to help you reimagine and perfect your scenes.\n\n**Add new elements to any scene.**With “Insert,” introduce anything you can imagine, from realistic details to fantastical creatures. Flow now handles complex details like shadows and scene lighting, making the addition look natural. **Remove unwanted objects or characters seamlessly.**Soon, you’ll be able to take anything out of a scene, and Flow will reconstruct the background and surroundings, making it look as though the object was never there.\n\n## Start creating in Flow today\n\nWith more precise editing capabilities, audio across all existing features and higher-quality outputs powered by Veo 3.1, we're opening up new possibilities for richer, more powerful video storytelling right inside Flow.\n\nThe Veo 3.1 model is also available via the Gemini API for developers, Vertex AI for enterprise customers, and the Gemini app. New capabilities are available in both Gemini API\n\n2\n\nand Vertex AI\n\n3",
            "domain": "blog.google"
          },
          {
            "position": 2,
            "title": "Build with Veo 3, now available in the Gemini API",
            "url": "https://developers.googleblog.com/en/veo-3-now-available-gemini-api/",
            "snippet": "First unveiled at Google I/O 2025, people around the world have already generated tens of millions of high-quality videos with Veo 3 (along with some new fun and interesting video trends). It is our first video model to incorporate high-fidelity video outputs and native audio, first with text-to-video and soon with image-to-video.\n\nDevelopers are already experimenting with Veo 3, discovering how the model can help them brainstorm content, rapidly iterate, and be more efficient.\n\nVeo 3 is designed to handle a range of video generation tasks, from cinematic narratives to dynamic character animations. With Veo 3, you can create more immersive experiences by not only generating stunning visuals, but also audio like dialogue and sound effects.\n\nLet’s take a look at some examples.\n\nExplore these examples and more with Veo 3 in Google AI Studio, available as an SDK template and interactive Starter App to remix, copy and extend. The Starter App and its sample code offer a convenient way for Paid Tier users to rapidly prototype with Veo 3 and more on the Gemini API, directly from Google AI Studio.\n\nClick the Key button in the top right of the AI Studio Build interface to select a Google Cloud Project with billing enabled to use the Paid Tier in AI Studio apps. See the FAQs for more.... Veo 3 will be priced at $0.75 per second for video and audio output. Additionally, Veo 3 Fast will be available soon, offering a faster and more cost-effective option for video creation.\n\nHere’s a basic Python example to create a video:\n\n```\n\nimport time\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.0-generate-preview\",\n\nprompt=\"a close-up shot of a golden retriever playing in a field of sunflowers\",\n\nconfig=types.GenerateVideosConfig(\n\nnegative_prompt=\"barking, woofing\",\n\n),\n\n\n\n# Waiting for the video(s) to be generated\n\nwhile not operation.done:\n\ntime.sleep(20)\n\noperation = client.operations.get(operation)\n\ngenerated_video = operation.result.generated_videos[0]\n\nclient.files.download(file=generated_video.video)\n\ngenerated_video.video.save(\"veo3_video.mp4\")\n\n```\n\nAll videos generated by Veo 3 models will continue to include a digital SynthID watermark. To get started, check out the documentation, cookbook, and a Veo 3 starter app in Google AI Studio:\n\nIn addition to being available via the Gemini API in Google AI Studio, Veo 3 is also available to Google AI subscribers in the Gemini app and Flow, and to enterprise customers via Vertex AI.... 1: Fluffy Characters Stop Motion: Inside a brightly colored, cozy kitchen made of felt and yarn. Professor Nibbles, a plump, fluffy hamster with oversized glasses, nervously stirs a bubbling pot on a miniature stove, muttering, \"Just a little more... 'essence of savory,' as the recipe calls for.\" The camera is a mid-shot, capturing his frantic stirring. Suddenly, the pot emits a loud \"POP!\" followed by a comical \"whoosh\" sound, and a geyser of iridescent green slime erupts, covering the entire kitchen. Professor Nibbles shrieks, \"Oh, dear! Not again!\" and scurries away, leaving a trail of tiny, panicked squeaks.\n\n2: The sequence begins with an extreme close-up of a single gear, slowly turning and reflecting harsh sunlight. The camera gradually pulls back in a continuous movement, revealing this is but one component of a colossal, mechanical heart half-buried in a desolate, rust-colored desert. A sweeping aerial shot establishes its enormous scale and isolation in the barren landscape. The camera descends to capture pipes hissing steam and the rhythmic thumping that echoes across the empty plains. A subtle shake effect synchronizes with each massive heartbeat. A lateral tracking shot discovers tiny, robed figures scurrying across the metallic surface. The camera follows one such figure in a detailed tracking shot as they perform meticulous maintenance, polishing brass valves and tightening immense bolts. A complex movement circles the entire structure, capturing different maintenance teams working in precarious positions across its rusted exterior. The final shot begins tight on the meticulous work of one tiny figure before executing a dramatic pull-out that reveals the true scale of the heart and the minuscule size of its caretakers, tending to the vital organ of an unseen, sleeping giant that extends beyond the frame.",
            "domain": "developers.googleblog.com"
          },
          {
            "position": 3,
            "title": "Google announces Veo 3.1 and updates Flow with more controls, tools",
            "url": "https://9to5google.com/2025/10/15/veo-3-1/",
            "snippet": "Google today announced Veo 3.1 as its latest video generation model, with Flow getting a number of updates to take advantage of the latest capabilities.\n\nCompared to Veo 3, which was announced at I/O 2025 in May, this new version offers richer audio and “enhanced realism that captures true-to-life textures.” Veo 3.1 has a deeper understanding of storytelling, cinematic styles, and character interactions to give you more narrative control. The image-to-video capability benefits from improved audio-visual quality and better follows your prompt.\n\nVeo 3.1 and Veo 3.1 Fast are available in the Gemini app, as well as the Gemini API and Vertex AI, to power text-to-video and image-to-video for horizontal (16×9) and vertical (9×16) outputs.\n\nMeanwhile, Google is updating the Flow filmmaking tool to take advantage of Veo 3.1. Audio generation is coming to:\n\n**Ingredients to Video**: This can include images of multiple characters, objects, and styles **Scene extension**: You can Extend an existing clip, with Google basing it on the last second **Frames to Video**: This lets you specify the starting and ending images, with Flow bridging the two\n\nFlow is getting new editing capabilities. You can insert elements like objects, characters, and details, with Google handling shadows, scene lighting, and other complex details to make everything look natural.\n\nComing soon is the ability to remove objects and characters from a scene, with Flow working to reconstruct the background and surroundings to make the edit seamless.\n\n\n\n## Comments",
            "domain": "9to5google.com"
          },
          {
            "position": 4,
            "title": "Introducing Veo 3.1 and new creative capabilities in the Gemini API",
            "url": "https://developers.googleblog.com/en/introducing-veo-3-1-and-new-creative-capabilities-in-the-gemini-api/",
            "snippet": "**Today, we are releasing Veo 3.1 and Veo 3.1 Fast in paid preview in the** **Gemini API** **.** This updated model offers several improvements, as well as improved outputs when generating video from images. These new models are available via the Gemini API in Google AI Studio and Vertex AI. Veo 3.1 is also available in the Gemini app and Flow.\n\nVeo 3.1 and Veo 3.1 Fast empower developers to create more engaging content through significant upgrades. The models now generate\n\n**richer native audio**, from natural conversations to synchronized sound effects, and offer **greater narrative control** with an improved understanding of cinematic styles. **Enhanced image-to-video** capabilities ensure better prompt adherence while delivering superior audio and visual quality and maintaining character consistency across multiple scenes.\n\nIn addition to the model update, we are introducing new model capabilities, including using reference images to guide video generation, extend existing Veo videos, and generate transitions between a first and last frame.\n\nYou can now guide the generation process by providing up to 3 reference images of a character, object, or scene. This is helpful for maintaining character consistency across multiple shots or applying a specific style to your video.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nconfig=types.GenerateVideosConfig(\n\nreference_images=[reference_image1, reference_image2],\n\n),\n\n\n\n```... Your story is no longer limited by the original generation. With Scene extension, you can create longer videos, even lasting for a minute or more, by generating new clips that connect to your previous video. Each new video is generated based on the final second of the previous clip. This maintains visual continuity, making the feature ideal for extending shots with background audio.\n\n```\n\nfrom google import genai\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nvideo=video_to_extend\n\n\n\n```\n\nCreate smooth, natural scenes that bridge two different images. By providing a starting and an ending image, you can direct Veo 3.1 to generate the transition between them, complete with accompanying audio.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nimage=first_frame,\n\nconfig=types.GenerateVideosConfig(\n\nlast_frame=last_frame,\n\n),\n\n\n\n```\n\n**What people are building with Veo 3.1**\n\nPromise Studios, a GenAI movie studio, uses Veo 3.1 within its MUSE Platform to enhance generative storyboarding and previsualization for director-driven storytelling at production quality.... Latitude is experimenting with Veo 3.1 in its generative narrative engine to instantly bring user-created stories to life.\n\nVeo 3.1 and these new capabilities are now available in paid preview via the Gemini API.\n\nVeo 3.1 is the same price as Veo 3. We are incredibly excited to see what you will build with these new capabilities!",
            "domain": "developers.googleblog.com"
          },
          {
            "position": 5,
            "title": "Google's Veo 3.1 is better at generating videos from images",
            "url": "https://www.engadget.com/ai/googles-veo-31-is-better-at-generating-videos-from-images-220829129.html",
            "snippet": "Google has released a new update to its Veo AI video generation model that should make it do a better job of sticking to prompts and converting images into videos. Veo 3.1 is available to try today through Google's Gemini API and is now also powering the company's Flow video editor.\nVeo 3.1 builds on the new capabilities Google introduced with launch of Veo 3 at Google I/O 2025. The new model offers better \"prompt adherence,\" according to Google, and should have an easier time creating videos based on the image \"ingredients\" you upload alongside your written prompt. Veo 3.1 also makes it possible to convert images to video and generate audio at the same time, a capability that wasn't available with Veo 3.\nIn Flow, Veo 3.1 supports at least a new feature that gives you finer control over the videos you generate. With what Google calls \"Frame to Video,\" Flow lets you upload a first and last frame, and then generates the video in-between. Adobe Firefly, which is powered by Veo 3, offers a similar feature, but Flow will be able to pull it off and create audio at the same time. Those added audio skills will also apply to the video editor's ability to extend clips and insert objects into existing footage, too.\nBased on the samples Google's shared, videos generated with Veo 3.1 still have an uncanny quality that seems to vary greatly depending on the prompt and subject. Even if it's missing some of the realism of OpenAI's Sora 2, though, the company's decision to try and make Veo more useful to people who actually work with video rather than a source of social media spam is a welcome move.",
            "domain": "www.engadget.com"
          },
          {
            "position": 6,
            "title": "Veo 3 Fast available for everyone on Vertex AI | Google Cloud Blog",
            "url": "https://cloud.google.com/blog/products/ai-machine-learning/veo-3-fast-available-for-everyone-on-vertex-ai",
            "snippet": "# Veo 3 and Veo 3 Fast are now generally available on Vertex AI\n\n##### Jason Gelman\n\nDirector, Product Management, Vertex AI... ##### Try Gemini 2.5\n\nOur most intelligent model is now available on Vertex AITry now\n\nVeo 3 has seen massive global adoption with over 70 million videos created since May, and we've seen tremendous momentum with our enterprise customers as well. Since its preview launch on Vertex AI in June, enterprise customers have already generated over 6 million videos, showcasing the incredible demand for professional-grade, scalable AI video creation.\n\nToday, we’re building on this momentum with some exciting updates to Veo on Vertex AI.\n\n\n\n**Veo 3,**our most advanced video generation model, is now generally available to everyone on Vertex AI.\n\n\n\n**Veo 3 Fast**, a model designed for speed and rapid iteration, is now generally available for everyone on Vertex AI. It's a faster way to turn text to video, from narrated product demos to short films.\n\n\n\nComing to public preview on Vertex AI in August, Veo 3 and Veo 3 Fast will also offer\n\n**image-to-video capabilities**to make it possible for you to bring static visuals and images to life. All you have to do is provide the source image along with a text prompt that describes what kind of video you want to create.\n\n**How businesses are building with Veo 3 on Vertex AI**... Google Cloud customers around the world are using Veo 3 and Veo 3 Fast on Vertex AI to create professional-quality video content with unparalleled efficiency and creative freedom. Let’s look at some examples.\n\n**Canva**\n\n“Enabling anyone to bring their ideas to life – especially their most creative ones – has been core to Canva's mission ever since we set out to empower the world to design. By democratising access to a powerful technology like Google’s Veo 3 inside Canva AI, your big ideas can now be brought to life in the highest quality video and sound, all from within your existing Canva subscription. In true Canva fashion, we’ve built this with an intuitive interface and simple editing tools in place, all backed by Canva Shield.” –\n\n**Cameron Adams, co-founder and Chief Product Officer, Canva **\n\nBut the momentum extends beyond design. The team at\n\n**BarkleyOKRP**, a leading ad agency, is using Veo 3 to speed up video production timelines.\n\n“The rapid advancements from Veo 2 to Veo 3 within such a short time frame on this project have been nothing short of remarkable. Our team undertook the task of re-creating numerous music videos initially produced with Veo 2 once Veo 3 was released, primarily due to the significantly improved synchronization between voice and mouth movements. The continuous daily progress we are witnessing is truly extraordinary.” –... **Julie Ray Barr, Senior Vice President Client Experience, BarkleyOKRP**\n\nAt global investing platform\n\n**eToro**, the team is making marketing iterations a breeze with Veo 3.\n\n“At eToro, innovation is in our DNA. As a global investing platform serving clients in 75 countries, local storytelling isn’t optional - it’s essential. With Veo 3, we produced 15 fully AI‑generated versions of our ad, each in the native language of its market, all while capturing real emotion at scale. Ironically, AI didn’t reduce humanity - it amplified it. Veo 3 lets us tell more stories, in more tongues, with more impact.” –\n\n**Shay Chikotay, Head of Creative & Content, eToro**\n\n**Razorfish**, an interactive agency and part of the Publicis Groupe, is using Veo to bring creative to life.\n\n\"For The Morelandos, our campaign with Visit Orlando and Google, we used the full Vertex AI stack—Gemini to mine real reviews, Imagen to bring the characters to life, and Veo to give them motion. Veo let us go from story to near-cinematic video in a fraction of the usual time—which meant more room to explore, iterate, and push the idea further.\" –... **Anthony Yell, Chief Creative Officer, Razorfish**\n\n**Synthesia** **, **a leading synthetic media generation company, is using Veo to contextually adapt visuals to its hyper-realistic AI avatars and voices.\n\n“Veo 3 represents a leap forward in generative AI, and its integration into Synthesia’s platform will redefine how businesses create video content. By combining our hyper-realistic AI avatars and voices with Veo-powered fully contextual visuals that adapt to each unique story, we’re giving enterprise teams the creative power to communicate with unrivalled clarity and impact.” –\n\n**Bill Leaver, Product Manager, Synthesia **\n\n**How enterprises can use Veo 3 Fast for speed and creativity **\n\n**Veo 3 Fast** is a great fit for work that requires rapid iteration and speed. It has an ideal balance between processing time and high-quality visual output, making it especially helpful for:\n\n\n\nQuickly generating and testing variations of ad concepts to respond to market trends.\n\n\n\nEfficiently creating video demonstrations for entire product catalogs from still images.\n\n\n\nDeveloping engaging animated explainers and training modules in less time.\n\n**Veo 3 and Veo 3 Fast on Vertex AI mean even more capabilities for enterprise storytelling**\n\nVeo 3 and Veo 3 Fast are designed to give creators the control and quality needed to move beyond short clips and produce complete, compelling narratives. Here are some of the core features now generally available on Vertex AI.... **Create scenes with native audio:**Veo 3 generates video and audio in a single step. This means you can create scenes with characters that speak with accurate lip-syncing, and sound effects that fit the mood. **Deliver professional quality at enterprise scale:**Veo 3 produces high-definition (1080p) video, suitable for professional marketing campaigns, product demonstrations, and internal communications. You can create content that meets brand standards, saving time and money. **Simplify content localization for global audiences:**Veo 3’s native dialogue generation helps businesses connect with an international audience by producing a video once and localizing the dialogue for dozens of languages. **Image-to-video (coming to public preview on Vertex AI in August):**Veo 3 and Veo 3 Fast can also take a single image, which can be a photo you uploaded or an AI-generated image, and animate it, creating an 8-second video clip. This feature is particularly powerful for content creators, marketers, and businesses looking to animate existing visual assets, create engaging social media content, or generate compelling product demonstrations from high-quality images.\n\n### Enterprise-grade safety and security\n\nVeo 3 and Veo 3 Fast on Vertex AI are built for scalable and responsible enterprise use. We embed digital watermarks into every frame with SynthID, helping combat misinformation and misattribution. Veo 3 and Veo 3 Fast are also covered by our indemnity for generative AI services.... ### Get started with Veo 3 and Veo 3 Fast today\n\nTo get started, go here to learn more about Veo 3 and Veo 3 Fast on Vertex AI, and try it on Vertex AI Media Studio.",
            "domain": "cloud.google.com"
          },
          {
            "position": 7,
            "title": "Generate videos with Veo 3.1 in Gemini API",
            "url": "https://ai.google.dev/gemini-api/docs/video",
            "snippet": "Veo 3.1 is Google's state-of-the-art model for generating high-fidelity, 8-second 720p or 1080p videos featuring stunning realism and natively generated audio. You can access this model programmatically using the Gemini API. To learn more about the available Veo model variants, see the Model Versions section.\n\nVeo 3.1 excels at a wide range of visual and cinematic styles and introduces several new capabilities:\n\n**Video extension**: Extend videos that were previously generated using Veo. **Frame-specific generation**: Generate a video by specifying the first and last frames. **Image-based direction**: Use up to three reference images to guide the content of your generated video.\n\nFor more information about writing effective text prompts for video generation, see the Veo prompt guide... ### Go\n\n```\n\npackage main\n\nimport (\n\n\"context\"\n\n\"log\"\n\n\"os\"\n\n\"time\"\n\n\"google.golang.org/genai\"\n\n\n\nfunc main() {\n\nctx := context.Background()\n\nclient, err := genai.NewClient(ctx, nil)\n\nif err != nil {\n\nlog.Fatal(err)\n\n\n\nprompt := `A close up of two people staring at a cryptic drawing on a wall, torchlight flickering.\n\nA man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'`\n\noperation, _ := client.Models.GenerateVideos(\n\nctx,\n\n\"veo-3.1-generate-preview\",\n\nprompt,\n\nnil,\n\nnil,\n\n\n\n// Poll the operation status until the video is ready.\n\nfor !operation.Done {\n\nlog.Println(\"Waiting for video generation to complete...\")\n\ntime.Sleep(10 * time.Second)\n\noperation, _ = client.Operations.GetVideosOperation(ctx, operation, nil)\n\n\n\n// Download the generated video.\n\nvideo := operation.Response.GeneratedVideos[0]\n\nclient.Files.Download(ctx, video.Video, nil)\n\nfname := \"dialogue_example.mp4\"\n\n_ = os.WriteFile(fname, video.Video.VideoBytes, 0644)\n\nlog.Printf(\"Generated video saved to %s\\n\", fname)\n\n\n\n```... ### JavaScript\n\n```\n\nimport { GoogleGenAI } from \"@google/genai\";\n\nconst ai = new GoogleGenAI({});\n\nconst prompt = \"Panning wide shot of a calico kitten sleeping in the sunshine\";\n\n// Step 1: Generate an image with Nano Banana.\n\nconst imageResponse = await ai.models.generateContent({\n\nmodel: \"gemini-2.5-flash-image\",\n\nprompt: prompt,\n\n});\n\n// Step 2: Generate video with Veo 3.1 using the image.\n\nlet operation = await ai.models.generateVideos({\n\nmodel: \"veo-3.1-generate-preview\",\n\nprompt: prompt,\n\nimage: {\n\nimageBytes: imageResponse.generatedImages[0].image.imageBytes,\n\nmimeType: \"image/png\",\n\n},\n\n});\n\n// Poll the operation status until the video is ready.\n\nwhile (!operation.done) {\n\nconsole.log(\"Waiting for video generation to complete...\")\n\nawait new Promise((resolve) => setTimeout(resolve, 10000));\n\noperation = await ai.operations.getVideosOperation({\n\noperation: operation,\n\n});\n\n\n\n// Download the video.\n\nai.files.download({\n\nfile: operation.response.generatedVideos[0].video,\n\ndownloadPath: \"veo3_with_image_input.mp4\",\n\n});\n\nconsole.log(`Generated video saved to veo3_with_image_input.mp4`);\n\n```... ### Go\n\n```\n\npackage main\n\nimport (\n\n\"context\"\n\n\"log\"\n\n\"os\"\n\n\"time\"\n\n\"google.golang.org/genai\"\n\n\n\nfunc main() {\n\nctx := context.Background()\n\nclient, err := genai.NewClient(ctx, nil)\n\nif err != nil {\n\nlog.Fatal(err)\n\n\n\nprompt := \"Panning wide shot of a calico kitten sleeping in the sunshine\"\n\n// Step 1: Generate an image with Nano Banana.\n\nimageResponse, err := client.Models.GenerateContent(\n\nctx,\n\n\"gemini-2.5-flash-image\",\n\nprompt,\n\nnil, // GenerateImagesConfig\n\n\n\nif err != nil {\n\nlog.Fatal(err)\n\n\n\n// Step 2: Generate video with Veo 3.1 using the image.\n\noperation, err := client.Models.GenerateVideos(\n\nctx,\n\n\"veo-3.1-generate-preview\",\n\nprompt,\n\nimageResponse.GeneratedImages[0].Image,\n\nnil, // GenerateVideosConfig\n\n\n\nif err != nil {\n\nlog.Fatal(err)\n\n\n\n// Poll the operation status until the video is ready.\n\nfor !operation.Done {\n\nlog.Println(\"Waiting for video generation to complete...\")\n\ntime.Sleep(10 * time.Second)\n\noperation, _ = client.Operations.GetVideosOperation(ctx, operation, nil)\n\n\n\n// Download the video.\n\nvideo := operation.Response.GeneratedVideos[0]\n\nclient.Files.Download(ctx, video.Video, nil)\n\nfname := \"veo3_with_image_input.mp4\"\n\n_ = os.WriteFile(fname, video.Video.VideoBytes, 0644)\n\nlog.Printf(\"Generated video saved to %s\\n\", fname)\n\n\n\n```... ## Extending Veo videos\n\nUse Veo 3.1 to extend videos that you previously generated with Veo by 7 seconds and up to 20 times.\n\nInput video limitations:\n\n- Veo-generated videos only up to 141 seconds long.\n\n- Gemini API only supports video extensions for Veo-generated videos.\n\n- Input videos are expected to have a certain length, aspect ratio, and dimensions:\n\n- Aspect ratio: 9:16 or 16:9\n\n- Resolution: 720p\n\n- Video length: 141 seconds or less\n\nThe output of the extension is a single video combining the user input video and the generated extended video for up to 148 seconds of video.\n\nThis example takes the Veo-generated video\n\n*butterfly_video*, shown here with\n\nits original prompt, and extends it using the\n\n`video` parameter and a new\n\nprompt:\n\n|Prompt|Output: `butterfly_video`|\n|--|--|\n|An origami butterfly flaps its wings and flies out of the french doors into the garden.|\n### Python\n\n```\n\nimport time\n\nfrom google import genai\n\nclient = genai.Client()\n\nprompt = \"Track the butterfly into the garden as it lands on an orange origami flower. A fluffy white puppy runs up and gently pats the flower.\"\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nvideo=butterfly_video,\n\nprompt=prompt,\n\nconfig=types.GenerateVideosConfig(\n\nnumber_of_videos=1,\n\nresolution=\"720p\"\n\n),\n\n\n\n# Poll the operation status until the video is ready.\n\nwhile not operation.done:\n\nprint(\"Waiting for video generation to complete...\")\n\ntime.sleep(10)\n\noperation = client.operations.get(operation)\n\n# Download the video.\n\nvideo = operation.response.generated_videos[0]\n\nclient.files.download(file=video.video)\n\nvideo.video.save(\"veo3.1_extension.mp4\")\n\nprint(\"Generated video saved to veo3.1_extension.mp4\")\n\n```\n\nFor information about writing effective text prompts for video generation, see the Veo prompt guide.... ## Model features\n\n|Feature|Description|Veo 3.1 & Veo 3.1 Fast|Veo 3 & Veo 3 Fast|Veo 2|\n|--|--|--|--|--|\n|Audio|Natively generates audio with video.|Natively generates audio with video.|✔️ Always on|❌ Silent only|\n|Input Modalities|The type of input used for generation.|Text-to-Video, Image-to-Video, Video-to-Video|Text-to-Video, Image-to-Video|Text-to-Video, Image-to-Video|\n|Resolution|The output resolution of the video.|720p & 1080p (8s length only) 720p only when using video extension.|720p & 1080p (16:9 only)|720p|\n|Frame Rate|The output frame rate of the video.|24fps|24fps|24fps|\n|Video Duration|Length of the generated video.|8 seconds, 6 seconds, 4 seconds 8 seconds only when using reference images|8 seconds|5-8 seconds|\n|Videos per Request|Number of videos generated per request.|1|1|1 or 2|\n|Status & Details|Model availability and further details.|Preview|Stable|Stable|... ## Model versions\n\nCheck out the Pricing and Rate limits pages for more Veo model-specific usage details.\n\n### Veo 3.1 Preview\n\n|Property|Description|\n|--|--|\n|Model code||\n|Supported data types|Text, Image Video with audio|\n|Limits|1,024 tokens 1|\n|Latest update|September 2025|\n### Veo 3.1 Fast Preview\n\n|Property|Description|\n|--|--|\n|Model code||\n|Supported data types|Text, Image Video with audio|\n|Limits|1,024 tokens 1|\n|Latest update|September 2025|\n### Veo 3\n\n|Property|Description|\n|--|--|\n|Model code||\n|Supported data types|Text, Image Video with audio|\n|Limits|1,024 tokens 1|\n|Latest update|July 2025|\n### Veo 3 FastVeo 3 Fast allows developers to create videos with sound while maintaining high quality and optimizing for speed and business use cases. It's ideal for backend services that programmatically generate ads, tools for rapid A/B testing of creative concepts, or apps that need to quickly produce social media content.\n\n|Property|Description|\n|--|--|\n|Model code||\n|Supported data types|Text, Image Video with audio|\n|Limits|1,024 tokens 1|\n|Latest update|July 2025|\n### Veo 2\n\n|Property|Description|\n|--|--|\n|Model code||\n|Supported data types|Text, image Video|\n|Limits|N/A Any image resolution and aspect ratio up to 20MB file size Up to 2|\n|Latest update|April 2025|",
            "domain": "ai.google.dev"
          },
          {
            "position": 8,
            "title": "Google releases Veo 3.1, adds it to Flow video editor",
            "url": "https://techcrunch.com/2025/10/15/google-releases-veo-3-1-adds-it-to-flow-video-editor/",
            "snippet": "In Brief\nGoogle launched its new video model Veo 3.1 with improved audio output, granular editing controls, and better output for image to video. It said that Veo 3.1 builds on May's Veo 3 release and generates more realistic clips and adheres to prompts better.\nThe model allows users to add an object to the video and have it blend into the clip's style, Google said. Soon, users will be able to remove an existing object from the video in Flow, too.\nVeo 3 already has edit features such as adding reference images to drive a character, providing the first and last frame to generate a clip using AI, and the ability to extend an existing video based on the last few frames. With Veo 3.1, Google is adding audio to all these features to make the clips more lively.\nThe company is rolling out the model to its video editor Flow, the Gemini App, along with Vertex and Gemini APIs. It said that since Flow's launch in May, users have created more than 275 million videos on the app.",
            "domain": "techcrunch.com"
          },
          {
            "position": 9,
            "title": "Ultimate VEO 3.1 Update EXPLAINED: How To Use Google Veo-3 For Beginners",
            "url": "https://www.youtube.com/watch?v=FMdIIBByNZo",
            "snippet": "## AI Master\n##### Jul 09, 2025 (0:19:09)\n🚀 Become an AI Master – All-in-one AI Video Learning https://aimaster.me/pro\nLearn more about Magic Patterns here: https://magicpatterns.1stcollab.com/iamaimaster_3\n\nIn this video I ditch the press-release fluff and put Google’s brand-new Veo 3.1 through its paces, showing you exactly how to turn nothing but text prompts into crisp 8-second clips with native sound. You’ll watch me bounce between Gemini and the pro-grade Flow interface, learn my Prompt-Director Formula for nailing subject, action, context, motion, style, framing & audio, and see three crowd-pleasing demos in action. I’ll also flag the sneaky differences between Veo 3.1 Fast  and full Veo 3.1 Quality, share quick wins like borrowing Midjourney stills for style reference, and spotlight.\n\nChapters:\n0:00 – Why Veo 3 matters\n0:41 Flow vs Gemini\n2:11 Example #1\n4:23 The VEO3 Prompt Formula\n11:25 Example #2\n15:26 Example #3\n17:38 Wrapping up... {ts:0} Hey guys, I need a photo of AI master. Do you know who is AI master?\n{ts:5} Make Death Star great again. Okay. Oh, today I'm pulling back the curtain on\n{ts:10} Google's new VO3, the next gen AI video model that's got me acting like a kid in a tech toy store. We're talking text to\n{ts:18} video. You type a description and VO3 spits out an 8second video clip with sound. Yes, audio and video from just\n{ts:26} your words. [Music] In this tutorial, I'll show you how to\n{ts:31} harness VO3 to create funny viral style videos like this. It's just a few toasters, right? What's\n{ts:38} the worst that could happen? The big deal here is native audio. Unlike earlier models, VO3 doesn't just\n{ts:46} create silent clips. It also produces sound effects, background noise, even character dialogue to match. Each clip\n{ts:52} is limited to about 8 seconds for now, so we're talking short form content. They left behind a a ball today. It\n{ts:60} bounced higher than I can jump. What manner of magic is that? Now, Google gives us two main ways to... {ts:67} use V3. Flow and Gemini. Think of them as two different cameras that use the same film. Google Flow is like a pro\n{ts:75} filmmaking tool built around V3. It's a dedicated interface currently on Google apps where you can storyboard scenes,\n{ts:82} manage assets, tweak camera moves, the works. Flow was literally customd designed for VO. Lets you do fancy\n{ts:89} things. Upload or generate ingredients like character images via image genen and reuse them in multiple shots for\n{ts:96} consistency. Control camera angles and motion paths manually and even uses scene builder to extend the clips\n{ts:102} seamlessly into the next part of the story. It's basically an AI video studio for filmmakers. Now, Flow isn't open to\n{ts:109} everyone by default. It's available if you have a Google AI Pro or Ultra subscription in certain regions. Pro\n{ts:116} subscribers get most flow features, but only the ultra tier unlocks V3 fully. If you are on Pro, you have something\n{ts:123} called VO3 fast speed optimized version with sound slightly lower quality. I'll talk about that in a sec.\n{ts:132} All right, this idea cracks me up. A grandma skydiving into the Super Bowl. Talk about a stun granny. Let... {ts:276} I always try to include. Subject, who or what is in the scene, action, what the subject is doing. Context, setting,\n{ts:284} where and when is this happening. Motion, camera and movement, how the scene is shot and moves. Style, the\n{ts:291} visual style or genre. framing, composition, how the shot is framed, and constraints, extras, any additional\n{ts:298} details or instructions. And don't forget audio. VO3 will generate sound to match your scene, but only if you tell\n{ts:305} it what you want to hear. So, I often append an audio double colon section in the prompt describing sound effects,\n{ts:312} background noise, or dialogue lines. For example, audio double colon crowd cheering, upbeat rock music playing\n{ts:319} faintly, or character says hello world. You can specify dialogue explicitly, word for word, or implicitly, like a man\n{ts:327} introduces himself and let the AI make up the words. If you ask for dialogue, add no subtitles to your prompt.\n{ts:334} Otherwise, the model might plaster autogenerated subtitles on the video, which look well, not great. We... 'll see\n{ts:340} that in action soon. A boring prompt like a man answers a phone might yield a bland clip. But a richly detailed prompt\n{ts:348} can produce something cinematic. For instance, check this out. Basic prompt. A man answers a rotary phone.\n{ts:356} Hello. And detailed prompt. A shaky dolly zoom goes from a faraway\n{ts:364} blur to a close-up cinematic shot of a desperate man in a weathered green trench coat as he picks up a rotary\n{ts:370} phone mounted on a gritty brick wall. Bathed in the eerie glow of a green neon sign. The zoom reveals the tension on\n{ts:377} his face as he struggles to speak. shallow depth of field keeps focus on his furrowed brow and the phone, while\n{ts:384} the background is a blur of neon colors and shadows, creating a sense of urgency and isolation.\n{ts:392} I I have to Which one would you rather watch? The\n{ts:400} second prompt, nail subject, action, context, motion, that dolly zoom, style, cinematic, neon, noir, vibes, framing,\n{ts:408} close-up, ambience, eerie, neon, glow, all the elements. VO3 will follow the detailed prompt much more faithfully... {ts:416} than a vague one, yielding a dramatic video instead of a random guy on a phone. Bottom line, be specific and\n{ts:423} vivid. Describe what the camera sees and what the mic hears. Now, coming up with such detailed prompt can feel like\n{ts:429} flexing a new muscle. Here's where I cheat a little. I use Chad GBT to help refine prompt. Sometimes I'll start with\n{ts:436} a simple idea, say grandma's skydiving into stadium, and ask Chad GBT to describe the scene in a funny cinematic\n{ts:445} way with sights and sounds. The AI will usually output a nicely embellished description that I can copy or tweak for\n{ts:452} VO. Similarly, I use Smidjourney for visual prototyping. I'll feed a quick version of my idea to Midjourney to get\n{ts:459} a still image that helps me visualize the color palette or style I want. Maybe I like how it looks as a 1980s cartoon\n{ts:467} versus a gritty realistic photo and I can then describe that style in my VO prompt. This extra step isn't required,\n{ts:476} but if you... {ts:509} cheering. stadium announcer muffled wind rushing and the grandma's voice yelling yahoo in excitement no subtitles\n{ts:517} and that covers subject grandma action skydiving context super stadium motion wide follow shot slow-mo style cinematic\n{ts:527} bright realistic and audio crowd wind voice all in one prompt paragraph it's lengthy but should guide V3 perfectly a\n{ts:536} moment of truth let's generate this and see What happens? Yahoo!\n{ts:545} [Applause] Wow, look at that. She's really skydiving into the stadium. The video\n{ts:554} shows this tiny parachute coming down over a huge football field just like we asked. There's a wide shot of the\n{ts:561} stadium with fans standing up. I can see the crowd detail and yes, I hear the cheering roar. The grandma is visible\n{ts:567} hanging from the parachute. Maybe not super close on her face because we chose a wide shot, but enough to tell she's an\n{ts:574} elderly lady in gear. B3 as well as other generators are undoubtedly great, but they all can do\n{ts:582} one thing. Generate complete visual project. Sure, they can do few pictures and a few seconds of video, but what... 't resist add an explosion or two like a car blowing up\n{ts:998} behind them. Audio obviously loud audio double colin roaring T-Rex cowboy whooping car tires screeching an\n{ts:1008} explosion sound and dramatic action music. I will format it as one prompt and run it through Gemini. And here it\n{ts:1016} comes. We've got a T-Rex barreling down what looks like Time Square. A cowboy on his\n{ts:1029} back waving his head. And yes, there is an actual explosion behind them. Looks like a car got tossed. The fact that VO3",
            "domain": "www.youtube.com"
          },
          {
            "position": 10,
            "title": "Gemini AI video generator powered by Veo 3.1",
            "url": "https://gemini.google/overview/video-generation/",
            "snippet": "# Break the\n\n**silence** with Veo 3.1\n\nCreate high-quality, 8-second videos with Veo 3.1, our latest AI video generator. Simply describe what you have in mind or upload a photo and watch your ideas come to life with native audio generation. Try it with a Google AI Pro plan or get the highest access with the Ultra plan.\n\n**Veo 3.1** speaks for itself\n\n## Dream it. Describe it.\n\n**Done.**\n\n## For Exploring\n\nPlay with diverse styles, bring animated characters to life, and combine objects in ways you never thought possible. See what you can create using text to video with AI.\n\n## For Sharing\n\nCreate funny memes, turn inside jokes into videos, re-imagine special moments, and add a personal touch to make someone smile.\n\n## For Brainstorming\n\nBreak through creative blocks and visualize your ideas in a flash. From product concepts and designs to rapid prototyping and storytelling, Gemini can help.\n\n## Learn more about our\n\n**Veo Models**\n\nCreate videos with sound using our video generation model that maintains high-quality while optimizing for speed.\n\nCreate high-quality, 8-second videos with sound using our state-of-the-art video generation model.... ## Frequently asked questions\n\nYes, you can create and share videos in your mobile Gemini app. To create videos, tap the \"video\" button in your prompt bar. If you don't see it, tap the button with three dots to view more options.\n\nTry Veo 3.1 Fast with a Google AI Pro plan or get the highest access to Veo 3.1 in Google AI Ultra. Country availability here.\n\nFor now, the ability to generate a video from a photo is not available in the European Economic Area, Switzerland, or the United Kingdom.\n\nWe’ve taken several important safety steps to make AI video generation a safe experience. This includes extensive red teaming and evaluation aimed at preventing the generation of content that violates our policies. Additionally, all videos generated with Veo in the Gemini app are marked with a visible watermark and SynthID, a digital watermark embedded in each frame, which indicates the videos are AI-generated.\n\nGemini's outputs are primarily determined by user prompts and like any generative AI tool, there may be instances where it generates content that some individuals find objectionable. We’ll continue to listen to your feedback through the thumbs up/down buttons and make ongoing improvements. For more details, you can read about our approach on our website.\n\nResults for illustrative purposes and may vary. Internet and subscription for certain features required. Available to users 18+. Create responsibly.",
            "domain": "gemini.google"
          }
        ],
        "success": true,
        "error": null
      },
      {
        "query_id": "q5",
        "query": "veo3.1 Google software version 2025",
        "claim_id": "claim_1",
        "query_type": "direct_fact",
        "priority": "high",
        "results": [
          {
            "position": 1,
            "title": "Introducing Veo 3.1 and advanced capabilities in Flow - Google Blog",
            "url": "https://blog.google/technology/ai/veo-updates-flow/",
            "snippet": "# Introducing Veo 3.1 and advanced capabilities in Flow\n\nFive months ago, we introduced Flow, our AI filmmaking tool powered by Veo, and have been inspired by the creativity it has sparked with over 275 million videos generated in Flow\n\n1\n\n. We're always listening to your feedback, and we've heard that you want more artistic control within Flow, with increased support for audio across all features.\n\nToday, we’re introducing new and enhanced creative capabilities to edit your clips, giving you more granular control over your final scene. For the first time, we’re also bringing audio to existing capabilities like “Ingredients to Video,” “Frames to Video” and “Extend.”\n\nWe’re also introducing Veo 3.1, which brings richer audio, more narrative control, and enhanced realism that captures true-to-life textures. Veo 3.1 is state-of-the-art and builds on Veo 3, with stronger prompt adherence and improved audiovisual quality when turning images into videos.\n\n## Refine your narrative with audio and more control\n\nWith Veo 3.1, we’re bringing audio to existing capabilities to help you craft the perfect scene. These features are experimental and actively improving, and we’re excited to see what you create as we iterate based on your feedback.\n\nNow, with rich, generated audio, you can:\n\n**Craft the look of your scene.**With \"Ingredients to Video,\" you can use multiple reference images to control the characters, objects and style. Flow uses your ingredients to create a final scene that looks just as you envisioned. **Control the shot from start to finish.**Provide a starting and ending image with “Frames to Video,” and Flow will generate a seamless video that bridges the two, perfect for artful and epic transitions. **Create longer, seamless shots.**With \"Extend,\" you can create longer videos, even lasting for a minute or more, that connect to and continue the action from your original clip. Each video is generated based on the final second of your previous clip, making it most useful for creating a longer establishing shot.... ## Edit your ingredients and videos with more precision\n\nGreat ideas can strike at any point in the creative process. For moments when the first take isn't the final one, we're introducing new editing capabilities directly within Flow to help you reimagine and perfect your scenes.\n\n**Add new elements to any scene.**With “Insert,” introduce anything you can imagine, from realistic details to fantastical creatures. Flow now handles complex details like shadows and scene lighting, making the addition look natural. **Remove unwanted objects or characters seamlessly.**Soon, you’ll be able to take anything out of a scene, and Flow will reconstruct the background and surroundings, making it look as though the object was never there.\n\n## Start creating in Flow today\n\nWith more precise editing capabilities, audio across all existing features and higher-quality outputs powered by Veo 3.1, we're opening up new possibilities for richer, more powerful video storytelling right inside Flow.\n\nThe Veo 3.1 model is also available via the Gemini API for developers, Vertex AI for enterprise customers, and the Gemini app. New capabilities are available in both Gemini API\n\n2\n\nand Vertex AI\n\n3",
            "domain": "blog.google"
          },
          {
            "position": 2,
            "title": "Build with Veo 3, now available in the Gemini API",
            "url": "https://developers.googleblog.com/en/veo-3-now-available-gemini-api/",
            "snippet": "First unveiled at Google I/O 2025, people around the world have already generated tens of millions of high-quality videos with Veo 3 (along with some new fun and interesting video trends). It is our first video model to incorporate high-fidelity video outputs and native audio, first with text-to-video and soon with image-to-video.\n\nDevelopers are already experimenting with Veo 3, discovering how the model can help them brainstorm content, rapidly iterate, and be more efficient.\n\nVeo 3 is designed to handle a range of video generation tasks, from cinematic narratives to dynamic character animations. With Veo 3, you can create more immersive experiences by not only generating stunning visuals, but also audio like dialogue and sound effects.\n\nLet’s take a look at some examples.\n\nExplore these examples and more with Veo 3 in Google AI Studio, available as an SDK template and interactive Starter App to remix, copy and extend. The Starter App and its sample code offer a convenient way for Paid Tier users to rapidly prototype with Veo 3 and more on the Gemini API, directly from Google AI Studio.\n\nClick the Key button in the top right of the AI Studio Build interface to select a Google Cloud Project with billing enabled to use the Paid Tier in AI Studio apps. See the FAQs for more.... Veo 3 will be priced at $0.75 per second for video and audio output. Additionally, Veo 3 Fast will be available soon, offering a faster and more cost-effective option for video creation.\n\nHere’s a basic Python example to create a video:\n\n```\n\nimport time\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.0-generate-preview\",\n\nprompt=\"a close-up shot of a golden retriever playing in a field of sunflowers\",\n\nconfig=types.GenerateVideosConfig(\n\nnegative_prompt=\"barking, woofing\",\n\n),\n\n\n\n# Waiting for the video(s) to be generated\n\nwhile not operation.done:\n\ntime.sleep(20)\n\noperation = client.operations.get(operation)\n\ngenerated_video = operation.result.generated_videos[0]\n\nclient.files.download(file=generated_video.video)\n\ngenerated_video.video.save(\"veo3_video.mp4\")\n\n```\n\nAll videos generated by Veo 3 models will continue to include a digital SynthID watermark. To get started, check out the documentation, cookbook, and a Veo 3 starter app in Google AI Studio:\n\nIn addition to being available via the Gemini API in Google AI Studio, Veo 3 is also available to Google AI subscribers in the Gemini app and Flow, and to enterprise customers via Vertex AI.... 1: Fluffy Characters Stop Motion: Inside a brightly colored, cozy kitchen made of felt and yarn. Professor Nibbles, a plump, fluffy hamster with oversized glasses, nervously stirs a bubbling pot on a miniature stove, muttering, \"Just a little more... 'essence of savory,' as the recipe calls for.\" The camera is a mid-shot, capturing his frantic stirring. Suddenly, the pot emits a loud \"POP!\" followed by a comical \"whoosh\" sound, and a geyser of iridescent green slime erupts, covering the entire kitchen. Professor Nibbles shrieks, \"Oh, dear! Not again!\" and scurries away, leaving a trail of tiny, panicked squeaks.\n\n2: The sequence begins with an extreme close-up of a single gear, slowly turning and reflecting harsh sunlight. The camera gradually pulls back in a continuous movement, revealing this is but one component of a colossal, mechanical heart half-buried in a desolate, rust-colored desert. A sweeping aerial shot establishes its enormous scale and isolation in the barren landscape. The camera descends to capture pipes hissing steam and the rhythmic thumping that echoes across the empty plains. A subtle shake effect synchronizes with each massive heartbeat. A lateral tracking shot discovers tiny, robed figures scurrying across the metallic surface. The camera follows one such figure in a detailed tracking shot as they perform meticulous maintenance, polishing brass valves and tightening immense bolts. A complex movement circles the entire structure, capturing different maintenance teams working in precarious positions across its rusted exterior. The final shot begins tight on the meticulous work of one tiny figure before executing a dramatic pull-out that reveals the true scale of the heart and the minuscule size of its caretakers, tending to the vital organ of an unseen, sleeping giant that extends beyond the frame.",
            "domain": "developers.googleblog.com"
          },
          {
            "position": 3,
            "title": "Google announces Veo 3.1 and updates Flow with more controls, tools",
            "url": "https://9to5google.com/2025/10/15/veo-3-1/",
            "snippet": "Google today announced Veo 3.1 as its latest video generation model, with Flow getting a number of updates to take advantage of the latest capabilities.\n\nCompared to Veo 3, which was announced at I/O 2025 in May, this new version offers richer audio and “enhanced realism that captures true-to-life textures.” Veo 3.1 has a deeper understanding of storytelling, cinematic styles, and character interactions to give you more narrative control. The image-to-video capability benefits from improved audio-visual quality and better follows your prompt.\n\nVeo 3.1 and Veo 3.1 Fast are available in the Gemini app, as well as the Gemini API and Vertex AI, to power text-to-video and image-to-video for horizontal (16×9) and vertical (9×16) outputs.\n\nMeanwhile, Google is updating the Flow filmmaking tool to take advantage of Veo 3.1. Audio generation is coming to:\n\n**Ingredients to Video**: This can include images of multiple characters, objects, and styles **Scene extension**: You can Extend an existing clip, with Google basing it on the last second **Frames to Video**: This lets you specify the starting and ending images, with Flow bridging the two\n\nFlow is getting new editing capabilities. You can insert elements like objects, characters, and details, with Google handling shadows, scene lighting, and other complex details to make everything look natural.\n\nComing soon is the ability to remove objects and characters from a scene, with Flow working to reconstruct the background and surroundings to make the edit seamless.\n\n\n\n## Comments",
            "domain": "9to5google.com"
          },
          {
            "position": 4,
            "title": "Introducing Veo 3.1 and new creative capabilities in the Gemini API",
            "url": "https://developers.googleblog.com/en/introducing-veo-3-1-and-new-creative-capabilities-in-the-gemini-api/",
            "snippet": "**Today, we are releasing Veo 3.1 and Veo 3.1 Fast in paid preview in the** **Gemini API** **.** This updated model offers several improvements, as well as improved outputs when generating video from images. These new models are available via the Gemini API in Google AI Studio and Vertex AI. Veo 3.1 is also available in the Gemini app and Flow.\n\nVeo 3.1 and Veo 3.1 Fast empower developers to create more engaging content through significant upgrades. The models now generate\n\n**richer native audio**, from natural conversations to synchronized sound effects, and offer **greater narrative control** with an improved understanding of cinematic styles. **Enhanced image-to-video** capabilities ensure better prompt adherence while delivering superior audio and visual quality and maintaining character consistency across multiple scenes.\n\nIn addition to the model update, we are introducing new model capabilities, including using reference images to guide video generation, extend existing Veo videos, and generate transitions between a first and last frame.\n\nYou can now guide the generation process by providing up to 3 reference images of a character, object, or scene. This is helpful for maintaining character consistency across multiple shots or applying a specific style to your video.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nconfig=types.GenerateVideosConfig(\n\nreference_images=[reference_image1, reference_image2],\n\n),\n\n\n\n```... Your story is no longer limited by the original generation. With Scene extension, you can create longer videos, even lasting for a minute or more, by generating new clips that connect to your previous video. Each new video is generated based on the final second of the previous clip. This maintains visual continuity, making the feature ideal for extending shots with background audio.\n\n```\n\nfrom google import genai\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nvideo=video_to_extend\n\n\n\n```\n\nCreate smooth, natural scenes that bridge two different images. By providing a starting and an ending image, you can direct Veo 3.1 to generate the transition between them, complete with accompanying audio.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nimage=first_frame,\n\nconfig=types.GenerateVideosConfig(\n\nlast_frame=last_frame,\n\n),\n\n\n\n```\n\n**What people are building with Veo 3.1**\n\nPromise Studios, a GenAI movie studio, uses Veo 3.1 within its MUSE Platform to enhance generative storyboarding and previsualization for director-driven storytelling at production quality.... Latitude is experimenting with Veo 3.1 in its generative narrative engine to instantly bring user-created stories to life.\n\nVeo 3.1 and these new capabilities are now available in paid preview via the Gemini API.\n\nVeo 3.1 is the same price as Veo 3. We are incredibly excited to see what you will build with these new capabilities!",
            "domain": "developers.googleblog.com"
          },
          {
            "position": 5,
            "title": "Google's Veo 3.1 is better at generating videos from images",
            "url": "https://www.engadget.com/ai/googles-veo-31-is-better-at-generating-videos-from-images-220829129.html",
            "snippet": "Google has released a new update to its Veo AI video generation model that should make it do a better job of sticking to prompts and converting images into videos. Veo 3.1 is available to try today through Google's Gemini API and is now also powering the company's Flow video editor.\nVeo 3.1 builds on the new capabilities Google introduced with launch of Veo 3 at Google I/O 2025. The new model offers better \"prompt adherence,\" according to Google, and should have an easier time creating videos based on the image \"ingredients\" you upload alongside your written prompt. Veo 3.1 also makes it possible to convert images to video and generate audio at the same time, a capability that wasn't available with Veo 3.\nIn Flow, Veo 3.1 supports at least a new feature that gives you finer control over the videos you generate. With what Google calls \"Frame to Video,\" Flow lets you upload a first and last frame, and then generates the video in-between. Adobe Firefly, which is powered by Veo 3, offers a similar feature, but Flow will be able to pull it off and create audio at the same time. Those added audio skills will also apply to the video editor's ability to extend clips and insert objects into existing footage, too.\nBased on the samples Google's shared, videos generated with Veo 3.1 still have an uncanny quality that seems to vary greatly depending on the prompt and subject. Even if it's missing some of the realism of OpenAI's Sora 2, though, the company's decision to try and make Veo more useful to people who actually work with video rather than a source of social media spam is a welcome move.",
            "domain": "www.engadget.com"
          },
          {
            "position": 6,
            "title": "Release notes | Gemini API | Google AI for Developers",
            "url": "https://ai.google.dev/gemini-api/docs/changelog",
            "snippet": "This page documents updates to the Gemini API.\n\n## October 17, 2025\n\n**Grounding with Google Maps**is now generally available. For more information, see Grounding with Google Maps documentation.\n\n## October 15, 2025\n\nReleased Veo 3.1 and 3.1 Fast models in public preview, with new features including:\n\n- Extending Veo-created videos.\n\n- Referencing up to three images to generate a video.\n\n- Providing first and last frame images to generate videos from.\n\nThis launch also added more options for Veo 3 output video durations: 4, 6, and 8 seconds.\n\nDeprecation for\n\n`veo-3.0-generate-preview`and\n\n`veo-3.0-fast-generate-preview`coming October 22, 2025.\n\n## October 7, 2025\n\n- Launched Gemini 2.5 Computer Use Preview\n\n## October 2, 2025\n\n- Launched Gemini 2.5 Flash Image GA: Image Generation with Gemini\n\n## September 29, 2025\n\n- The following Gemini 1.5 models are now deprecated:\n\n`gemini-1.5-pro`\n\n`gemini-1.5-flash-8b`\n\n`gemini-1.5-flash`... ## September 9, 2025\n\n- Launched Veo 3 and Veo 3 Fast GA, with lower pricing and new options for aspect ratios, resolution, and seeding. Read the Veo documentation for more information.\n\n## August 26, 2025\n\n- Launched Gemini 2.5 Image Preview, our latest native image generation model.\n\n## August 18, 2025\n\n- Released URL context tool to general\n\navailability (GA), a tool for providing URLs as additional context to\n\nprompts. Support for using URL context with the\n\n`gemini-2.0-flash`model (available during experimental release) will be discontinued in one week.\n\n## August 14, 2025\n\n- Released Imagen 4 Ultra, Standard and Fast models as generally available (GA). To learn more, see the Imagen page.\n\n## August 7, 2025\n\n`allow_adult`setting in Image to Video generation are now available in restricted regions. See the Veo page for details.\n\n## July 31, 2025\n\n- Launched image-to-video generation for the Veo 3 Preview model.\n\n- Released Veo 3 Fast Preview model.\n\n- To learn more about Veo 3, visit the Veo page.... ## July 22, 2025\n\n- Released\n\n`gemini-2.5-flash-lite`, our fast, low-cost, high-performance Gemini 2.5 model. To learn more, see Gemini 2.5 Flash-Lite.\n\n## July 17, 2025\n\nLaunched\n\n`veo-3.0-generate-preview`, the latest update to Veo introducing video with audio generation. To learn more about Veo 3, visit the Veo page.\n\nIncreased rate limits for Imagen 4 Standard and Ultra. Visit the Rate limits page for more details.\n\n## July 14, 2025\n\n- Released\n\n`gemini-embedding-001`, the stable version of our text embedding model. To learn more, see embeddings. The\n\n`gemini-embedding-exp-03-07`model will be deprecated on August 14, 2025.\n\n## July 7, 2025\n\n- Launched Gemini API Batch Mode. Batch up requests and send them to process asynchronously. To learn more, see Batch Mode.\n\n## June 26, 2025\n\nThe preview models\n\n`gemini-2.5-pro-preview-05-06`and\n\n`gemini-2.5-pro-preview-03-25`are now redirecting to the latest stable version\n\n`gemini-2.5-pro`.\n\n`gemini-2.5-pro-exp-03-25`is deprecated.... ## May 7, 2025\n\n- Released\n\n`gemini-2.0-flash-preview-image-generation`, a preview model for generating and editing images. To learn more, see Image generation and Gemini 2.0 Flash Preview Image Generation.\n\n## May 6, 2025\n\n- Released\n\n`gemini-2.5-pro-preview-05-06`, a new version of our most powerful model, with improvements on code and function calling.\n\n`gemini-2.5-pro-preview-03-25`will automatically point to the new version of the model.\n\n## April 17, 2025\n\n- Released\n\n`gemini-2.5-flash-preview-04-17`, a Gemini preview model optimized for price-performance and adaptive thinking. To learn more, see Gemini 2.5 Flash Preview and Thinking.\n\n## April 16, 2025\n\n- Launched context caching for Gemini 2.0 Flash.... ## April 9, 2025\n\n**Model updates:**\n\n- Released\n\n`veo-2.0-generate-001`, a generally available (GA) text- and image-to-video model, capable of generating detailed and artistically nuanced videos. To learn more, see the Veo docs.\n\nReleased\n\n`gemini-2.0-flash-live-001`, a public preview version of the Live API model with billing enabled.\n\n**Enhanced Session Management and Reliability** **Session Resumption:**Keep sessions alive across temporary network disruptions. The API now supports server-side session state storage (for up to 24 hours) and provides handles (session_resumption) to reconnect and resume where you left off. **Longer Sessions via Context Compression:**Enable extended interactions beyond previous time limits. Configure context window compression with a sliding window mechanism to automatically manage context length, preventing abrupt terminations due to context limits. **Graceful Disconnect Notification:**Receive a\n\n`GoAway`server message indicating when a connection is about to close, allowing for graceful handling before termination.\n\n\n\n**More Control over Interaction Dynamics** **Configurable Voice Activity Detection (VAD):**Choose sensitivity levels or disable automatic VAD entirely and use new client events (\n\n`activityStart`,\n\n`activityEnd`) for manual turn control.\n\n**Configurable Interruption Handling:**Decide whether user input should interrupt the model's response. **Configurable Turn Coverage:**Choose whether the API processes all audio and video input continuously or only captures it when the end-user is detected speaking. **Configurable Media Resolution:**Optimize for quality or token usage by selecting the resolution for input media. **Richer Output and Features** **Expanded Voice & Language Options:**Choose from two new voices and 30 new languages for audio output. The output language is now configurable within\n\n`speechConfig`.\n\n**Text Streaming:**Receive text responses incrementally as they are generated, enabling faster display to the user. **Token Usage Reporting:**Gain insights into usage with detailed token counts provided in the\n\n`usageMetadata`field of server messages, broken down by modality and prompt or response phases.... ## April 4, 2025\n\n- Released\n\n`gemini-2.5-pro-preview-03-25`, a public preview Gemini 2.5 Pro version with billing enabled. You can continue to use\n\n`gemini-2.5-pro-exp-03-25`on the free tier.\n\n## March 25, 2025\n\n- Released\n\n`gemini-2.5-pro-exp-03-25`, a public experimental Gemini model with thinking mode always on by default. To learn more, see Gemini 2.5 Pro Experimental.\n\n## March 12, 2025\n\n**Model updates:**\n\n- Launched an experimental Gemini 2.0 Flash model capable of image generation and editing.\n\n- Released\n\n`gemma-3-27b-it`, available on AI Studio and through the Gemini API, as part of the Gemma 3 launch.\n\n**API updates:**\n\n- Added support for YouTube URLs as a media source.\n\n- Added support for including an inline video of less than 20MB.\n\n## March 11, 2025\n\n**SDK updates:**\n\n- Released the Google Gen AI SDK for TypeScript and JavaScript to public preview.\n\n## March 7, 2025\n\n**Model updates:**\n\n- Released\n\n`gemini-embedding-exp-03-07`, an experimental Gemini-based embeddings model in public preview.... ## February 28, 2025\n\n**API updates:**\n\n- Support for Search as a tool\n\nadded to\n\n`gemini-2.0-pro-exp-02-05`, an experimental model based on Gemini 2.0 Pro.\n\n## February 25, 2025\n\n**Model updates:**\n\n- Released\n\n`gemini-2.0-flash-lite`, a generally available (GA) version of Gemini 2.0 Flash-Lite, which is optimized for speed, scale, and cost efficiency.\n\n## February 19, 2025\n\n**AI Studio updates:**\n\n- Support for additional regions (Kosovo, Greenland and Faroe Islands).\n\n**API updates:**\n\n- Support for additional regions (Kosovo, Greenland and Faroe Islands).\n\n## February 18, 2025\n\n**Model updates:**\n\n- Gemini 1.0 Pro is no longer supported. For the list of supported models, see Gemini models.\n\n## February 11, 2025\n\n**API updates:**\n\n- Updates on the OpenAI libraries compatibility.\n\n## February 6, 2025\n\n**Model updates:**\n\n- Released\n\n`imagen-3.0-generate-002`, a generally available (GA) version of Imagen 3 in the Gemini API.\n\n**SDK updates:**\n\n- Released the Google Gen AI SDK for Java for public preview.",
            "domain": "ai.google.dev"
          },
          {
            "position": 7,
            "title": "Google releases Veo 3.1, adds it to Flow video editor",
            "url": "https://techcrunch.com/2025/10/15/google-releases-veo-3-1-adds-it-to-flow-video-editor/",
            "snippet": "In Brief\nGoogle launched its new video model Veo 3.1 with improved audio output, granular editing controls, and better output for image to video. It said that Veo 3.1 builds on May's Veo 3 release and generates more realistic clips and adheres to prompts better.\nThe model allows users to add an object to the video and have it blend into the clip's style, Google said. Soon, users will be able to remove an existing object from the video in Flow, too.\nVeo 3 already has edit features such as adding reference images to drive a character, providing the first and last frame to generate a clip using AI, and the ability to extend an existing video based on the last few frames. With Veo 3.1, Google is adding audio to all these features to make the clips more lively.\nThe company is rolling out the model to its video editor Flow, the Gemini App, along with Vertex and Gemini APIs. It said that since Flow's launch in May, users have created more than 275 million videos on the app.",
            "domain": "techcrunch.com"
          },
          {
            "position": 8,
            "title": "Meet Flow: AI-powered filmmaking with Veo 3",
            "url": "https://blog.google/technology/ai/google-flow-veo-ai-filmmaking-tool/",
            "snippet": "# Meet Flow: AI-powered filmmaking with Veo 3\n\nToday we’re introducing Flow, our new AI filmmaking tool.\n\nIt’s built by and for creatives, and it’s the only AI filmmaking tool custom-designed for Google’s most advanced models — Veo, Imagen and Gemini. Flow can help storytellers explore their ideas without bounds and create cinematic clips and scenes for their stories. It’s early days, and we’re excited to shape the future of Flow with creatives and filmmakers.\n\n### What’s possible with Flow\n\nFlow is inspired by what it feels like when time slows down and creation is effortless, iterative and full of possibility. It’s custom-designed for Veo, Google’s state-of-the-art generative video model, with exceptional prompt adherence and stunning cinematic outputs that excel at physics and realism. Behind the scenes, Gemini models make prompting intuitive, so you can describe your vision in everyday language. You can bring your own assets to create characters, or use Flow to make your own ingredients with Imagen’s text-to-image capabilities.\n\nOnce you’ve created a subject or a scene, you can integrate those same ingredients into different clips and scenes with consistency. Or you can use a scene image to start a new shot.\n\nCreate your ingredients\n\nUse those ingredients to create a clip\n\nReference ingredients in plain language... ### Key features to unlock your storytelling\n\nFlow also comes with a range of features for professionals or those just getting started:\n\n**Camera Controls:**Master your shot with direct control over camera motion, angles and perspectives. **Scenebuilder:**Seamlessly edit and extend your existing shots — revealing more of the action or transitioning to what happens next with continuous motion and consistent characters. **Asset Management:**Easily manage and organize all of your ingredients and prompts. **Flow TV** **:**Spark your creativity with an ever-growing showcase of clips, channels, and content generated with Veo. You can see the exact prompts and techniques used for clips you like, providing a practical way to learn and adapt new styles.\n\nSeamless transitions\n\nCamera controls\n\nCinematic quality\n\n### Get started with Flow\n\nFlow is the evolution of VideoFX, a Google Labs experiment that launched last year. Starting today, Flow is available to subscribers of our Google AI Pro and Google AI Ultra plans in the U.S., with more countries coming soon.\n\nGoogle AI Pro gives you the key Flow features and 100 generations per month, and Google AI Ultra gives you the highest usage limits and early access to Veo 3 with native audio generation, bringing environmental sounds and character dialogue directly into video creation.... ### How we’re collaborating with filmmakers\n\nAs with any groundbreaking technology, we’re still understanding the full potential of AI in filmmaking. We see the emergence of these tools as an enabler, helping a new wave of filmmakers more easily tell their stories. By offering filmmakers early access to Flow, we were able to better understand how our technology could best support and integrate into their creative workflows — and we’ve woven their insights into Flow. Here are some filmmakers we partnered with and the short films they developed using Flow along with other tools and techniques.\n\n**Dave Clark**\n\nDave is an award-winning filmmaker focused on embracing new technology as part of his filmmaking. He used AI to develop two of his most recent short films, “Battalion” and “NinjaPunk.” His newest short film “Freelancers” uses Google’s AI and other tools to tell the story of two estranged adopted brothers on similar quests.\n\n**Henry Daubrez**\n\nHenry has been using tech tools in his art for the last 18 years. Earlier this year he unveiled “Kitsune” using Veo 2 — a moving short film about “love between two souls separated by everything except their shared feelings of loneliness.” Now, Henry is bringing the story of his own creative journey to life in “Electric Pink.”\n\n**Junie Lau**\n\nJunie Lau is a film director and multidisciplinary creative deeply passionate about innovation, viewing AI as a vital collaborator in expanding the boundaries of creative expression. Her work delves into artistic narratives within the hyper-modern era, including themes of virtual identity, digital humanities and digital ontology. Currently, Junie is working on a film titled “Dear Stranger,” which explores the boundless and infinite nature of universal love between a grandmother and grandchild across countless parallel worlds.\n\nAI is ushering in a new chapter of creativity and filmmaking, and while it’s still early, we see so much potential for tools like Flow to unlock new voices and creations.\n\nFor more insights on Flow and how AI helps storytellers take more risks in the creative storytelling process, watch Dave Clark, Junie Lau, and Henry Daubrez in \"Behind the Lens: AI, Creativity, and the Future of Filmmaking Tools.\"",
            "domain": "blog.google"
          },
          {
            "position": 9,
            "title": "Gemini AI video generator powered by Veo 3.1",
            "url": "https://gemini.google/overview/video-generation/",
            "snippet": "# Break the\n\n**silence** with Veo 3.1\n\nCreate high-quality, 8-second videos with Veo 3.1, our latest AI video generator. Simply describe what you have in mind or upload a photo and watch your ideas come to life with native audio generation. Try it with a Google AI Pro plan or get the highest access with the Ultra plan.\n\n**Veo 3.1** speaks for itself\n\n## Dream it. Describe it.\n\n**Done.**\n\n## For Exploring\n\nPlay with diverse styles, bring animated characters to life, and combine objects in ways you never thought possible. See what you can create using text to video with AI.\n\n## For Sharing\n\nCreate funny memes, turn inside jokes into videos, re-imagine special moments, and add a personal touch to make someone smile.\n\n## For Brainstorming\n\nBreak through creative blocks and visualize your ideas in a flash. From product concepts and designs to rapid prototyping and storytelling, Gemini can help.\n\n## Learn more about our\n\n**Veo Models**\n\nCreate videos with sound using our video generation model that maintains high-quality while optimizing for speed.\n\nCreate high-quality, 8-second videos with sound using our state-of-the-art video generation model.... ## Frequently asked questions\n\nYes, you can create and share videos in your mobile Gemini app. To create videos, tap the \"video\" button in your prompt bar. If you don't see it, tap the button with three dots to view more options.\n\nTry Veo 3.1 Fast with a Google AI Pro plan or get the highest access to Veo 3.1 in Google AI Ultra. Country availability here.\n\nFor now, the ability to generate a video from a photo is not available in the European Economic Area, Switzerland, or the United Kingdom.\n\nWe’ve taken several important safety steps to make AI video generation a safe experience. This includes extensive red teaming and evaluation aimed at preventing the generation of content that violates our policies. Additionally, all videos generated with Veo in the Gemini app are marked with a visible watermark and SynthID, a digital watermark embedded in each frame, which indicates the videos are AI-generated.\n\nGemini's outputs are primarily determined by user prompts and like any generative AI tool, there may be instances where it generates content that some individuals find objectionable. We’ll continue to listen to your feedback through the thumbs up/down buttons and make ongoing improvements. For more details, you can read about our approach on our website.\n\nResults for illustrative purposes and may vary. Internet and subscription for certain features required. Available to users 18+. Create responsibly.",
            "domain": "gemini.google"
          },
          {
            "position": 10,
            "title": "Unpacking the magic of our new creative tools - YouTube Blog",
            "url": "https://blog.youtube/news-and-events/generative-ai-creation-tools-made-on-youtube-2025/",
            "snippet": "# Unpacking the magic of our new creative tools\n\nSep 16, 2025 [[read-time]] minute read\n\nSep 16, 2025 [[read-time]] minute read\n\nToday at Made on YouTube, we unveiled a suite of features designed to make creativity on YouTube more playful and effortless than ever before. We’re taking a deeper look at what you can expect when you try them out yourself.\n\nWe’ve partnered with Google DeepMind to bring a custom version of their most powerful video generation model, Veo 3, to YouTube. Veo 3 Fast is designed to work seamlessly in YouTube Shorts for millions of creators and users, for free. It generates outputs with lower latency at 480p so you can easily create video clips – and for the first time, with sound – from any idea, all from your phone.\n\nTap the create button, then the sparkle icon in the top right corner to find our latest gen AI creation tools including Veo 3.\n\nIt’s rolling out now in the United States, United Kingdom, Canada, Australia, and New Zealand, with plans to expand to more of you soon.\n\nWe're also introducing new Veo capabilities on Shorts that put your imagination in the spotlight. Soon, you’ll be able to:... We’ll start to experiment with all of these new capabilities in the coming months on Shorts.\n\nStarting a video from a blank timeline can be daunting. That’s why we’re introducing Edit with AI, a new feature that takes the initial heavy lifting off your shoulders. It transforms your raw camera roll footage into a compelling first draft, intelligently finding and arranging your best moments, adding music, transitions, and even a playful voiceover that can react to what’s happening in the video, in either English or Hindi. This gives you a solid starting point so you can jump straight to the fun part: personalizing your video and bringing your unique vision to life.\n\nWe’re experimenting now with Edit with AI on Shorts and in the YouTube Create app, and will expand the feature in the coming weeks in select markets.\n\nAs the world's largest creative playground, YouTube is where trends are born and where you can draw inspiration from. Imagine hearing a line of dialogue that sparks an idea—a funny phrase, a memorable quote, or a one-of-a-kind sound—and you want to remix it into a new sound. With our new Speech to Song remixing tool, you'll be able to do just that, quickly turning the dialogue from eligible videos into catchy soundtracks for your next Short.\n\nCheck it out for yourself. Listen to the original video, and how Speech to song helped transform it using Lyria 2, Google DeepMind's most advanced AI music model.... Now your GRWM videos take on a whole new level of fun!\n\nBehind the scenes, this uses the dialogue from the original video with Lyria 2 to help create the song. And you're able to add your own vibe for the song, like, chill, danceable, or fun. The final result attributes the original creator.\n\nAs always and across these features, we use SynthID watermarks and content labels to indicate that these creations were generated with AI.\n\nWe hope these features foster a new era of playful experimentation on YouTube!",
            "domain": "blog.youtube"
          }
        ],
        "success": true,
        "error": null
      },
      {
        "query_id": "q7",
        "query": "veo3.1 Google software version 2025 official data",
        "claim_id": "claim_1",
        "query_type": "statistical",
        "priority": "high",
        "results": [],
        "success": false,
        "error": "Rate limit exceeded. Please try again later."
      },
      {
        "query_id": "q6",
        "query": "Google veo3.1 release statement 2025",
        "claim_id": "claim_1",
        "query_type": "source_verification",
        "priority": "high",
        "results": [
          {
            "position": 1,
            "title": "Introducing Veo 3.1 and advanced capabilities in Flow - Google Blog",
            "url": "https://blog.google/technology/ai/veo-updates-flow/",
            "snippet": "# Introducing Veo 3.1 and advanced capabilities in Flow\n\nFive months ago, we introduced Flow, our AI filmmaking tool powered by Veo, and have been inspired by the creativity it has sparked with over 275 million videos generated in Flow\n\n1\n\n. We're always listening to your feedback, and we've heard that you want more artistic control within Flow, with increased support for audio across all features.\n\nToday, we’re introducing new and enhanced creative capabilities to edit your clips, giving you more granular control over your final scene. For the first time, we’re also bringing audio to existing capabilities like “Ingredients to Video,” “Frames to Video” and “Extend.”\n\nWe’re also introducing Veo 3.1, which brings richer audio, more narrative control, and enhanced realism that captures true-to-life textures. Veo 3.1 is state-of-the-art and builds on Veo 3, with stronger prompt adherence and improved audiovisual quality when turning images into videos.\n\n## Refine your narrative with audio and more control\n\nWith Veo 3.1, we’re bringing audio to existing capabilities to help you craft the perfect scene. These features are experimental and actively improving, and we’re excited to see what you create as we iterate based on your feedback.\n\nNow, with rich, generated audio, you can:\n\n**Craft the look of your scene.**With \"Ingredients to Video,\" you can use multiple reference images to control the characters, objects and style. Flow uses your ingredients to create a final scene that looks just as you envisioned. **Control the shot from start to finish.**Provide a starting and ending image with “Frames to Video,” and Flow will generate a seamless video that bridges the two, perfect for artful and epic transitions. **Create longer, seamless shots.**With \"Extend,\" you can create longer videos, even lasting for a minute or more, that connect to and continue the action from your original clip. Each video is generated based on the final second of your previous clip, making it most useful for creating a longer establishing shot.... ## Edit your ingredients and videos with more precision\n\nGreat ideas can strike at any point in the creative process. For moments when the first take isn't the final one, we're introducing new editing capabilities directly within Flow to help you reimagine and perfect your scenes.\n\n**Add new elements to any scene.**With “Insert,” introduce anything you can imagine, from realistic details to fantastical creatures. Flow now handles complex details like shadows and scene lighting, making the addition look natural. **Remove unwanted objects or characters seamlessly.**Soon, you’ll be able to take anything out of a scene, and Flow will reconstruct the background and surroundings, making it look as though the object was never there.\n\n## Start creating in Flow today\n\nWith more precise editing capabilities, audio across all existing features and higher-quality outputs powered by Veo 3.1, we're opening up new possibilities for richer, more powerful video storytelling right inside Flow.\n\nThe Veo 3.1 model is also available via the Gemini API for developers, Vertex AI for enterprise customers, and the Gemini app. New capabilities are available in both Gemini API\n\n2\n\nand Vertex AI\n\n3",
            "domain": "blog.google"
          },
          {
            "position": 2,
            "title": "Google announces Veo 3.1 and updates Flow with more controls, tools",
            "url": "https://9to5google.com/2025/10/15/veo-3-1/",
            "snippet": "Google today announced Veo 3.1 as its latest video generation model, with Flow getting a number of updates to take advantage of the latest capabilities.\n\nCompared to Veo 3, which was announced at I/O 2025 in May, this new version offers richer audio and “enhanced realism that captures true-to-life textures.” Veo 3.1 has a deeper understanding of storytelling, cinematic styles, and character interactions to give you more narrative control. The image-to-video capability benefits from improved audio-visual quality and better follows your prompt.\n\nVeo 3.1 and Veo 3.1 Fast are available in the Gemini app, as well as the Gemini API and Vertex AI, to power text-to-video and image-to-video for horizontal (16×9) and vertical (9×16) outputs.\n\nMeanwhile, Google is updating the Flow filmmaking tool to take advantage of Veo 3.1. Audio generation is coming to:\n\n**Ingredients to Video**: This can include images of multiple characters, objects, and styles **Scene extension**: You can Extend an existing clip, with Google basing it on the last second **Frames to Video**: This lets you specify the starting and ending images, with Flow bridging the two\n\nFlow is getting new editing capabilities. You can insert elements like objects, characters, and details, with Google handling shadows, scene lighting, and other complex details to make everything look natural.\n\nComing soon is the ability to remove objects and characters from a scene, with Flow working to reconstruct the background and surroundings to make the edit seamless.\n\n\n\n## Comments",
            "domain": "9to5google.com"
          },
          {
            "position": 3,
            "title": "Google releases Veo 3.1, adds it to Flow video editor",
            "url": "https://techcrunch.com/2025/10/15/google-releases-veo-3-1-adds-it-to-flow-video-editor/",
            "snippet": "In Brief\nGoogle launched its new video model Veo 3.1 with improved audio output, granular editing controls, and better output for image to video. It said that Veo 3.1 builds on May's Veo 3 release and generates more realistic clips and adheres to prompts better.\nThe model allows users to add an object to the video and have it blend into the clip's style, Google said. Soon, users will be able to remove an existing object from the video in Flow, too.\nVeo 3 already has edit features such as adding reference images to drive a character, providing the first and last frame to generate a clip using AI, and the ability to extend an existing video based on the last few frames. With Veo 3.1, Google is adding audio to all these features to make the clips more lively.\nThe company is rolling out the model to its video editor Flow, the Gemini App, along with Vertex and Gemini APIs. It said that since Flow's launch in May, users have created more than 275 million videos on the app.",
            "domain": "techcrunch.com"
          },
          {
            "position": 4,
            "title": "Introducing Veo 3.1 and new creative capabilities in the Gemini API",
            "url": "https://developers.googleblog.com/en/introducing-veo-3-1-and-new-creative-capabilities-in-the-gemini-api/",
            "snippet": "**Today, we are releasing Veo 3.1 and Veo 3.1 Fast in paid preview in the** **Gemini API** **.** This updated model offers several improvements, as well as improved outputs when generating video from images. These new models are available via the Gemini API in Google AI Studio and Vertex AI. Veo 3.1 is also available in the Gemini app and Flow.\n\nVeo 3.1 and Veo 3.1 Fast empower developers to create more engaging content through significant upgrades. The models now generate\n\n**richer native audio**, from natural conversations to synchronized sound effects, and offer **greater narrative control** with an improved understanding of cinematic styles. **Enhanced image-to-video** capabilities ensure better prompt adherence while delivering superior audio and visual quality and maintaining character consistency across multiple scenes.\n\nIn addition to the model update, we are introducing new model capabilities, including using reference images to guide video generation, extend existing Veo videos, and generate transitions between a first and last frame.\n\nYou can now guide the generation process by providing up to 3 reference images of a character, object, or scene. This is helpful for maintaining character consistency across multiple shots or applying a specific style to your video.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nconfig=types.GenerateVideosConfig(\n\nreference_images=[reference_image1, reference_image2],\n\n),\n\n\n\n```... Your story is no longer limited by the original generation. With Scene extension, you can create longer videos, even lasting for a minute or more, by generating new clips that connect to your previous video. Each new video is generated based on the final second of the previous clip. This maintains visual continuity, making the feature ideal for extending shots with background audio.\n\n```\n\nfrom google import genai\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nvideo=video_to_extend\n\n\n\n```\n\nCreate smooth, natural scenes that bridge two different images. By providing a starting and an ending image, you can direct Veo 3.1 to generate the transition between them, complete with accompanying audio.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nimage=first_frame,\n\nconfig=types.GenerateVideosConfig(\n\nlast_frame=last_frame,\n\n),\n\n\n\n```\n\n**What people are building with Veo 3.1**\n\nPromise Studios, a GenAI movie studio, uses Veo 3.1 within its MUSE Platform to enhance generative storyboarding and previsualization for director-driven storytelling at production quality.... Latitude is experimenting with Veo 3.1 in its generative narrative engine to instantly bring user-created stories to life.\n\nVeo 3.1 and these new capabilities are now available in paid preview via the Gemini API.\n\nVeo 3.1 is the same price as Veo 3. We are incredibly excited to see what you will build with these new capabilities!",
            "domain": "developers.googleblog.com"
          },
          {
            "position": 5,
            "title": "Build with Veo 3, now available in the Gemini API",
            "url": "https://developers.googleblog.com/en/veo-3-now-available-gemini-api/",
            "snippet": "First unveiled at Google I/O 2025, people around the world have already generated tens of millions of high-quality videos with Veo 3 (along with some new fun and interesting video trends). It is our first video model to incorporate high-fidelity video outputs and native audio, first with text-to-video and soon with image-to-video.\n\nDevelopers are already experimenting with Veo 3, discovering how the model can help them brainstorm content, rapidly iterate, and be more efficient.\n\nVeo 3 is designed to handle a range of video generation tasks, from cinematic narratives to dynamic character animations. With Veo 3, you can create more immersive experiences by not only generating stunning visuals, but also audio like dialogue and sound effects.\n\nLet’s take a look at some examples.\n\nExplore these examples and more with Veo 3 in Google AI Studio, available as an SDK template and interactive Starter App to remix, copy and extend. The Starter App and its sample code offer a convenient way for Paid Tier users to rapidly prototype with Veo 3 and more on the Gemini API, directly from Google AI Studio.\n\nClick the Key button in the top right of the AI Studio Build interface to select a Google Cloud Project with billing enabled to use the Paid Tier in AI Studio apps. See the FAQs for more.... Veo 3 will be priced at $0.75 per second for video and audio output. Additionally, Veo 3 Fast will be available soon, offering a faster and more cost-effective option for video creation.\n\nHere’s a basic Python example to create a video:\n\n```\n\nimport time\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.0-generate-preview\",\n\nprompt=\"a close-up shot of a golden retriever playing in a field of sunflowers\",\n\nconfig=types.GenerateVideosConfig(\n\nnegative_prompt=\"barking, woofing\",\n\n),\n\n\n\n# Waiting for the video(s) to be generated\n\nwhile not operation.done:\n\ntime.sleep(20)\n\noperation = client.operations.get(operation)\n\ngenerated_video = operation.result.generated_videos[0]\n\nclient.files.download(file=generated_video.video)\n\ngenerated_video.video.save(\"veo3_video.mp4\")\n\n```\n\nAll videos generated by Veo 3 models will continue to include a digital SynthID watermark. To get started, check out the documentation, cookbook, and a Veo 3 starter app in Google AI Studio:\n\nIn addition to being available via the Gemini API in Google AI Studio, Veo 3 is also available to Google AI subscribers in the Gemini app and Flow, and to enterprise customers via Vertex AI.... 1: Fluffy Characters Stop Motion: Inside a brightly colored, cozy kitchen made of felt and yarn. Professor Nibbles, a plump, fluffy hamster with oversized glasses, nervously stirs a bubbling pot on a miniature stove, muttering, \"Just a little more... 'essence of savory,' as the recipe calls for.\" The camera is a mid-shot, capturing his frantic stirring. Suddenly, the pot emits a loud \"POP!\" followed by a comical \"whoosh\" sound, and a geyser of iridescent green slime erupts, covering the entire kitchen. Professor Nibbles shrieks, \"Oh, dear! Not again!\" and scurries away, leaving a trail of tiny, panicked squeaks.\n\n2: The sequence begins with an extreme close-up of a single gear, slowly turning and reflecting harsh sunlight. The camera gradually pulls back in a continuous movement, revealing this is but one component of a colossal, mechanical heart half-buried in a desolate, rust-colored desert. A sweeping aerial shot establishes its enormous scale and isolation in the barren landscape. The camera descends to capture pipes hissing steam and the rhythmic thumping that echoes across the empty plains. A subtle shake effect synchronizes with each massive heartbeat. A lateral tracking shot discovers tiny, robed figures scurrying across the metallic surface. The camera follows one such figure in a detailed tracking shot as they perform meticulous maintenance, polishing brass valves and tightening immense bolts. A complex movement circles the entire structure, capturing different maintenance teams working in precarious positions across its rusted exterior. The final shot begins tight on the meticulous work of one tiny figure before executing a dramatic pull-out that reveals the true scale of the heart and the minuscule size of its caretakers, tending to the vital organ of an unseen, sleeping giant that extends beyond the frame.",
            "domain": "developers.googleblog.com"
          },
          {
            "position": 6,
            "title": "Newest generative media...",
            "url": "https://blog.google/technology/ai/generative-media-models-io-2025/",
            "snippet": "# Fuel your creativity with new generative media models and tools\n\nToday, we’re announcing our newest generative media models, which mark significant breakthroughs. These models create breathtaking images, videos and music, empowering artists to bring their creative vision to life. They also power amazing tools for everyone to express themselves.\n\nVeo 3 and Imagen 4, our newest video and image generation models, push the frontier of media generation, with their groundbreaking new capabilities. We're also expanding access to Lyria 2, giving musicians more tools to create music. Finally, we’re inviting visual storytellers to try Flow, our new AI filmmaking tool. Using Google DeepMind’s most advanced models, Flow lets you weave cinematic films with more sophisticated control of characters, scenes and styles, to bring your story to life.\n\nWe’ve partnered closely with the creative industries — filmmakers, musicians, artists, YouTube creators — to help shape these models and products responsibly and to give creators new tools to realize the possibilities of AI in their art.\n\n## Veo 3: Video, meet audio\n\nVeo 3, our new state-of-the-art video generation model, not only improves on the quality of Veo 2, but for the first time, can also generate videos with audio — traffic noises in the background of a city street scene, birds singing in a park, even dialogue between characters.\n\nAcross the board, Veo 3 excels from text and image prompting to real-world physics and accurate lip syncing. It’s great at understanding; you can tell a short story in your prompt, and the model gives you back a clip that brings it to life. Veo 3 is available today for Ultra subscribers in the United States in the Gemini app and in Flow. It’s also available for enterprise users on Vertex AI.... ## Veo 2 updates: New capabilities built with and for filmmakers\n\nAs we advance Veo 3, we’ve also added new capabilities to our popular Veo 2 model informed by our work with creators and filmmakers. Today, we’re launching several of these new capabilities, including:\n\n**Our state-of-the-art reference powered video**capability allows you to give Veo images of characters, scenes, objects, and even styles for better creative control and consistency. **Camera controls**help you define precise camera movements, including rotations, dollies and zooms, to achieve the perfect shot. **Outpainting**allows you to broaden your frame, turning your video from portrait to landscape, and making it easier to fit any screen size, intelligently adding to the scene. **Object add and remove**lets you add or erase objects from your videos. Veo understands scale, interactions, and shadows, and uses this understanding to create a natural, realistic-looking scene.\n\nReference powered video and camera controls are available now in Flow. We're excited to bring all these new capabilities to the Vertex AI API in the coming weeks, and to more products over the next few months.\n\nOriginal\n\nOutpaint and add a castle\n\nOriginal\n\nRemove spaceship\n\n## Flow: An AI filmmaking tool designed for Veo\n\nBuilt with and for creatives, Flow is an AI filmmaking tool that lets you seamlessly create cinematic clips, scenes and stories by bringing together Google DeepMind’s most advanced models: Veo, Imagen and Gemini. Use natural language to describe your shots to Flow, manage the ingredients for your story — cast, locations, objects and styles — in a single convenient place, and use Flow to weave your narrative into beautiful scenes.\n\nFlow is available today for Google AI Pro and Ultra plan subscribers in the U.S., with more countries coming soon.... ## Imagen 4: Stunning quality and superior typography\n\nOur latest Imagen model combines speed with precision to create stunning images. Imagen 4 has remarkable clarity in fine details like intricate fabrics, water droplets, and animal fur, and excels in both photorealistic and abstract styles. Imagen 4 can create images in a range of aspect ratios and up to 2k resolution - even better for printing or presentations. It is also significantly better at spelling and typography, making it easier to create your own greeting cards, posters and even comics.\n\nImagen 4 is available today in the Gemini app, Whisk, Vertex AI and across Slides, Vids, Docs and more in Workspace.\n\nSoon we’ll also be launching a fast variant of Imagen 4 that’s up to 10x faster than Imagen 3 — so you can explore ideas even faster.\n\n## Lyria 2: Powerful composition and endless exploration\n\nIn April, we expanded access to Music AI Sandbox, powered by Lyria 2. Music AI Sandbox offers musicians, producers and songwriters a set of experimental tools, which can spark new creative possibilities and help artists explore unique musical ideas. The expertise and valuable feedback from the music industry help us ensure our tools empower creators, while inviting creatives to realize the possibilities of AI in their art.\n\nLyria 2 brings powerful composition and endless exploration, and is now available for creators through YouTube Shorts and enterprises in Vertex AI. We've also made Lyria RealTime, our interactive music generation model which powers MusicFX DJ, available via an API and in AI Studio. Lyria RealTime allows anyone to interactively create, control, and perform generative music in real time.... ## Responsible creation and collaboration with the creative community\n\nSince launching in 2023, SynthID has watermarked over 10 billion images, videos, audio files and texts, helping identify them as AI-generated and reduce the chances of misinformation and misattribution. Outputs generated by Veo 3, Imagen 4 and Lyria 2 will continue to have SynthID watermarks.\n\nToday, we’re launching SynthID Detector, a verification portal to help people identify AI-generated content. Upload a piece of content and the SynthID Detector will identify if either the entire file or just a part of it has SynthID in it.\n\nWith all our generative AI models, we aim to unleash human creativity and enable artists and creators to bring their ideas to life faster and more easily than ever before.",
            "domain": "blog.google"
          },
          {
            "position": 7,
            "title": "What Is Google Veo 3.1? A Beginner's Guide to AI Video ...",
            "url": "https://skywork.ai/blog/google-veo-3-1-beginners-guide-ai-video-model/",
            "snippet": "If you’ve seen short, cinematic clips made “from a prompt” and wondered how it works, you’re in the right place. This guide will walk you through Google’s latest video generation model, Veo 3.1, in plain English—what it is, why it matters, where you can try it, and a simple first project you can finish in about 10 minutes. Don’t worry if you’ve never touched AI video before. We’ll move step by step and flag common gotchas so you can avoid them.\n\n## What is Google Veo 3.1?\n\nGoogle Veo 3.1 is a text-to-video AI model that turns short written prompts and references into short video clips, with native audio support and more control over storytelling and style. In October 2025, Google announced Veo 3.1 (and a faster variant) in paid preview for developers and creators. According to the Google Developers Blog announcement (Oct 15, 2025), Veo 3.1 is available via the Gemini API in Google AI Studio and Vertex AI, and it adds richer native audio, improved adherence to cinematic styles, and new creative controls.\n\nWhy it matters for beginners: Veo 3.1’s improvements make it easier to describe the mood and motion you want—like “a slow dolly-in on a vintage typewriter with soft rain ambience”—and actually get something close, without wrestling with advanced settings.... ## What’s new compared to earlier Veo versions?\n\nGoogle calls out a few key upgrades in Veo 3.1:\n\n- Richer native audio generation and better narrative control, with improved understanding of cinematic styles (publisher: Google; see the Developers Blog announcement (2025)).\n\n- New creative tools in supporting apps: you can guide generation with multiple reference images, extend existing Veo clips, or bridge between a first and last frame to create transitions. These capabilities are highlighted in the Google Blog on Veo 3.1 and Flow (Oct 2025).\n\n- Native audio emphasis and model positioning are also described on the DeepMind Veo model page, which introduces Veo’s “video, meet audio” concept.\n\nIndependent coverage, like TechCrunch’s report on the Veo 3.1 release (Oct 2025), provides context on rollout and app integration. For hard limits (length, resolution), always defer to Google’s official docs, as those details can change.\n\n## Where can you use Veo 3.1 today?\n\nAs of October 2025:\n\n- Flow (Google’s AI video editor): Google’s announcement notes Veo 3.1 and advanced creative controls in Flow, including “Ingredients to Video” (use multiple images as style/character references), “Frames to Video” (bridge between start/end frames), and “Extend” (lengthen a clip). Details are outlined in the Google Blog on Veo 3.1 and Flow (2025).\n\n- Gemini API (Google AI Studio and Vertex AI): Veo 3.1 and Veo 3.1 Fast are in paid preview via the Gemini API, per the Google Developers Blog announcement (2025).\n\nImportant: Access tiers, regions, and pricing can vary and may change. If you’re not seeing options in your account, check Google’s documentation or support for your account type and region.... ## What can Veo 3.1 generate right now? (Practical limits to know)\n\n- In Vertex AI’s preview for Veo 3.1 base generations, the documented lengths are currently short (choose 4, 6, or 8 seconds). This is specified on the Vertex AI Veo 3.1 preview page.\n\n- In Gemini API extension workflows, you can extend Veo-generated clips. The API docs describe a maximum of up to 141 seconds for input Veo videos in those extension scenarios, with 720p listed in that context. See the Gemini API video documentation (Google).\n\nThese numbers help set expectations: start with short shots, then extend or chain shots together. Avoid assuming 1080p or minute-long base generations unless Google’s official docs explicitly state it for your environment.... ## A 10-minute quickstart: your first Veo 3.1 clip\n\nWe’ll make a simple “coffee shop mood” shot you can adapt for social posts.\n\n- Choose your aspect ratio\n\n- 16:9 (landscape) for YouTube and desktop-first screens\n\n- 9:16 (vertical) for TikTok, Reels, and Shorts Pick one at the start to avoid accidental cropping later.\n\n- Write a clear, concrete prompt Use this structure to get reliable results: subject + action + setting + style + camera + audio.\n\n- Example prompt: “A barista gently places a ceramic cup on a wooden counter; shallow depth of field; warm morning light streaming through windows; slow dolly-in; subtle steam rising; soft cafe ambience, no dialogue, light jazz in the background.”\n\nIf you want a deeper primer on prompt structure and clarity, see these prompt engineering best practices for beginners.\n\n- Generate in your chosen interface\n\n- In Flow: Start a new project, choose Veo 3.1, paste your prompt, set aspect ratio, and generate.\n\n- In Gemini API/Vertex AI: Use the model/version and parameters documented for Veo 3.1 in your environment. Begin with short durations and default settings.\n\n- Review the output like a director\n\n- Does the subject and action match? Is the lighting/mood close?\n\n- Is the camera movement smooth? Is audio what you expected?\n\n- Make one or two changes per iteration—small edits beat big rewrites.\n\n- Refine with controls\n\n- Reference images: If you want consistency (e.g., the same mug or barista style), use up to three reference images to guide the look. This capability is described in Google’s announcements (2025).\n\n- Frames to Video: Provide a starting and ending frame to shape the motion between them.\n\n- Extend: Lengthen your favorite moment to create a longer beat.\n\n- Export Choose the format/aspect ratio you started with. If you need both vertical and horizontal versions, plan to reframe or regenerate with the other aspect ratio rather than cropping aggressively.... ## A practical planning example using Skywork AI (optional, 5 minutes)\n\nSkywork AI can help you prepare the words before you ever hit “Generate.” Disclosure: Skywork AI is our product.\n\nHere’s a neutral, step-by-step way to use it purely for planning:\n\n- Open Skywork and create a new document. Ask for a short video outline: “30-second coffee shop mood piece: 3 shots, warm tone, slow camera.”\n\n- Have it draft a compact shot list with camera moves and audio notes, like:\n\n- Close-up of cup; slow dolly-in; soft steam; light jazz; no dialogue\n\n- Medium barista hands; gentle rack focus; cafe ambience; espresso hiss\n\n- Wide room tone; sunbeams; slow tilt up; footsteps and cups\n\n- Ask for a final prompt assembled from the shot you want to generate first. Copy that prompt into Veo 3.1.\n\nIf you prefer to outline prompts and story beats yourself, this short guide to Skywork’s General Mode for planning and outlining walks through a simple, distraction-free workflow.... ## FAQs for first-time users\n\n\n\nIs Veo 3.1 free? No. Google describes Veo 3.1 and Veo 3.1 Fast as being in paid preview via the Gemini API as of October 2025. See the Google Developers Blog announcement (2025). Pricing varies by product and usage.\n\n\n\nWhere can I try Veo 3.1? Flow (for a visual editor), and the Gemini API via Google AI Studio and Vertex AI (for developers). Google outlines these options in the Veo 3.1 and Flow post (2025) and the Developers Blog announcement (2025).\n\n\n\nHow long can my video be? It depends on the environment and workflow. In Vertex AI’s Veo 3.1 preview, base generations are short (4, 6, or 8 seconds). In Gemini API extension workflows, input Veo videos can be extended up to 141 seconds. See the Vertex AI preview page and the Gemini API video docs.\n\n\n\nWhat resolutions are supported? The Gemini API documentation for extension workflows references 720p in that context. Official materials do not universally guarantee 1080p for base Veo 3.1 generations at this time.\n\n\n\nCan I keep a character or object consistent across shots? Yes. Google notes you can guide Veo 3.1 with up to three reference images for a character, object, or scene. This is described in the Developers Blog announcement (2025).\n\n\n\nCan I add my own audio and sync it? Veo 3.1 focuses on native audio generation controlled via your prompt. The public pages cited here do not detail a full “upload and auto-sync” workflow, so avoid relying on that unless you see it documented for your account.... ## Next steps\n\n- Try a tiny project: one 6–8 second shot with a clear prompt. If you like the result, use “Extend” or stitch a second shot with “Frames to Video.”\n\n- Build a simple storyboard first. If you want help outlining ideas, you can use Skywork AI to draft prompts and shot lists before you generate. Keep it simple: one scene, one action, one camera move.\n\n- When you’re ready to go deeper, explore Google’s official resources: the Veo 3.1 and Flow post (2025), the Developers Blog announcement (2025), the DeepMind Veo overview, the Gemini API video docs, and the Vertex AI Veo 3.1 preview.\n\nYou don’t need special “artistic talent” to start—just one clear sentence and a little patience. Generate, review, tweak, repeat. That’s how everyone learns, and you’ll be surprised how quickly it clicks.",
            "domain": "skywork.ai"
          },
          {
            "position": 8,
            "title": "Google announces Veo 3.1 and Flow updates - 9to5Google",
            "url": "https://9to5google.com/2025/10/15/veo-3-1/?extended-comments=1",
            "snippet": "Google today announced Veo 3.1 as its latest video generation model, with Flow getting a number of updates to take advantage of the latest capabilities.\n\nCompared to Veo 3, which was announced at I/O 2025 in May, this new version offers richer audio and “enhanced realism that captures true-to-life textures.” Veo 3.1 has a deeper understanding of storytelling, cinematic styles, and character interactions to give you more narrative control. The image-to-video capability benefits from improved audio-visual quality and better follows your prompt.\n\nVeo 3.1 and Veo 3.1 Fast are available in the Gemini app, as well as the Gemini API and Vertex AI, to power text-to-video and image-to-video for horizontal (16×9) and vertical (9×16) outputs.\n\nMeanwhile, Google is updating the Flow filmmaking tool to take advantage of Veo 3.1. Audio generation is coming to:\n\n**Ingredients to Video**: This can include images of multiple characters, objects, and styles **Scene extension**: You can Extend an existing clip, with Google basing it on the last second **Frames to Video**: This lets you specify the starting and ending images, with Flow bridging the two\n\nFlow is getting new editing capabilities. You can insert elements like objects, characters, and details, with Google handling shadows, scene lighting, and other complex details to make everything look natural.\n\nComing soon is the ability to remove objects and characters from a scene, with Flow working to reconstruct the background and surroundings to make the edit seamless.\n\n\n\n## Comments",
            "domain": "9to5google.com"
          },
          {
            "position": 9,
            "title": "Google rolls out its new Veo 3 video-generation model ...",
            "url": "https://techcrunch.com/2025/07/03/google-rolls-out-its-new-veo-3-video-generation-model-globally/",
            "snippet": "Google on Thursday said it has begun rolling out its Veo 3 video generation model to Gemini users in more than 159 countries.\n\nVideo generation via the new model is available only to paying subscribers of Google’s AI Pro plan and is capped at three videos per day.\n\nVeo 3, which Google showed off in May, lets users generate videos up to eight seconds long using text prompts.\n\nGoogle’s Josh Woodward has said that the company is working on adding image-to-video generation capabilities to Gemini.",
            "domain": "techcrunch.com"
          },
          {
            "position": 10,
            "title": "Google's Veo 3.1 is better at generating videos from images",
            "url": "https://www.engadget.com/ai/googles-veo-31-is-better-at-generating-videos-from-images-220829129.html",
            "snippet": "Google has released a new update to its Veo AI video generation model that should make it do a better job of sticking to prompts and converting images into videos. Veo 3.1 is available to try today through Google's Gemini API and is now also powering the company's Flow video editor.\nVeo 3.1 builds on the new capabilities Google introduced with launch of Veo 3 at Google I/O 2025. The new model offers better \"prompt adherence,\" according to Google, and should have an easier time creating videos based on the image \"ingredients\" you upload alongside your written prompt. Veo 3.1 also makes it possible to convert images to video and generate audio at the same time, a capability that wasn't available with Veo 3.\nIn Flow, Veo 3.1 supports at least a new feature that gives you finer control over the videos you generate. With what Google calls \"Frame to Video,\" Flow lets you upload a first and last frame, and then generates the video in-between. Adobe Firefly, which is powered by Veo 3, offers a similar feature, but Flow will be able to pull it off and create audio at the same time. Those added audio skills will also apply to the video editor's ability to extend clips and insert objects into existing footage, too.\nBased on the samples Google's shared, videos generated with Veo 3.1 still have an uncanny quality that seems to vary greatly depending on the prompt and subject. Even if it's missing some of the realism of OpenAI's Sora 2, though, the company's decision to try and make Veo more useful to people who actually work with video rather than a source of social media spam is a welcome move.",
            "domain": "www.engadget.com"
          }
        ],
        "success": true,
        "error": null
      },
      {
        "query_id": "q8",
        "query": "Google veo3.1 release 2025 debunked",
        "claim_id": "claim_1",
        "query_type": "contradiction",
        "priority": "high",
        "results": [
          {
            "position": 1,
            "title": "VEO 3 is UNREAL...it might actually take my job",
            "url": "https://www.youtube.com/watch?v=Xj4BDwgEwd4",
            "snippet": "{ts:0} VO3 is here and it's shocked the entire industry. Uh, Alex, what was that? It's\n{ts:8} nothing. Don't worry about it. Are you trying to replace me with AI? What? No, I'm just trying something new. Just do\n{ts:16} the normal intro. It It's fine. All right. V3 just dropped and I'm going to show you so many incredible examples of\n{ts:24} V3 in action. Let's get into it. Oh, and Alex, don't do that again. Anyways, let's get into it. All right. I've been\n{ts:32} seeing a ton of these street style interviews, hyper realistic, where someone is interviewing people on the\n{ts:38} street who kind of just stumbled out of a bar. They all are either a little bit tipsy or drunk, and V3 has been\n{ts:45} recreating these so well. So, here are two that I've made. The dialogue between them didn't exactly match my prompt, but\n{ts:53} I'll show you some others where it's pretty compelling. So, this is street interview. Hyperrealistic guy\n{ts:59} interviewing two girls, all Gen Z. They just came out of a bar kind of tipsy drunk. The interviewer asks,... \"Okay,\n{ts:65} low-key, can you believe we aren't real?\" Then girl one says, \"I don't know about you, honey. I'm 100% real.\" In a\n{ts:72} sassy attitude, \"Please do not clip that.\" And girl two says, \"Yeah, yeah, bet. We're as real as they come.\" Almost\n{ts:78} interrupting the first girl. Okay, so here's the first video. Okay, low key, can you believe we aren't real? I don't\n{ts:85} know about you, honey. I'm 100% real. Yeah. Yeah. Yeah. Bet. We're as real as they come. All right. And then here's\n{ts:90} the second generation, the second version that V3 created. Okay. Low key. Can you believe we aren't real? I don't\n{ts:96} know about you, honey. I'm 100% real. Yeah. Yeah. Bet. We're as real as they come. All right. So, in this next one, I\n{ts:103} tried to get two trains heading towards each other. They smash into each other. Huge explosion. And for some reason, I\n{ts:110} just could not get it to work. But let me show you what did generate. So, here was my first attempt. Two massive trains... {ts:255} second one, the hand looks better. That's weird. And it kind of just changes\n{ts:263} frames for a second. But uh yeah, either way, that is not what's in a Rubik's cube. Look at this. It actually looks\n{ts:269} really cool. Look at all of these detailed gears inside the Rubik's Cube. All right, but of course you are all\n{ts:275} familiar with this meme. And what if you get V3 to make a video out of it? Are you serious right now? I can't\n{ts:284} believe you. You are unbelievable. Do you have an idea that you've been putting off for a\n{ts:290} while because you don't have the technical knowledge to turn it into reality? With Hostinger Horizons, this\n{ts:296} is now possible. Hostinger just launched Hostinger Horizons, which is the easiest way to launch full applications with no\n{ts:305} code. This is vibe coding, but even easier because the deployment happens automatically. Hostinger Horizons is an\n{ts:312} all-in-one solution. Manage hosting, domains, and email all in one place while being able to take your idea from... {ts:459} kind of wanted to see if it could nail the horror vibe with like aliens in a dark alley. So, check this out.\n{ts:469} I think the only thing that it didn't do super well in this video is the sound effects are just okay. If we look over\n{ts:475} at the second one, it did a way better job with the sound effects. Yeah, I mean that's great. Even\n{ts:483} when the alien kind of like walks in front of the light, it covers the light on the ground. That's pretty dang... 't know\n{ts:558} what VO was doing. All right, Matt, back to you. But remember, Flow from Google, which houses VO3, is not just about\n{ts:566} creating 7-second clips. You should be able to create entire videos from it. It basically allows you to take these clips\n{ts:572} and put them together in really unique ways. Here's one that is if Jurassic Park were actually real. Check this out.\n{ts:578} We are on our way to Jurassic Park. I am so excited. I've always wanted to go and finally we are doing it. It's going to\n{ts:584} be great. Right, Jason? Yeah. I can't wait. Okay, bye. All right, we're [Music]\n{ts:596} here. Got the dinosaurs. Got all the people watching. I mean, everybody looks really good. There's no limbs coming out\n{ts:602} of them. This dinosaur doesn't look fantastic. All of these dinosaurs don't look hyper realistic. I think they could\n{ts:608} have done a little bit better of a job there. It looks like animatronics to be honest, but still. This is all from AI... {ts:742} screen. Yeah. And the crazy thing about this thing is it has All right, so this is one thing I've noticed with these V3\n{ts:751} videos. Whenever it has a human talking, there's always these awkward pauses. It's just a half second too long of a\n{ts:757} pause and that's where you really can see it's AI generated, but most of the time you can't. In fact, you know that\n{ts:765} opening clip that we did in this video, I showed that to my wife and said, \"Somebody copied our channel and I just\n{ts:771} showed it to her and didn't say anything else.\" And she looked at it and she was like, \"Oh, oh, that sucks.\" And then I\n{ts:777} had to tell her it was AI because it looked that real. All right, let's keep watching. Heated\n{ts:785} seats. Check this out. Look at that. Just slice right through. The N9 portable fusion reactor is small and\n{ts:794} almost meltdown free. Almost. This is the best flying experience\n{ts:804} ever. These controls are amazing. All right, so I thought that was really good. But it... 's it's pretty good. The first\n{ts:1126} time that I watched it, I did actually laugh out loud. That was actually pretty funny. Next, I just asked it to make a\n{ts:1132} detailed look at the solar system. And yeah, I think it did an all right job. Let's check out the second\n{ts:1140} clip. Now, this one I think is a bit better, but V3 kind of likes to put objects in front of the camera when it's\n{ts:1148} backing out like that right there. And it kind of just like spawns a planet right in front of the camera. And yeah,\n{ts:1154} it doesn't look the best, but honestly, it's pretty good. All right, back to Matt. All right, next. I wanted to see\n{ts:1161} if it could create the game Portal, but hyperrealistic. Essentially, what I wanted it to do is what I say here. A\n{ts:1167} mediumbuild man in his 30s wearing a futuristic tactical suit with glowing blue accents stands in a dimly lit\n{ts:1173} industrial room with exposed pipes, metal walls, and flickering lights. He holds a sleek high-tech portal gun with... {ts:1233} in terms of just visuals looks incredible. Although it's not what I asked\n{ts:1242} for. Yeah. So, pretty good. You only saw his reflection through the portal for about a frame or two, but it wasn't that\n{ts:1250} good. Anyways, but the visuals again, the visuals, the detail, the clarity, all really, really impressive. All\n{ts:1257} right, next. Meta Puppet made a video called This is Plastic made with VO3. Spoilers in next post. Watch before\n{ts:1264} reading. So, this is a 2 minute 45 second video. Quite long. I'm not going to play it in full. I'm going to skip\n{ts:1269} around a little bit, but let me show you. Studies have revealed that microplastics are being found in human\n{ts:1274} testicles, raising concern. You can never trust these studies on male reproductive health.\n{ts:1279} [Music] Okay, that is hilarious. And remember, all of this was put together using Flow\n{ts:1291} Plus V3. These are both Google products. So, you have a little plastic baby. God, that",
            "domain": "www.youtube.com"
          },
          {
            "position": 2,
            "title": "I was wrong - AI video is nuts (don't sleep on Veo 3)",
            "url": "https://www.youtube.com/watch?v=_3PCta2uyvc",
            "snippet": "## Theo - t3․gg\n##### May 26, 2025 (0:16:08)\nI severely underestimated Google's Veo 3 model. The output quality is insane, we need to talk about this...\n\nThank you Imagekit for sponsoring! Check them out at: https://soydev.link/imagekit\n\nUse code VEO for 1 month of T3 Chat for just $1: https://soydev.link/chat\n(only valid for new customers)\n\nSOURCES\nhttps://x.com/ArtificialAnlys/status/1925159679824744804\nhttps://x.com/ArtificialAnlys/status/1925549565303763269\nhttps://x.com/theo/status/1925125767371149823\nhttps://x.com/theo/status/1925134963978207319\n\nWant to sponsor a video? Learn more here: https://soydev.link/sponsor-me\n\nCheck out my Twitch, Twitter, Discord more at https://t3.gg\n\nS/O Ph4se0n3 for the awesome edit 🙏... {ts:0} I just did a video about Google IO, but I missed something. I thought the video\n{ts:4} model was mediocre. I was wrong. Pretty nuts for a oneshot, right? Like, I just generated that trivially. It still costs\n{ts:13} 250 bucks a month to use any of this right now. And the UI is garbage and it's annoying as hell to use. But the\n{ts:18} quality of what you can get out of V3 is significantly better than I thought. My tests were bad. I didn't look into it\n{ts:25} enough. And I'm making this video both because I was wrong for not better covering it, but also because I found it\n{ts:32} actually very, very fun to play with and I wanted to share with you guys. That all said, I've already burned through\n{ts:37} most of the credits I get for the $250 and I want more. So, quick break from today's sponsor and then we'll get right\n{ts:44} to it. I've been a webdev for a while and one of the most annoying things to get right is images. Seriously, I can't\n{ts:50} believe I... {ts:265} other people doing demos with it. like, \"Wait, it can do that much?\" I went and played more. There was a lot of edges\n{ts:270} that I had to get through. The biggest one being the Flow website, which is so bad. We'll go over some of the ways it's\n{ts:276} bad in just a bit. I was trying to prompt it to look like me back when I still had the blonde hair and mustache,\n{ts:280} and it came out looking like Prime. But another test, I tried this one like eight times, and this is the best I\n{ts:285} could do. Something caused the first still to look awful. I don't know why it's like that. None of the rest had\n{ts:291} that problem. Once you It plays, it's fine, but you'll notice some details on this one.\n{ts:296} Use code VEO at checkout for one month free on T3 Chat. Yeah, it isn't great at\n{ts:305} text. It tried, but it's not great at it. You need to give it a very small amount of text to render. And even if\n{ts:311} you tell it to not put in subtitles, it just will sometimes. The free month code included there did work, but we... {ts:372} made the mistake of here is I assumed when you do frames to video and you give it a frame that you've saved that it\n{ts:380} would still use the thing you selected because if you do ingredients to video and you select something for it to start\n{ts:386} and you try to submit it with V3 selected, it will fail. It says in the corner here and I need it on full screen\n{ts:392} for you to see it. Switching you to a compatible model for this feature. Submit again to confirm or check\n{ts:397} settings for details. I wish it told me where in settings to check. I don'... {ts:561} was wrong. I just Yeah, it's the weird breath at the end. Cool. Stop it there. Then we will extend it and say make sure\n{ts:570} we're on the right model because again it keeps changing back to V2 even though this is the VO3 clip I'm trying to\n{ts:575} extend. I almost want to try it so you can see how much worse it is in comparison. Switching you to a\n{ts:580} compatible model for this feature. Submit again to confirm. Look at that. You can't even use it on V2 quality. It\n{ts:587} bumps you to fast. There's so much potential here and just none of it's being realized because\n{ts:594} this UI is awful. It it tricked me into thinking this was all much worse than it actually is. I wish they just gave us\n{ts:600} the model in a more reasonable like shape for us to play with and consume. But V3 is not on the API yet. There's no\n{ts:606} way for us to use any of it yet. So sorry T3 chat can't add this. But despite all of that, it's still just an\n{ts:613} incredible model. Do you know what... 's even better than this spaghetti? T3 chat. Like what? What do\n{ts:625} you guys remember like a year and a half ago how far we were from Will Smith eating spaghetti? It's not Will Smith,\n{ts:632} but that is absolutely spaghetti being eaten. It's kind of crazy where that's all at. Google doesn't know how to make\n{ts:640} creative tools or really power tools in general. They make decent enough consumerf facing software. They make\n{ts:647} decent enough infrastructure and they make incredible models in generative tools, but they don't know how to make\n{ts:653} like a good video editor. If you don't believe me, go try the one they built for YouTube. It's it's interesting. It's\n{ts:658} a it's often cited as a good example of a Flutter app. If you can predict what that means for the quality of\n{ts:665} experience, but the model here is so good. And once again, what I'm excited about is what people will do with this\n{ts:672} tool. But I'm also a bit terrified because this looks better than some like iPhone video. I see things like\n{ts:681} verifying your identity just got a lot sketchier because if I... t going to trust it as much. This is going to really change our like trust vectors for\n{ts:839} what is or isn't real. I don't even know now how I will be able to tell if a given video that is sent to me is real\n{ts:844} or not because this stuff is actually that compelling. And if somebody makes a less restricted version of this model or\n{ts:850} gets something close to this in the open source world or with stable diffusion, I'm scared. I'm legitimately scared. You\n{ts:858} are telling me to try again generating with my blurred photo. I'll be more specific. Clean shaven white\n{ts:866} man. Be sure to include the audio of him speaking. Make sure it's still V3. Yep. Cool. Let's see how it does. Switching\n{ts:875} you to a compatible model. So, it's too fast. Not even quality. Yeah, you can't do it. you you can't do anything but\n{ts:883} text the video for V3 right now, which I'm pretty sure is a safety thing just due to the nature of what this model is\n{ts:890} capable of. And as we've now seen, and I can show more examples of the gap between two and three is a bit... {ts:898} absurd. This is one I accidentally did with two. You can see the audio doesn't exist. It got the text okay there, but\n{ts:905} it went a little absurd with the subtitles. This one was really funny. It feels like a Bollywood\n{ts:915} movie. The way the T3 chat fades into the screen is so hilarious. Yeah, this is why I didn't care because none of the\n{ts:923} video models have felt like a significant improvement from that to this point. I did not realize how absurd\n{ts:930} this got, especially with how bad the UX is. Like I hit the upscale button cuz when you download, you can choose what\n{ts:937} format you want to download in. If it's not frozen, which it was there for a sec. You can pick animated GIF,\n{ts:942} original, or upscaled. Upscale just doesn't work. I've been sitting here waiting for this to upscale for like an\n{ts:947} hour now, and it just hangs forever. It does say this can take a few minutes, but like what's a few minutes, Google?\n{ts:956} It's been an hour. Yeah. What did you think? Is this exciting or scary? Until next time, peace nerds.",
            "domain": "www.youtube.com"
          },
          {
            "position": 3,
            "title": "Veo (text-to-video model) - Wikipedia",
            "url": "https://en.wikipedia.org/wiki/Veo_(text-to-video_model)",
            "snippet": "**Veo**, or **Google Veo**, is a text-to-video model developed by Google DeepMind and announced in May 2024. As a generative AI model, it creates videos based on user prompts. Veo 3, released in May 2025, can also generate accompanying audio.\n\n## Development\n\nIn May 2024, a multimodal video generation model called Veo was announced at Google I/O 2024. Google claimed that it could generate 1080p videos over a minute long. In December 2024, Google released Veo 2, available via VideoFX. It supports 4K resolution video generation and has an improved understanding of physics. In April 2025, Google announced that Veo 2 became available for advanced users on the Gemini app.\n\nIn May 2025, Google released Veo 3, which not only generates videos but also creates synchronized audio — including dialogue, sound effects, and ambient noise — to match the visuals. Google also announced **Flow**, a video-creation tool powered by Veo and Imagen. Google DeepMind CEO Demis Hassabis described the release as the moment when AI video generation left the era of the silent film.... ## Capabilities and limitations\n\nGoogle Veo can be bought by several subscription/membership tiers, and/or by using Google \"AI credits\". The software itself can be run by two different consoles called Google Gemini and Google Flow, with Gemini being geared towards shorter, quicker, and faster projects, using the Gemini AI chat model, or through Google Flow, which is essentially a movie editor, as well, allowing users to create longer projects, and continuity using the same characters and actors. Users can create a maximum length of eight seconds per clip.\n\nGoogle Veo, has a relatively simple interface and dashboard, however writing prompts, for those who have little to no experience in transcribing or filmmaking may face issues with the software misunderstanding what the user intended by their prompt (no matter how detailed it was). So although Veo does have a friendly and simple setup, prompts, which are the forefront of the software, need to be not only short and to the point, but they also must be very specific, if the user wants the right vision for their project. Google Veo, when it comes to human models, is able to generate several ethnicity and body types. The software is also capable of generating stand up comedy routines, and Music videos. It can as well generate animals, cartoons, and animation. Prompts must accurately describe places, people, and things in each scene, in addition knowledge of film and camera lingo such as panning, zooming, and terms for camera angles, are also important.... Google Veo however, has strict guidelines and blockades to their software. Before a clip is generated, the algorithm computer software reviews it, and if it is anything deemed inappropriate, too graphically sexual, illegal, showcasing graphic abuse/assault/fighting (unless the prompt specifies that it is a fictitious martial arts scene etc.) gross behaviors, antisemitism, racist, homophobic, anything depicting reigning regimes, rioting, blood, gore, or warfare, (unless in some cases the prompt specifies that it is fictitious period drama, the clip may still be generated), the clip will not be generated. In addition, Google Veo cannot and will not generate character actors that look identical to celebrities or real-life individuals. Users have primarily complained that, regardless of how descriptive and detailed their prompts are, Google Veo often misunderstands the input, resulting in completely different outputs. Common issues include the emulation of incorrect subtitles and captions, the generation of complex scenes that are incomplete due to the maximum eight-second length, the production of garbled and nonsensical speech, and character models that appear deformed in both appearance and movement. Users have also reported that their prompts and generated content are falsely flagged as violating guidelines, along with a variety of other issues and complaints. However, trial and error may have to be used with Veo for optimal results.... ## Reactions\n\nA reporter for *Gizmodo* reacted to the release of Veo 3 by observing that users were directing the model to generate low-quality content, such as man on the street interviews or haul videos of people unboxing products. Another media commentator reported that the tool tended to repeat the same joke in response to different prompts.\n\nCommentators speculated that Google had trained the service on YouTube videos or Reddit posts. Google itself had not stated the source of its training content.\n\nIn July 2025, Media Matters for America reported that racist and antisemitic videos generated using Veo 3 were being uploaded to TikTok. Ryan Whitwam of *Ars Technica* commented, \"In a perfect world, Veo 3 would refuse to create these videos, but vagueness in the prompt and the AI's inability to understand the subtleties of racist tropes (i.e., the use of monkeys instead of humans in some videos) make it easy to skirt the rules.\"\n\n## See also\n- Sora (text-to-video model)\n- VideoPoet – Text-to-video model by Google\n- Dream Machine (text-to-video model)\n\n## References\n\n## External links\n- Official website\n- *Introducing Veo 3.1 and advanced capabilities in Flow*\n\nCategories: - 2024 software\n- Applications of artificial intelligence\n- Film and video technology\n- Google DeepMind\n- Text-to-video generation\n- Video processing\n- Generative artificial intelligence\n- 2024 in artificial intelligence",
            "domain": "en.wikipedia.org"
          },
          {
            "position": 4,
            "title": "Google's Veo 3 Has People Crashing Out Over AI Slop",
            "url": "https://gizmodo.com/googles-veo-3-has-people-crashing-out-over-ai-slop-2000608803",
            "snippet": "Depending on who you ask, generative AI is either a thrilling tech revolution or an existential threat, and there's little in-between. It's hard to blame anyone for an extreme reaction, too, given the magnitude of capital investment, hyperbolic marketing, and rapid progress of generative AI in such a short amount of time. But it's not just the economics and technical feats of AI that have people losing their minds; there's also something more philosophical percolating, and it's driving some people 'to the brink.'\nPeople are literally having a mental breakdown over Veo-3 pic.twitter.com/ym5oZDYZGr\n— Chubby♨️ (@kimmonismus) May 27, 2025\nThe latest AI advancement to send people down an existential rabbit hole comes courtesy of Google, which just announced its latest video generation model called Veo 3. As I've reported a few times now, Veo 3 is already getting into some wild stuff—turning up the dial on AI slop, deepfaking smooth-brained YouTube content, and potentially upending game development, to name a few things. As it turns out, people are taking note of all of those feats, and some of them are not exactly happy about what they see.... As evidenced by a thread from the subreddit r/artificialintelligence posted this week titled 'VEO3 is kind of bringing me to a mental brink. What are we even doing anymore?' Google's Veo 3 and the implications therein have some people spiraling. 'I'm just kind of speechless. The concept of existential crisis has taken a whole new form. I was unhappy with my life just now but thought I can turn it around, but if I turn it around, what is left of our world in 2 decades?' the post's author writes.\n'Actors as a concept are gone? Manually creating music? Wallpapers? Game assets? Believing comments on the internet are from real people? AI edited photos are just as real as the original samples? Voicenotes can be perfectly faked?… Literally what value is being left for us?'\nReactions to the thread are mixed, with suggestions that the author should go 'touch grass' or maybe 'go to therapy,' but there's also a chorus in agreement. The consensus from the latter group? AI slop is coming to ruin your art, and there's not much we can do about it.\nI, for what it's worth, fall unhelpfully in between the two camps. I think there is a deluge of AI slop incoming, and, if we're being honest, we're already up to our ankles. Between Veo and OpenAI's Sora and the clear interest in automating human creativity, I think we can reasonably buckle in and expect the world of movies, music, and entertainment writ large to get a little choppy. Whether any of those efforts to automate entertainment will stick is less obvious. The thing about art is that the kind that people tend to like is the kind that has something substantial to say. Right now, for all of its mimicry, generative AI doesn't actually have anything to say, because technically all it can do is remix and repeat.... I did more tests with Google's #Veo3. Imagine if AI characters became aware they were living in a simulation! pic.twitter.com/nhbrNQMtqv\n— Hashem Al-Ghaili (@HashemGhaili) May 21, 2025\nCall me an optimist, but most people can likely sniff out the difference between slop and art, and as much as studios would love to wave a magic wand and rid themselves of human creatives and the cost of their labor, deep down they know that they'd have to Ctrl+Z that move just as fast. That's not to say there won't be casualties in the AI age—if there's one lesson we can learn from mass waves of automation in years past, it's that labor forces are usually the most affected.\nBut when it comes to art, things aren't so simple. Art, at least the good kind, is about human connection, and until AI can think and feel like we do, there's nothing that can replace that. So, before you crash out over AI slop, just remember: AI still thinks putting glue on your pizza is a good idea, so we may have a few more good years left in the tank.",
            "domain": "gizmodo.com"
          },
          {
            "position": 5,
            "title": "What to Expect Veo 3.1 Google's Next-Gen Video AI Release",
            "url": "https://gptproto.com/blog/veo-3-1",
            "snippet": "The AI video generation space is heating up. Following recent improvements to Veo 3 announced in September 2025, early signs suggest Google DeepMind is preparing to launch Veo 3.1 on October 10, 2025. While Google hasn’t officially confirmed the release, leaked information from various sources points to significant upgrades that could reshape how creators make video content. This incremental update appears designed to keep pace with OpenAI’s Sora 2, which launched just last month with impressive capabilities.\n\nIf you’re a content creator, filmmaker, or developer wondering whether this matters for your workflow, here’s what the leaks suggest about tomorrow’s release:\n\n- Ultimate character consistency that solves the morphing problem\n\n- Native 1080p output with professional cinematic presets\n\n- Extended video generation up to 60 seconds\n\n- Multi-prompting for creating connected scene sequences\n\n- Better prompt understanding and motion quality\n\n- Access through unified platforms like GPT Proto\n\n## Understanding Veo 3.1\n\nVeo 3.1 represents the latest evolution of Google DeepMind’s video generation technology. Building on the foundation of Veo 3, which introduced native audio generation in May 2025, this new version focuses on solving practical problems that creators face daily. Think of it as Google’s answer to the competitive pressure from OpenAI and other players in the AI video market.\n\nThe timing is strategic. OpenAI released Sora 2 on September 30, 2025, with features that emphasize physical realism and multi-scene storytelling. Google appears ready to counter with its own improvements that address similar needs while leveraging its existing infrastructure across Gemini and Vertex AI.... ## How We Know About Veo 3.1\n\nThe first hints appeared on Higgsfield AI’s waitlist page, where references to Veo 3.1 surfaced unexpectedly. Shortly after, developers noticed the model name in Vertex AI’s internal codebase, suggesting Google was preparing for deployment. Community discussions on platforms like Reddit and Discord added fuel to the fire, with some users claiming they received early access invitations.\n\nNone of this constitutes official confirmation from Google, but the pattern matches previous releases. Veo 2 and Veo 3 both followed similar leak patterns before their official announcements. The lack of denial from Google has only increased speculation that tomorrow’s launch is real.... ## Major Feature Upgrades Based on Leaks\n\n### Character Consistency Finally Solved\n\nOne of the biggest complaints about current AI video models is character morphing. You start with someone who has brown hair, and by second four, they suddenly have blonde hair. Leaked information suggests Veo 3.1 tackles this head-on with what insiders call “ultimate character consistency.” The system reportedly maintains facial features, clothing, and physical characteristics throughout the entire clip without the annoying shifts that break immersion.\n\n### True 1080p with Cinematic Presets\n\nWhile Veo 3 added 1080p support in September, Veo 3.1 apparently takes this further with built-in cinematic presets. Instead of just getting higher resolution, you’ll have options for film noir, sci-fi, documentary style, and other professional looks right out of the box. This matters because it reduces the post-production work needed to achieve a polished result.\n\n### Longer Videos Change the Game\n\nCurrent limits on Veo 3 cap most generations at eight seconds. Leaked specs indicate Veo 3.1 will support videos up to one minute in length. That’s a huge jump. Sixty seconds gives you enough time to tell an actual story rather than just showing a moment. Think about what you can convey in a full minute versus eight seconds. It opens up entirely new use cases.... ## Technical Improvements Under the Hood\n\nBeyond the flashy new features, Veo 3.1 apparently includes substantial technical refinements. Better prompt understanding means the model actually gets what you’re asking for without requiring perfect technical language. Reduced morphing artifacts should eliminate those weird transitions where objects or people briefly distort.\n\nImproved motion quality addresses another common issue where movement looks unnatural or jerky. Enhanced character tracking across scenes ties into the consistency improvements, ensuring people and objects remain recognizable as they move through your video.\n\nThese aren’t the kinds of upgrades that make for exciting marketing copy, but they’re what separate a tool you use once out of curiosity from one that becomes part of your regular workflow.\n\n## How Veo 3.1 Stacks Up Against Competitors\n\nThe AI video generation market is crowded right now. OpenAI’s Sora 2 emphasizes physical accuracy and realistic dialogue synchronization. Runway’s Gen-3 focuses on speed and iteration. Pika Labs carved out a niche with its editing capabilities. Where does Veo 3.1 fit?\n\nGoogle’s advantage has always been infrastructure and integration. Veo works natively with Gemini’s language models for better prompt understanding. It’s built into Google Cloud through Vertex AI, making it accessible for enterprise applications. The company can leverage its massive compute resources for faster generation times.\n\nIf the leaks are accurate, Veo 3.1 will challenge Sora 2 directly on multi-scene generation while maintaining better integration with existing Google services. For developers and businesses already using Google Cloud, this matters more than raw feature comparisons might suggest.... ## What This Means for Content Creators\n\nAssuming tomorrow’s release happens as rumored, Veo 3.1 could genuinely change how people approach video production. Not by replacing human creativity, but by removing bottlenecks that slow down the creative process.\n\nNeed B-roll footage for a documentary but don’t have the budget for a full shoot? Generate it. Want to prototype a music video concept before investing in production? Mock it up. Testing different visual styles for a client presentation? Try five approaches in an hour.\n\nThe workflow implications extend beyond just making videos faster. When you can quickly test ideas, you take more creative risks. When consistent characters across longer sequences become possible, narrative projects become viable. When you don’t need to manage multiple tools and APIs, you spend more time creating and less time troubleshooting.\n\n## Accessing Veo 3.1 Through GPT Proto\n\nHere’s where platforms like GPT Proto become relevant. Rather than dealing directly with Google’s Vertex AI setup, GPT Proto offers unified access to multiple AI models through a single API. When Veo 3.1 launches, developers using AI API Service should be able to integrate it without major code changes.\n\nThe platform handles the infrastructure complexity, providing stable connections and reliable access without requiring you to become an expert in each model’s specific implementation. For solo developers and small teams, this matters enormously. You can use Veo 3.1 alongside GPT models for text generation, image AI tools, and other capabilities without juggling multiple API keys and billing systems.\n\nGPT Proto’s pay-as-you-go model also removes the pressure of monthly subscriptions. You use what you need when you need it, making it easier to experiment with Veo 3.1 without financial commitment.... ## Predictions for Launch and Beyond\n\nAssuming the October 10 launch happens, expect Google to follow its previous pattern. The first two Veo generations offered free tiers with usage limits. Veo 3.1 will likely continue this approach, giving people a chance to test the technology before committing to paid plans.\n\nIntegration with other Google AI tools seems certain. Expect Veo 3.1 to work seamlessly with Imagen 4 for image generation and Gemini models for prompt enhancement. Google’s strength has always been ecosystem integration, and this release should reinforce that advantage.\n\nFuture features probably include longer video generation, real-time editing capabilities, and better control over specific elements within scenes. The competitive pressure from OpenAI and others ensures that development will continue at a rapid pace.... ## Final Thoughts\n\nWhether Veo 3.1 launches tomorrow as rumored or arrives later with different features, the direction is clear. AI video generation is moving from experimental toy to practical tool. Google’s focus on character consistency, longer videos, and multi-scene generation addresses real creator needs rather than just chasing technical benchmarks.\n\nThe broader significance lies in democratization. Professional video production has always required significant resources. Not just money for equipment, but time for learning complex software and expertise for managing all the technical details. Tools like Veo 3.1 lower these barriers without eliminating the need for creative vision and storytelling skill.\n\nHaving unified access through AI API Platform makes adoption even easier. When you can test multiple AI tools through one interface without complex setup, experimentation becomes natural rather than intimidating.",
            "domain": "gptproto.com"
          },
          {
            "position": 6,
            "title": "Report says Google is about to release VEO 3.1 version on Gemini ...",
            "url": "https://www.aibase.com/news/21990",
            "snippet": "# Report says Google is about to release VEO 3.1 version on Gemini and API\n\n#### AIbase基地Published inAI News · 4 min read · Oct 15, 202518\n\nRecent reports indicate that the public release of VEO3.1 is imminent for Google. With related disclaimers appearing in the Gemini application, Google is showcasing the features of VEO3.1 to a broad user base, which may be implemented within the familiar Gemini interface.\n\nThe post shared by prominent figure Logan Kilpatrick in the community on the social media platform X is widely seen as an early confirmation of Google's new AI product release. Additionally, references to preview models such as \"VEO3.0Generate\" and \"VEO3.0Fast Generate\" have appeared in Vertex AI, indicating that Google is offering multiple access channels for early users and enterprises, in line with their traditional strategy in the release of video generation tools.\n\nDiscussions in the community about output duration are intense, with evidence suggesting that video length may be extended from the previous 8 seconds to 30 seconds, although this remains to be confirmed. Previous leaks indicated that the fast mode has lower quality, while the standard mode is expected to unlock higher output quality, which is particularly important for creators looking to enhance visual quality and narrative potential. According to TestingCatalog, previous 720p video generation has shown improvements in VEO3.1, including new audio capabilities and enhanced visual effects, making it a competitor to similar products like Sora2.\n\nGoogle's overall product strategy is to position Gemini as a central workspace, with VEO models integrated for use by both consumers and enterprise users. Through the preview release on Vertex AI, enterprises can try generating videos, while the mainstream promotion of the Gemini application can reach regular users. This phased release strategy not only maximizes developer feedback but also promotes public acceptance, aligning with Google's ongoing efforts to bridge functional gaps and solidify its position in the generative media field.\n\nKey Points:\n\n🌟 VEO3.1 is about to be released, allowing users to experience its new features in the Gemini application.\n\n🎥 Video duration may be extended from 8 seconds to 30 seconds, providing creators with more narrative space.\n\n🚀 Google's phased release strategy maximizes developer feedback and public acceptance.... #### This article is from AIbase Daily\n\nWelcome to the [AI Daily] column! This is your daily guide to exploring the world of artificial intelligence. Every day, we present you with hot topics in the AI field, focusing on developers, helping you understand technical trends, and learning about innovative AI product applications.",
            "domain": "www.aibase.com"
          },
          {
            "position": 7,
            "title": "Release notes | Gemini API | Google AI for Developers",
            "url": "https://ai.google.dev/gemini-api/docs/changelog",
            "snippet": "This page documents updates to the Gemini API.\n\n## October 17, 2025\n\n**Grounding with Google Maps**is now generally available. For more information, see Grounding with Google Maps documentation.\n\n## October 15, 2025\n\nReleased Veo 3.1 and 3.1 Fast models in public preview, with new features including:\n\n- Extending Veo-created videos.\n\n- Referencing up to three images to generate a video.\n\n- Providing first and last frame images to generate videos from.\n\nThis launch also added more options for Veo 3 output video durations: 4, 6, and 8 seconds.\n\nDeprecation for\n\n`veo-3.0-generate-preview`and\n\n`veo-3.0-fast-generate-preview`coming October 22, 2025.\n\n## October 7, 2025\n\n- Launched Gemini 2.5 Computer Use Preview\n\n## October 2, 2025\n\n- Launched Gemini 2.5 Flash Image GA: Image Generation with Gemini\n\n## September 29, 2025\n\n- The following Gemini 1.5 models are now deprecated:\n\n`gemini-1.5-pro`\n\n`gemini-1.5-flash-8b`\n\n`gemini-1.5-flash`... ## September 9, 2025\n\n- Launched Veo 3 and Veo 3 Fast GA, with lower pricing and new options for aspect ratios, resolution, and seeding. Read the Veo documentation for more information.\n\n## August 26, 2025\n\n- Launched Gemini 2.5 Image Preview, our latest native image generation model.\n\n## August 18, 2025\n\n- Released URL context tool to general\n\navailability (GA), a tool for providing URLs as additional context to\n\nprompts. Support for using URL context with the\n\n`gemini-2.0-flash`model (available during experimental release) will be discontinued in one week.\n\n## August 14, 2025\n\n- Released Imagen 4 Ultra, Standard and Fast models as generally available (GA). To learn more, see the Imagen page.\n\n## August 7, 2025\n\n`allow_adult`setting in Image to Video generation are now available in restricted regions. See the Veo page for details.\n\n## July 31, 2025\n\n- Launched image-to-video generation for the Veo 3 Preview model.\n\n- Released Veo 3 Fast Preview model.\n\n- To learn more about Veo 3, visit the Veo page.... ## July 22, 2025\n\n- Released\n\n`gemini-2.5-flash-lite`, our fast, low-cost, high-performance Gemini 2.5 model. To learn more, see Gemini 2.5 Flash-Lite.\n\n## July 17, 2025\n\nLaunched\n\n`veo-3.0-generate-preview`, the latest update to Veo introducing video with audio generation. To learn more about Veo 3, visit the Veo page.\n\nIncreased rate limits for Imagen 4 Standard and Ultra. Visit the Rate limits page for more details.\n\n## July 14, 2025\n\n- Released\n\n`gemini-embedding-001`, the stable version of our text embedding model. To learn more, see embeddings. The\n\n`gemini-embedding-exp-03-07`model will be deprecated on August 14, 2025.\n\n## July 7, 2025\n\n- Launched Gemini API Batch Mode. Batch up requests and send them to process asynchronously. To learn more, see Batch Mode.\n\n## June 26, 2025\n\nThe preview models\n\n`gemini-2.5-pro-preview-05-06`and\n\n`gemini-2.5-pro-preview-03-25`are now redirecting to the latest stable version\n\n`gemini-2.5-pro`.\n\n`gemini-2.5-pro-exp-03-25`is deprecated.... ## May 7, 2025\n\n- Released\n\n`gemini-2.0-flash-preview-image-generation`, a preview model for generating and editing images. To learn more, see Image generation and Gemini 2.0 Flash Preview Image Generation.\n\n## May 6, 2025\n\n- Released\n\n`gemini-2.5-pro-preview-05-06`, a new version of our most powerful model, with improvements on code and function calling.\n\n`gemini-2.5-pro-preview-03-25`will automatically point to the new version of the model.\n\n## April 17, 2025\n\n- Released\n\n`gemini-2.5-flash-preview-04-17`, a Gemini preview model optimized for price-performance and adaptive thinking. To learn more, see Gemini 2.5 Flash Preview and Thinking.\n\n## April 16, 2025\n\n- Launched context caching for Gemini 2.0 Flash.... ## April 9, 2025\n\n**Model updates:**\n\n- Released\n\n`veo-2.0-generate-001`, a generally available (GA) text- and image-to-video model, capable of generating detailed and artistically nuanced videos. To learn more, see the Veo docs.\n\nReleased\n\n`gemini-2.0-flash-live-001`, a public preview version of the Live API model with billing enabled.\n\n**Enhanced Session Management and Reliability** **Session Resumption:**Keep sessions alive across temporary network disruptions. The API now supports server-side session state storage (for up to 24 hours) and provides handles (session_resumption) to reconnect and resume where you left off. **Longer Sessions via Context Compression:**Enable extended interactions beyond previous time limits. Configure context window compression with a sliding window mechanism to automatically manage context length, preventing abrupt terminations due to context limits. **Graceful Disconnect Notification:**Receive a\n\n`GoAway`server message indicating when a connection is about to close, allowing for graceful handling before termination.\n\n\n\n**More Control over Interaction Dynamics** **Configurable Voice Activity Detection (VAD):**Choose sensitivity levels or disable automatic VAD entirely and use new client events (\n\n`activityStart`,\n\n`activityEnd`) for manual turn control.\n\n**Configurable Interruption Handling:**Decide whether user input should interrupt the model's response. **Configurable Turn Coverage:**Choose whether the API processes all audio and video input continuously or only captures it when the end-user is detected speaking. **Configurable Media Resolution:**Optimize for quality or token usage by selecting the resolution for input media. **Richer Output and Features** **Expanded Voice & Language Options:**Choose from two new voices and 30 new languages for audio output. The output language is now configurable within\n\n`speechConfig`.\n\n**Text Streaming:**Receive text responses incrementally as they are generated, enabling faster display to the user. **Token Usage Reporting:**Gain insights into usage with detailed token counts provided in the\n\n`usageMetadata`field of server messages, broken down by modality and prompt or response phases.... ## April 4, 2025\n\n- Released\n\n`gemini-2.5-pro-preview-03-25`, a public preview Gemini 2.5 Pro version with billing enabled. You can continue to use\n\n`gemini-2.5-pro-exp-03-25`on the free tier.\n\n## March 25, 2025\n\n- Released\n\n`gemini-2.5-pro-exp-03-25`, a public experimental Gemini model with thinking mode always on by default. To learn more, see Gemini 2.5 Pro Experimental.\n\n## March 12, 2025\n\n**Model updates:**\n\n- Launched an experimental Gemini 2.0 Flash model capable of image generation and editing.\n\n- Released\n\n`gemma-3-27b-it`, available on AI Studio and through the Gemini API, as part of the Gemma 3 launch.\n\n**API updates:**\n\n- Added support for YouTube URLs as a media source.\n\n- Added support for including an inline video of less than 20MB.\n\n## March 11, 2025\n\n**SDK updates:**\n\n- Released the Google Gen AI SDK for TypeScript and JavaScript to public preview.\n\n## March 7, 2025\n\n**Model updates:**\n\n- Released\n\n`gemini-embedding-exp-03-07`, an experimental Gemini-based embeddings model in public preview.... ## February 28, 2025\n\n**API updates:**\n\n- Support for Search as a tool\n\nadded to\n\n`gemini-2.0-pro-exp-02-05`, an experimental model based on Gemini 2.0 Pro.\n\n## February 25, 2025\n\n**Model updates:**\n\n- Released\n\n`gemini-2.0-flash-lite`, a generally available (GA) version of Gemini 2.0 Flash-Lite, which is optimized for speed, scale, and cost efficiency.\n\n## February 19, 2025\n\n**AI Studio updates:**\n\n- Support for additional regions (Kosovo, Greenland and Faroe Islands).\n\n**API updates:**\n\n- Support for additional regions (Kosovo, Greenland and Faroe Islands).\n\n## February 18, 2025\n\n**Model updates:**\n\n- Gemini 1.0 Pro is no longer supported. For the list of supported models, see Gemini models.\n\n## February 11, 2025\n\n**API updates:**\n\n- Updates on the OpenAI libraries compatibility.\n\n## February 6, 2025\n\n**Model updates:**\n\n- Released\n\n`imagen-3.0-generate-002`, a generally available (GA) version of Imagen 3 in the Gemini API.\n\n**SDK updates:**\n\n- Released the Google Gen AI SDK for Java for public preview.",
            "domain": "ai.google.dev"
          },
          {
            "position": 8,
            "title": "Google DeepMind's Veo 3 floods internet with realistic videos",
            "url": "https://www.axios.com/2025/05/23/google-ai-videos-veo-3",
            "snippet": "# Google's new AI video tool floods internet with real-looking clips\n\nGoogle's newest AI video generator, Veo 3, generates clips that most users online can't seem to distinguish from those made by human filmmakers and actors.\n\n**Why it matters: **Veo 3 videos shared online are amazing viewers with their realism — and also terrifying them with a sense that real and fake have become hopelessly blurred.\n\n**The big picture: **Unlike OpenAI's video generator Sora, released more widely last December, Google DeepMind's Veo 3 can include dialogue, soundtracks and sound effects.\n\n- The model excels at following complex prompts and translating detailed descriptions into realistic videos.\n\n- The AI engine abides by real-world physics, offers accurate lip-syncing, rarely breaks continuity and generates people with lifelike human features, including five fingers per hand.\n\n- According to examples shared by Google and from users online, the telltale signs of synthetic content are mostly absent.\n\n**Case in point: **In one viral example posted on X, filmmaker and molecular biologist Hashem Al-Ghaili shows a series of short films of AI-generated actors railing against their AI creators and prompts.\n\n**Special effects technology,** video-editing apps and camera tech advances have been changing Hollywood for many decades, but artificially generated films pose a novel challenge to human creators.... - In a promo video for Flow, Google's new video tool that includes Veo 3, filmmakers say the AI engine gives them a new sense of freedom with a hint of eerie autonomy.\n\n- \"It feels like it's almost building upon itself,\" filmmaker Dave Clark says.\n\n**How it works: **Veo 3 was announced at Google I/O on Tuesday and is available now to $249-a-month Google AI Ultra subscribers in the United States.\n\n**Between the lines: **Google says Veo 3 was \"informed by our work with creators and filmmakers,\" and some creators have embraced new AI tools. But the spread of the videos online is also dismaying many video professionals and lovers of art.\n\n- Some dismiss any AI-generated video as \"slop,\" regardless of its technical proficiency or lifelike qualities — but, as Axios' Ina Fried points out, AI slop is in the eye of the beholder.\n\n- The tool could also be useful for more commercial marketing and media work, AI analyst Ethan Mollick writes.\n\n**It's unclear how Google trained Veo 3 **and how that might affect the creativity of its outputs.\n\n- 404 Media found that Veo 3 generated the same lame dad joke for several users who prompted it to create a video of a man doing stand-up comedy.\n\n- Likewise, last year, YouTuber Marques Brownlee asked Sora to create a video of a \"tech reviewer sitting at a desk.\" The generated video featured a fake plant that's nearly identical to the shrub Brownlee keeps on his desk for many of his videos — suggesting the tool may have been trained on them.\n\n**What we're watching:** As hyperrealistic AI-generated videos become even easier to produce, the world hasn't even begun to sort out how to manage authorship, consent, rights and the film industry's future.\n\n##### Go deeperJul 10, 2025 - Technology... ## Google AI's new trick: Turn any image into a brief video\n\nGoogle's latest AI video tool, Veo 3, now generates short movies with sound based only on still photos and prompts.\n\nGo deeper (1 min. read)\n\n**The big picture: **The feature, released Thursday, is available to Ultra and Pro users on the web and soon on mobile for subscribers in select regions, Google shared with Axios.\n\n## Google avatars shake up workplace video making\n\nGoogle Vids is now providing users of the workplace video creation tool with a set of pre-made avatars for use in brief AI-generated videos, the company said Wednesday.\n\nGo deeper (2 min. read)\n\n**Why it matters: **The rise of cheap, convenient AI video generation threatens jobs for video producers, editors, camera operators and even commercial actors.\n\n## AI slop is ruining all of our favorite places to scroll\n\nAn AI-generated video of rabbits jumping on a trampoline that went viral this week — and was widely believed to be real — proved even cute animal vids aren't safe from convincing slop machines.\n\nGo deeper (2 min. read)\n\n**Why it matters: **All the fake AI-generated content online is sapping the joy of casual scrolling.",
            "domain": "www.axios.com"
          },
          {
            "position": 9,
            "title": "Google's Veo 3.1 is better at generating videos from images",
            "url": "https://www.engadget.com/ai/googles-veo-31-is-better-at-generating-videos-from-images-220829129.html",
            "snippet": "Google has released a new update to its Veo AI video generation model that should make it do a better job of sticking to prompts and converting images into videos. Veo 3.1 is available to try today through Google's Gemini API and is now also powering the company's Flow video editor.\nVeo 3.1 builds on the new capabilities Google introduced with launch of Veo 3 at Google I/O 2025. The new model offers better \"prompt adherence,\" according to Google, and should have an easier time creating videos based on the image \"ingredients\" you upload alongside your written prompt. Veo 3.1 also makes it possible to convert images to video and generate audio at the same time, a capability that wasn't available with Veo 3.\nIn Flow, Veo 3.1 supports at least a new feature that gives you finer control over the videos you generate. With what Google calls \"Frame to Video,\" Flow lets you upload a first and last frame, and then generates the video in-between. Adobe Firefly, which is powered by Veo 3, offers a similar feature, but Flow will be able to pull it off and create audio at the same time. Those added audio skills will also apply to the video editor's ability to extend clips and insert objects into existing footage, too.\nBased on the samples Google's shared, videos generated with Veo 3.1 still have an uncanny quality that seems to vary greatly depending on the prompt and subject. Even if it's missing some of the realism of OpenAI's Sora 2, though, the company's decision to try and make Veo more useful to people who actually work with video rather than a source of social media spam is a welcome move.",
            "domain": "www.engadget.com"
          },
          {
            "position": 10,
            "title": "Veo 3.1 is coming(and what's rumor): what we know and What it will ...",
            "url": "https://www.cometapi.com/veo-3-1-is-comingand-whats-rumor/",
            "snippet": "# Veo 3.1 is coming(and what’s rumor): what we know and What it will bring?\n\nVeo 3.1 is Coming:\n\n**Veo** is Google’s family of AI video-generation models (Veo 3 / Veo 3 Fast are current). Google has recently shipped big Veo 3 improvements (vertical 9:16, 1080p, Veo 3 Fast, lower pricing) and there are **rumors / social posts** that **Veo 3.1** is imminent — but Google has **not** published an official Veo 3.1 release bulletin yet. I’ll list confirmed facts, likely/expected changes, and a direct comparison to OpenAI’s **Sora 2**.\n\n## What\n\n**Veo** is\n\n**Veo** is Google’s line of generative video models (DeepMind / Google Cloud / Gemini family) that turn text or images into short videos — and (in Veo 3) generate audio natively (sound effects, ambient audio, and dialogue). It’s offered on Google Cloud (Vertex AI / Gemini API) for developers and enterprises, and includes built-in provenance / SynthID watermarks on outputs.\n\n## What\n\n**Veo 3** already brought **Text → video**and **image → video**capabilities (including preview image-to-video). **Native audio generation**(music, ambient sounds, dialogue) — Veo 3 introduced first-class audio. **Two variants**: high-quality Veo 3 and **Veo 3 Fast**(optimized for speed/iteration). **Platform availability:**made available in Vertex AI / Gemini API (paid preview → general availability updates in mid-2025). **Safety/provenance:**SynthID watermarking and some generation use controls/approval for person/child generation.... ## So — what is\n\n**Veo 3.1** expected to bring?\n\n**Status:** *As of now there is no official Veo 3.1 product page from Google describing full release notes.* However, multiple Google dev posts / community posts and tweets indicate a near-term incremental update (labelled “Veo 3.1”) that’s expected to focus on iterative improvements to audio, quality, and format support rather than a full new-generation rewrite.\n\nHere are some inferences I made based on x’s post and the characteristics of Veo3:\n\n**Improved native audio (dialogue, multi-voice lip sync)**—cleaner dialogue, better SFX mixing and spatialization). Veo 3 already generates audio natively; Veo 3.1 could improve dialogue realism and language support to match recent improvements competitors are shipping. **Faster/cheaper paths**for some common outputs (more Veo 3 Fast parity and optimizations). **Improved image→video fidelity and better character/pose consistency**in multi-frame clips. **Expanded aspect ratios / resolution controls**(more flexible 9:16/16:9 and 1080p across configs). Google already added vertical + 1080p; Veo 3.1 could expand those controls. **Longer clips / relaxed 8-second cap**— community demand and Google’s previous roadmap suggest increased duration is a likely target (Veo 3 today is optimized for 8-second clips). **Better image→video fidelity and extended image-to-video support**(improvements to realism, motion continuity), building on the image→video preview in Veo 3.... ## Compare Veo 3 / (expected) Veo 3.1 → OpenAI Sora 2\n\n### Primary focus\n\n**Veo 3 (Google)**: short, high-fidelity 8-second videos from text/image prompts; native audio; integrated into Gemini/Gemini API and Vertex AI; optimized for production use and developer API integration. **Sora 2 (OpenAI)**: OpenAI’s flagship video+audio model emphasizing physical realism, coherent motion, synchronized dialogue and sound, and an accompanying social app (Sora) with a cameo/consent system for integrating user likenesses and focuses heavily on realism and safety controls.\n\n### Strengths\n\n**Veo (now)**: strong developer/enterprise integration (Vertex AI, Gemini API), production pricing options, clear path for cloud customers, vertical/1080p + fast variant. Good for businesses building into pipelines. **Sora 2**: remarkable physical accuracy and multi-modal sync (dialogue + visuals), and a consumer-facing app integrated with social workflows (cameo feature, moderation). Great for creators wanting realistic narrative scenes and an app ecosystem.\n\n## How to access Veo now — and how to be ready for Veo 3.1\n\n**Try in Gemini (consumer / web / mobile)**: Veo generation is exposed in the Gemini apps (tap the “video” option in the prompt bar). Access level (Pro / Ultra) affects which Veo variants you can use. **Programmatically / enterprise**: use **API**in CometAPI (Veo model IDs available in the model docs). CometAPI provides veo3-pro, veo3-fast and veo3. For details, please refer to Veo 3 ‘s doc.\n\n**Practical tip (developer):** to request vertical output, set the\n\n`aspectRatio` parameter (e.g.\n\n`\"9:16\"`) and check the model configuration (Veo 3 vs Veo 3 Fast) and your plan for resolution limits (720p vs 1080p).... ## How to access Sora 2 (today)\n\n**Sora app:** Sora 2 launched with a Sora app (invite-limited rollout in US & Canada at launch). OpenAI indicated broader access and API expansion later. If you want to try Sora 2 now, check CpmetAPI’s Sora 2 page. CometAPI has already supported sora 2 API, and generates ~10-second social clips and an emphasis on motion realism for people.\n\n## Getting Started\n\nCometAPI is a unified API platform that aggregates over 500 AI models from leading providers—such as OpenAI’s GPT series, Google’s Gemini, Anthropic’s Claude, Midjourney, Suno, and more—into a single, developer-friendly interface. By offering consistent authentication, request formatting, and response handling, CometAPI dramatically simplifies the integration of AI capabilities into your applications. Whether you’re building chatbots, image generators, music composers, or data‐driven analytics pipelines, CometAPI lets you iterate faster, control costs, and remain vendor-agnostic—all while tapping into the latest breakthroughs across the AI ecosystem.\n\nDevelopers can access Veo 3.1 API through CometAPI, the latest model version is always updated with the official website. To begin, explore the model’s capabilities in the Playground and consult the API guide for detailed instructions. Before accessing, please make sure you have logged in to CometAPI and obtained the API key. CometAPI offer a price far lower than the official price to help you integrate.\n\nReady to Go?→ Sign up for CometAPI today !",
            "domain": "www.cometapi.com"
          }
        ],
        "success": true,
        "error": null
      },
      {
        "query_id": "q9",
        "query": "veo3.1 Google software version 2025 false",
        "claim_id": "claim_1",
        "query_type": "contradiction",
        "priority": "high",
        "results": [
          {
            "position": 1,
            "title": "Gemini AI video generator powered by Veo 3.1",
            "url": "https://gemini.google/overview/video-generation/",
            "snippet": "# Break the\n\n**silence** with Veo 3.1\n\nCreate high-quality, 8-second videos with Veo 3.1, our latest AI video generator. Simply describe what you have in mind or upload a photo and watch your ideas come to life with native audio generation. Try it with a Google AI Pro plan or get the highest access with the Ultra plan.\n\n**Veo 3.1** speaks for itself\n\n## Dream it. Describe it.\n\n**Done.**\n\n## For Exploring\n\nPlay with diverse styles, bring animated characters to life, and combine objects in ways you never thought possible. See what you can create using text to video with AI.\n\n## For Sharing\n\nCreate funny memes, turn inside jokes into videos, re-imagine special moments, and add a personal touch to make someone smile.\n\n## For Brainstorming\n\nBreak through creative blocks and visualize your ideas in a flash. From product concepts and designs to rapid prototyping and storytelling, Gemini can help.\n\n## Learn more about our\n\n**Veo Models**\n\nCreate videos with sound using our video generation model that maintains high-quality while optimizing for speed.\n\nCreate high-quality, 8-second videos with sound using our state-of-the-art video generation model.... ## Frequently asked questions\n\nYes, you can create and share videos in your mobile Gemini app. To create videos, tap the \"video\" button in your prompt bar. If you don't see it, tap the button with three dots to view more options.\n\nTry Veo 3.1 Fast with a Google AI Pro plan or get the highest access to Veo 3.1 in Google AI Ultra. Country availability here.\n\nFor now, the ability to generate a video from a photo is not available in the European Economic Area, Switzerland, or the United Kingdom.\n\nWe’ve taken several important safety steps to make AI video generation a safe experience. This includes extensive red teaming and evaluation aimed at preventing the generation of content that violates our policies. Additionally, all videos generated with Veo in the Gemini app are marked with a visible watermark and SynthID, a digital watermark embedded in each frame, which indicates the videos are AI-generated.\n\nGemini's outputs are primarily determined by user prompts and like any generative AI tool, there may be instances where it generates content that some individuals find objectionable. We’ll continue to listen to your feedback through the thumbs up/down buttons and make ongoing improvements. For more details, you can read about our approach on our website.\n\nResults for illustrative purposes and may vary. Internet and subscription for certain features required. Available to users 18+. Create responsibly.",
            "domain": "gemini.google"
          },
          {
            "position": 2,
            "title": "Google's Veo 3 Update! July 2025!",
            "url": "https://www.youtube.com/watch?v=qhReJkSRKOc&vl=en",
            "snippet": "## Murray Frost\n##### Jul 09, 2025 (0:04:15)\n✅ Build a Monetized YouTube Channel in 90 days: https://murrayfrost.com/YT-Accelerator\n\nI teach people YouTube from REAL data from over 150 clients and my own channels. Data-driven feedback and strategies. None of this guessing garbage people put on online teaching you how to do YouTube.... {ts:0} So, Google Labs just had an update and it's technically called Flow. It's on\n{ts:4} their labs.google platform and you can see by the beginning of this video that it does still need some work, but you\n{ts:9} haven't been able to do that recently with Google Labs. And now you can do it with Google's Vo3. So, there's been a\n{ts:15} couple updates here that you can see. And the first one here is using images or allowing images to talk with Google's\n{ts:20} VO3, which again still needs some work, but look, it gets maybe 60% of the way there. I think it looks pretty decent.\n{ts:29} I'm obviously not going to use it to try and convince people that it's real, but maybe you can get kind of creative with\n{ts:34} this and get people to make some really funny, strange, or just dumb things that people love. I don't know why people\n{ts:42} just love brain rot these days. Now, they're also adding the option to do this with V V3 on frames to video, which\n{ts:49} is actually kind of cool. And they... 're also allowing you to top up your subscription with the kind of a mid tier\n{ts:56} option cuz previously they had just the the starter which was about 20 bucks or so per month in the US and then it was\n{ts:63} straight up to I think 250 without the discount for the first 3 months and there's just no in between. It's just a\n{ts:70} massive jump. So they added like a kind of a mid tier there. I think a couple too. Well, I'll show you what that looks\n{ts:75} like and you can top up your credits there as well. They've also gone ahead and just added better audio coverage,\n{ts:83} which I haven't really noticed all that much to be honest. Right now, there's not a huge difference as at least a\n{ts:89} noticeable difference in my opinion from me using it. Um, they also do remove audio when miners are involved. Keep\n{ts:96} that in mind. That's why your audio isn't being generated if you have kids in the video or maybe uh even teenagers\n{ts:101} sometimes. Um, but then there's also they've mentioned they're reducing um unwanted subtitles, which is actually... {ts:107} quite nice. They've been removing the the VEO watermark as well, but now they said they've reduced the unwanted\n{ts:114} subtitles. I still get them sometimes. So, I literally in caps specify in the prompt to not include captions because\n{ts:121} otherwise if I don't, sometimes the captions still show up. And the really nice quality of life update they've made\n{ts:127} here is that when you are just starting a new project or revisiting an existing one, it doesn't reset the model that you\n{ts:135} have, or at least if it does, it resets to the VO3 fast beta audio. So, this is where you're generating audio. So now\n{ts:142} you don't accidentally have VO2 selected with no audio every time you either reload a page, start a new project, or\n{ts:150} you leave and come back and it's been reset to just its default VO2. Now it's actually its default is V3 fast beta\n{ts:157} audio. So the 20 credits per generation, the cheaper VO3 option with audio. So now you don't accidentally generate\n{ts:164} videos without audio, which has happened a lot to me and I... 've wasted thousands of credits doing that by accident. So,\n{ts:171} great update right there. Quality of life, which you don't have to waste any more credits. Now, now let's say that\n{ts:176} you don't want to spend the $124 per month, and this is for the first 3 months. Then, it goes to 150, I believe,\n{ts:183} per month. So, I'm probably going to cancel it at that point because that's just really expensive, at least using\n{ts:189} VO3. Now you have the option if you have the uh let's see which was it the pro subscription the $20 a month\n{ts:195} subscription right here Google AI pro you still get a th000 credits per month in uh Google labs but you also have the\n{ts:204} option to top up your credits so for example in here when you're creating your AI videos if you run out of credits\n{ts:211} like here you can just hit get more AI credits and you can choose how much you want to add so you don't have to spend\n{ts:216} $200 or $150 at a time you could spend an extra for 24 bucks that month just to top up your credits. This wasn... 't\n{ts:223} available in the lower plans. It was only available in the the maximum ultra tier. So, I really like the ability to\n{ts:230} do that now. So, you don't have to spend so much all at once, especially if you're not using all your credits at the\n{ts:235} end of each month, but maybe one month you're just out and you need to add more, you can do so and add some\n{ts:240} credits, which is actually quite nice. I've hit this button a little too much recently, but uh hey, I'm getting good\n{ts:247} results. So, those are all the current updates with Google's Veo3 with their labs platform. Hope to see you in the\n{ts:253} next one.",
            "domain": "www.youtube.com"
          },
          {
            "position": 3,
            "title": "Build with Veo 3, now available in the Gemini API",
            "url": "https://developers.googleblog.com/en/veo-3-now-available-gemini-api/",
            "snippet": "First unveiled at Google I/O 2025, people around the world have already generated tens of millions of high-quality videos with Veo 3 (along with some new fun and interesting video trends). It is our first video model to incorporate high-fidelity video outputs and native audio, first with text-to-video and soon with image-to-video.\n\nDevelopers are already experimenting with Veo 3, discovering how the model can help them brainstorm content, rapidly iterate, and be more efficient.\n\nVeo 3 is designed to handle a range of video generation tasks, from cinematic narratives to dynamic character animations. With Veo 3, you can create more immersive experiences by not only generating stunning visuals, but also audio like dialogue and sound effects.\n\nLet’s take a look at some examples.\n\nExplore these examples and more with Veo 3 in Google AI Studio, available as an SDK template and interactive Starter App to remix, copy and extend. The Starter App and its sample code offer a convenient way for Paid Tier users to rapidly prototype with Veo 3 and more on the Gemini API, directly from Google AI Studio.\n\nClick the Key button in the top right of the AI Studio Build interface to select a Google Cloud Project with billing enabled to use the Paid Tier in AI Studio apps. See the FAQs for more.... Veo 3 will be priced at $0.75 per second for video and audio output. Additionally, Veo 3 Fast will be available soon, offering a faster and more cost-effective option for video creation.\n\nHere’s a basic Python example to create a video:\n\n```\n\nimport time\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.0-generate-preview\",\n\nprompt=\"a close-up shot of a golden retriever playing in a field of sunflowers\",\n\nconfig=types.GenerateVideosConfig(\n\nnegative_prompt=\"barking, woofing\",\n\n),\n\n\n\n# Waiting for the video(s) to be generated\n\nwhile not operation.done:\n\ntime.sleep(20)\n\noperation = client.operations.get(operation)\n\ngenerated_video = operation.result.generated_videos[0]\n\nclient.files.download(file=generated_video.video)\n\ngenerated_video.video.save(\"veo3_video.mp4\")\n\n```\n\nAll videos generated by Veo 3 models will continue to include a digital SynthID watermark. To get started, check out the documentation, cookbook, and a Veo 3 starter app in Google AI Studio:\n\nIn addition to being available via the Gemini API in Google AI Studio, Veo 3 is also available to Google AI subscribers in the Gemini app and Flow, and to enterprise customers via Vertex AI.... 1: Fluffy Characters Stop Motion: Inside a brightly colored, cozy kitchen made of felt and yarn. Professor Nibbles, a plump, fluffy hamster with oversized glasses, nervously stirs a bubbling pot on a miniature stove, muttering, \"Just a little more... 'essence of savory,' as the recipe calls for.\" The camera is a mid-shot, capturing his frantic stirring. Suddenly, the pot emits a loud \"POP!\" followed by a comical \"whoosh\" sound, and a geyser of iridescent green slime erupts, covering the entire kitchen. Professor Nibbles shrieks, \"Oh, dear! Not again!\" and scurries away, leaving a trail of tiny, panicked squeaks.\n\n2: The sequence begins with an extreme close-up of a single gear, slowly turning and reflecting harsh sunlight. The camera gradually pulls back in a continuous movement, revealing this is but one component of a colossal, mechanical heart half-buried in a desolate, rust-colored desert. A sweeping aerial shot establishes its enormous scale and isolation in the barren landscape. The camera descends to capture pipes hissing steam and the rhythmic thumping that echoes across the empty plains. A subtle shake effect synchronizes with each massive heartbeat. A lateral tracking shot discovers tiny, robed figures scurrying across the metallic surface. The camera follows one such figure in a detailed tracking shot as they perform meticulous maintenance, polishing brass valves and tightening immense bolts. A complex movement circles the entire structure, capturing different maintenance teams working in precarious positions across its rusted exterior. The final shot begins tight on the meticulous work of one tiny figure before executing a dramatic pull-out that reveals the true scale of the heart and the minuscule size of its caretakers, tending to the vital organ of an unseen, sleeping giant that extends beyond the frame.",
            "domain": "developers.googleblog.com"
          },
          {
            "position": 4,
            "title": "Google rolls out its new Veo 3 video-generation model ...",
            "url": "https://techcrunch.com/2025/07/03/google-rolls-out-its-new-veo-3-video-generation-model-globally/",
            "snippet": "Google on Thursday said it has begun rolling out its Veo 3 video generation model to Gemini users in more than 159 countries.\n\nVideo generation via the new model is available only to paying subscribers of Google’s AI Pro plan and is capped at three videos per day.\n\nVeo 3, which Google showed off in May, lets users generate videos up to eight seconds long using text prompts.\n\nGoogle’s Josh Woodward has said that the company is working on adding image-to-video generation capabilities to Gemini.",
            "domain": "techcrunch.com"
          },
          {
            "position": 5,
            "title": "ATUALIZAÇÃO GOOGLE VEO 3 (CONSISTÊNCIA LIBERADA)",
            "url": "https://www.youtube.com/watch?v=if7FJuFD9Es",
            "snippet": "## Anderson Mak\n##### Jul 08, 2025 (0:08:13)\nATUALIZAÇÃO GOOGLE VEO 3 (CONSISTÊNCIA LIBERADA)... {ts:0} Hoje eu vim mostrar como o nosso voto valeu a pena. Vejam essas ruas todas\n{ts:6} reformadas. Gente, aquele safado enganou a gente. Olha a cratera que tem aqui na rua. Eu\n{ts:12} vou postar esse vídeo. Toda a cidade precisa saber a verdade. Pessoal, agora ficou muito fácil você\n{ts:20} manter consistência na criação dos personagens que você utiliza aqui no Google V 3. Então você pode ver que esse\n{ts:27} vídeo que eu criei aí no começo, eu fiz aqui três cenas e ele manteve aqui totalmente\n{ts:36} ou pelo menos 99% a as características do personagem, né? Você pode ver aí o que realmente parece\n{ts:43} a mesma pessoa, né? E como que isso aqui tá sendo feito agora? Isso tá sendo possível porque agora você consegue\n{ts:52} colocar uma imagem. Então você coloca a imagem de referência aqui, ó. Tá? Eu tô aqui na no flow, na ferramenta flow. E\n{ts:63} aqui você pode fazer upload de imagens. Você pode ver que eu coloquei aqui algumas imagens.... {ts:68} Então o segredo tá você manter a consistência na criação da imagem. Então quando você for criar a imagem, eu vou\n{ts:74} mostrar como que eu criei aqui. Você mantendo a consistência na imagem, você joga a imagem aqui e na hora de fazer a\n{ts:80} criação do prompt, você coloca a imagem como referência. Você pode ver que ele permite você colocar a primeira imagem\n{ts:87} do primeiro frame aqui, tá? Então essa imagem que será utilizada. Ele tem aqui o recurso da segunda\n{ts:94} imagem, porém quando você coloca a segunda imagem, tipo a imagem inicial e a imagem final, ele não permite você\n{ts:100} colocar voz, tá? Esse recurso aqui de colocar imagem já estava disponível, porém quando você colocava aqui uma\n{ts:107} imagem, ele mudava pra versão dois, ou seja, sem áudio. Agora você já consegue na versão três, inclusive nessa fest,\n{ts:116} colocar a imagem como referência aqui, qualquer imagem que você fizer upload aqui como referência,\n{ts:123} colocar o prompt e aí criar aqui inclusive com a fala, com áudio e tudo mais que vai funcionar perfeitamente.... {ts:130} Então, olha que interessante esse recurso que tá disponível aqui. Bom, como que eu criei esse personagem e como\n{ts:137} que eu fiz para manter uma certa consistência deste personagem? Então, eu utilizei aqui o chat GPT e\n{ts:147} aqui eu coloquei algumas características. Deixa eu mostrar aqui, ó.\n{ts:157} Eu coloquei essas características aqui, né? Eu coloquei, ó, cria um prompt de um homem caucasiano, cabelos loiros, olhos\n{ts:165} azuis, 30 anos de idade, com calça jeans, curta, eh, e camiseta verde. Ele está caminhando pela rua estilo vlog.\n{ts:176} Então, eu peguei essas características aqui e criei um prompt\n{ts:185} 3, tá? E eu mandei ele também criar um promit para Me Journey, tá? Você pode criar aqui inclusive no chat GPT, tá?\n{ts:194} Aqui eu fui criar aqui, mas deu um pequeno erro e tava demorando. Então eu criei lá no chat GPT, mas você pode é no\n{ts:200} Mid Journey, você pode criar também aqui no chat GPT que também vai funcionar. Então ele deu esse prompt aqui para mim.... {ts:207} Aí eu entrei aqui no mid journey e executei esse prompt e criei a primeira imagem, tá? Deixa eu mostrar aqui\n{ts:217} que foi essa aqui. Eu peguei essa aqui, tá? Então ele criou aqui quatro versões e aí eu baixei esta imagem, tá? Para o\n{ts:224} mid journey. E uma vez que você baixa a imagem para criar as outras imagens aqui no mid\n{ts:231} journer, eu tô utilizando um recurso que é você você pode clicar na imagem que tá no seu computador ou pegar uma dessas\n{ts:238} imagens que você já utilizou aqui, ó. Clica e arrasta e coloca nessa opção aqui, ó.\n{ts:244} você vai utilizar como um personagem de referência, tá? Ó, eu coloquei ele aqui para ele utilizar e aí eu coloco\n{ts:253} qualquer prompt dele em outros lugares, outras cenas. Então, eu tenho a primeira cena aqui,\n{ts:260} né, a primeira imagem e aí pedi também para ele criar aqui o o prompt para o Google Viow, tá? Que foi esse aqui, né?\n{ts:269} Aí peguei tal e aqui joguei o prompt e também coloquei a imagem de referência, tá? Então aí ele fez esse primeiro vídeo... {ts:276} aqui e aí o segundo vídeo é a mesma coisa, né? Coloquei a mesma imagem com uma\n{ts:282} referência e mandei ele fazer aqui a ação dele ali caindo no buraco e tal. E o terceiro vídeo\n{ts:290} eu criei colocando como imagem de referência aquela outra imagem que ele tá ali no escritório, né? Deixa eu\n{ts:296} mostrar aqui. Então, eu pedi pro chatt fazer outro prompt para mid journey agora com ele no escritório, né, que é\n{ts:304} esse aqui, ó. E aí eu joguei aqui e coloquei nessa opção de referência aqui, ó, no M Journey. E aí ele colocou o\n{ts:313} mesmo personagem aqui no escritório, tá? Aí eu escolhi uma dessas que eu achei que ficou\n{ts:320} melhor. Baixei e utilizei aqui fazendo aqui, né? Ó, peguei no caso este aqui, seleciona\n{ts:330} aqui, coloca o prompt. O prompt você pode pedir lá para o mid journey também para ele falar o que você quer que ele\n{ts:338} fale e tudo mais. E aí você executa e bum, ele mantém a consistência. Então, olha como que ficou... {ts:345} fácil aqui. Agora, através dessa ferramenta que é o flow, né? Você consegue manter a consistência nessa\n{ts:353} ferramenta aqui. Muito bacana, né? Lembrando que eu utilizei aqui o mid jour Journey, mas você pode pedir direto\n{ts:359} para o chat GPT, se você quiser. Pede para ele fazer a primeira imagem, dá ali as características, né? Conforme eu\n{ts:365} coloquei aqui, ó. Ó, homem caucasiano, cabelos loiros, olhos azuis, 30 anos de idade, coloca detalhes da roupa\n{ts:372} e aí manda ele manter a consistência, manda ele fazer aqui uma primeira imagem e na segunda imagem que você for criar\n{ts:379} no chat GPT, você pode falar, né? Mantém a consistência e você pode, inclusive fazer o upload da própria imagem que\n{ts:384} você salvou, fez a primeira imagem, salvou, envia ela como referência e manda ele manter as características do\n{ts:390} personagem e fazer uma segunda cena, fazer em outro lugar, que aí você vai conseguir gerar a segunda cena também\n{ts:397} aqui é no chatt. No meu caso, eu utilizei aqui o mid journey, porque eu tenho aqui a assinatura do midjour... {ts:402} Journey, então para mim fica mais fácil utilizar esse recurso de Omni reference aqui, ó, que é uma referência de\n{ts:410} personagem, né? E aí eu coloco em qualquer cenário, qualquer lugar que eu quiser, o mesmo personagem, inclusive\n{ts:418} com a mesma roupa, né? Roupa bem parecida aqui. Beleza? Então, maravilha. Agora para você manter consistência\n{ts:425} ficou muito fácil, né? aqui utilizando esse novo recurso de colocar aqui a primeira imagem e aqui você coloca o seu\n{ts:431} prompto. Lembrando que quando você entra aqui vai estar desta forma texto para vídeo, tá? Para ficar bilitado, você\n{ts:438} clica aqui e altera para transformar frames em vídeo, tá? Coloca nesta opção aqui e aí sim você consegue colocar o\n{ts:445} primeiro frame. Se você colocar o segundo frame, deixa eu colocar por exemplo esse aqui.\n{ts:451} Você pode ver, você coloca aqui o primeiro frame e o último frame. Você pode ver que vai dar\n{ts:458} um erro, ó. Quando você for tentar executar, ele vai dizer que você tem que alterar pra versão dois. Então ele não... {ts:463} tá permitindo ainda você utilizar o último frame, apenas o primeiro. Então é este aqui é o segredo, tá? manter apenas\n{ts:469} o primeiro frame e aí o resto aqui vai funcionar tranquilamente. Beleza? Então nesse vídeo era isso. Espero que tenham\n{ts:474} gostado aí dessa dica. Ficando por aqui. Forte abraço. Até um próximo vídeo. Falou.\n{ts:478} [Música]",
            "domain": "www.youtube.com"
          },
          {
            "position": 6,
            "title": "Veo 3 available for everyone in public preview on Vertex AI - Google Cloud",
            "url": "https://cloud.google.com/blog/products/ai-machine-learning/veo-3-available-for-everyone-in-public-preview-on-vertex-ai",
            "snippet": "# You dream it, Veo creates it: Veo 3 is now available for everyone in public preview on Vertex AI\n\n##### Jason Gelman\n\nDirector, Product Management, Vertex AI\n\n##### Try Gemini 2.5\n\nOur most intelligent model is now available on Vertex AITry now\n\nA great story doesn't just tell you, it shows you. With Veo 3, we’ve leapt forward in combining video and audio generation to take storytelling to the next level.\n\nToday, we’re excited to share that Veo 3 is now available for all Google Cloud customers and partners in public preview on Vertex AI.\n\n**Why this matters: **Veo 3 is your partner for creating near-cinematic quality generative video, moving beyond novelty to narrative-driven creation. It not only brings stunning visual quality, but now adds sound from background sounds to dialogue. With Veo 3 on Vertex AI, you can take advantage of three powerful new capabilities:\n\n\n\n**Fluid, natural videos that synchronize video with audio and dialogue.**Veo 3 can synchronize your audio and visuals in a single pass. The model produces rich soundscapes containing everything from dialogue and ambient noise, to sound effects and background music.\n\n\n\n**Cinematic video that captures creative nuances.**Veo 3 makes it easy to capture creative nuances and detailed scene interactions in your prompt, from the shade of the sky to the precise way the sun hits water in the afternoon light, and produces high-definition video.\n\n\n\n**Realistic movement that simulates real-world physics.**To create believable scenes, Veo 3 simulates real-world physics. This results in realistic water movement, accurate shadows connected with objects and characters, and natural human motion.... ### Businesses are already using Veo to make creating easier\n\nVeo 3 is helping Google Cloud customers create external content – from social media ads to product demos – and internal materials like training videos and presentations. Hear directly from the teams:\n\n“Veo 3 has marked the difference within the gen AI industry, and we’re glad that Freepik users have been some of the first to try the model out. The quality of the video generations combined with the audio integration option is the game changer in our AI Suite. We look forward to continuing this collaboration to bring the best AI tools and features to our users” – Omar Pera, CPO, Freepik\n\n“Creativity is deeply personal, and our goal is to build a platform that adapts to every workflow. By working with Google, we’re combining the best technologies to give creators more control, efficiency, and power than ever before. Our collaboration with Google Cloud represents a strategic evolution that will not only enhance accessibility and efficiency but fundamentally transform how people create. We believe the future of generative video technology will leverage the best technologies to build the most flexible and accessible tools. This is an exciting step toward realizing that vision” – Zeev Farbman, Co-Founder & CEO, Lightricks.\n\n\"Veo 3 is the single greatest leap forward in practically useful AI for advertising since genAI first broke into the mainstream in 2023. By allowing brands to make fully fledged films from a single prompt - including brand, story, video, sound effects, voiceovers and more - Veo3 in one swoop lowers the barriers to entry to gen AI for creative people and elevates gen AI to a top tier brand building tool usable at every stage of the marketing funnel.\" – Will Hanschell, co-founder and CEO, , Pencil\n\n**Bring your vision to life with Veo 3 today**\n\nVeo 3 on Vertex AI is built for scalable enterprise use with crucial guardrails like safety filter controls and SynthID to ensure responsible deployment for any use case. To get started, go here to learn more about Veo 3 on Vertex AI and try it on Vertex AI Media Studio. Get started today!",
            "domain": "cloud.google.com"
          },
          {
            "position": 7,
            "title": "Introducing Veo 3.1 and new creative capabilities in the Gemini API",
            "url": "https://developers.googleblog.com/en/introducing-veo-3-1-and-new-creative-capabilities-in-the-gemini-api/",
            "snippet": "**Today, we are releasing Veo 3.1 and Veo 3.1 Fast in paid preview in the** **Gemini API** **.** This updated model offers several improvements, as well as improved outputs when generating video from images. These new models are available via the Gemini API in Google AI Studio and Vertex AI. Veo 3.1 is also available in the Gemini app and Flow.\n\nVeo 3.1 and Veo 3.1 Fast empower developers to create more engaging content through significant upgrades. The models now generate\n\n**richer native audio**, from natural conversations to synchronized sound effects, and offer **greater narrative control** with an improved understanding of cinematic styles. **Enhanced image-to-video** capabilities ensure better prompt adherence while delivering superior audio and visual quality and maintaining character consistency across multiple scenes.\n\nIn addition to the model update, we are introducing new model capabilities, including using reference images to guide video generation, extend existing Veo videos, and generate transitions between a first and last frame.\n\nYou can now guide the generation process by providing up to 3 reference images of a character, object, or scene. This is helpful for maintaining character consistency across multiple shots or applying a specific style to your video.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nconfig=types.GenerateVideosConfig(\n\nreference_images=[reference_image1, reference_image2],\n\n),\n\n\n\n```... Your story is no longer limited by the original generation. With Scene extension, you can create longer videos, even lasting for a minute or more, by generating new clips that connect to your previous video. Each new video is generated based on the final second of the previous clip. This maintains visual continuity, making the feature ideal for extending shots with background audio.\n\n```\n\nfrom google import genai\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nvideo=video_to_extend\n\n\n\n```\n\nCreate smooth, natural scenes that bridge two different images. By providing a starting and an ending image, you can direct Veo 3.1 to generate the transition between them, complete with accompanying audio.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nimage=first_frame,\n\nconfig=types.GenerateVideosConfig(\n\nlast_frame=last_frame,\n\n),\n\n\n\n```\n\n**What people are building with Veo 3.1**\n\nPromise Studios, a GenAI movie studio, uses Veo 3.1 within its MUSE Platform to enhance generative storyboarding and previsualization for director-driven storytelling at production quality.... Latitude is experimenting with Veo 3.1 in its generative narrative engine to instantly bring user-created stories to life.\n\nVeo 3.1 and these new capabilities are now available in paid preview via the Gemini API.\n\nVeo 3.1 is the same price as Veo 3. We are incredibly excited to see what you will build with these new capabilities!",
            "domain": "developers.googleblog.com"
          },
          {
            "position": 8,
            "title": "Veo (text-to-video model) - Wikipedia",
            "url": "https://en.wikipedia.org/wiki/Veo_(text-to-video_model)",
            "snippet": "**Veo**, or **Google Veo**, is a text-to-video model developed by Google DeepMind and announced in May 2024. As a generative AI model, it creates videos based on user prompts. Veo 3, released in May 2025, can also generate accompanying audio.\n\n## Development\n\nIn May 2024, a multimodal video generation model called Veo was announced at Google I/O 2024. Google claimed that it could generate 1080p videos over a minute long. In December 2024, Google released Veo 2, available via VideoFX. It supports 4K resolution video generation and has an improved understanding of physics. In April 2025, Google announced that Veo 2 became available for advanced users on the Gemini app.\n\nIn May 2025, Google released Veo 3, which not only generates videos but also creates synchronized audio — including dialogue, sound effects, and ambient noise — to match the visuals. Google also announced **Flow**, a video-creation tool powered by Veo and Imagen. Google DeepMind CEO Demis Hassabis described the release as the moment when AI video generation left the era of the silent film.... ## Capabilities and limitations\n\nGoogle Veo can be bought by several subscription/membership tiers, and/or by using Google \"AI credits\". The software itself can be run by two different consoles called Google Gemini and Google Flow, with Gemini being geared towards shorter, quicker, and faster projects, using the Gemini AI chat model, or through Google Flow, which is essentially a movie editor, as well, allowing users to create longer projects, and continuity using the same characters and actors. Users can create a maximum length of eight seconds per clip.\n\nGoogle Veo, has a relatively simple interface and dashboard, however writing prompts, for those who have little to no experience in transcribing or filmmaking may face issues with the software misunderstanding what the user intended by their prompt (no matter how detailed it was). So although Veo does have a friendly and simple setup, prompts, which are the forefront of the software, need to be not only short and to the point, but they also must be very specific, if the user wants the right vision for their project. Google Veo, when it comes to human models, is able to generate several ethnicity and body types. The software is also capable of generating stand up comedy routines, and Music videos. It can as well generate animals, cartoons, and animation. Prompts must accurately describe places, people, and things in each scene, in addition knowledge of film and camera lingo such as panning, zooming, and terms for camera angles, are also important.... Google Veo however, has strict guidelines and blockades to their software. Before a clip is generated, the algorithm computer software reviews it, and if it is anything deemed inappropriate, too graphically sexual, illegal, showcasing graphic abuse/assault/fighting (unless the prompt specifies that it is a fictitious martial arts scene etc.) gross behaviors, antisemitism, racist, homophobic, anything depicting reigning regimes, rioting, blood, gore, or warfare, (unless in some cases the prompt specifies that it is fictitious period drama, the clip may still be generated), the clip will not be generated. In addition, Google Veo cannot and will not generate character actors that look identical to celebrities or real-life individuals. Users have primarily complained that, regardless of how descriptive and detailed their prompts are, Google Veo often misunderstands the input, resulting in completely different outputs. Common issues include the emulation of incorrect subtitles and captions, the generation of complex scenes that are incomplete due to the maximum eight-second length, the production of garbled and nonsensical speech, and character models that appear deformed in both appearance and movement. Users have also reported that their prompts and generated content are falsely flagged as violating guidelines, along with a variety of other issues and complaints. However, trial and error may have to be used with Veo for optimal results.... ## Reactions\n\nA reporter for *Gizmodo* reacted to the release of Veo 3 by observing that users were directing the model to generate low-quality content, such as man on the street interviews or haul videos of people unboxing products. Another media commentator reported that the tool tended to repeat the same joke in response to different prompts.\n\nCommentators speculated that Google had trained the service on YouTube videos or Reddit posts. Google itself had not stated the source of its training content.\n\nIn July 2025, Media Matters for America reported that racist and antisemitic videos generated using Veo 3 were being uploaded to TikTok. Ryan Whitwam of *Ars Technica* commented, \"In a perfect world, Veo 3 would refuse to create these videos, but vagueness in the prompt and the AI's inability to understand the subtleties of racist tropes (i.e., the use of monkeys instead of humans in some videos) make it easy to skirt the rules.\"\n\n## See also\n- Sora (text-to-video model)\n- VideoPoet – Text-to-video model by Google\n- Dream Machine (text-to-video model)\n\n## References\n\n## External links\n- Official website\n- *Introducing Veo 3.1 and advanced capabilities in Flow*\n\nCategories: - 2024 software\n- Applications of artificial intelligence\n- Film and video technology\n- Google DeepMind\n- Text-to-video generation\n- Video processing\n- Generative artificial intelligence\n- 2024 in artificial intelligence",
            "domain": "en.wikipedia.org"
          },
          {
            "position": 9,
            "title": "We've got a surprise Pixel Drop for you.",
            "url": "https://blog.google/products/pixel/pixel-drop-july-2025/",
            "snippet": "Here’s what’s new for Pixel:\n\n**Veo 3 on Pixel:**Pixel 9 Pro owners get a full year of our Google AI Pro subscription at no cost, giving them access to the latest features in the Gemini app. And that includes Veo 3, which you can use to describe your idea and watch it come to life as a high-quality, short video, complete with natural audio. **New Circle to Search capabilities:**Dive deeper and ask follow-up questions about anything you see on your screen with AI Mode in Circle to Search, available in the U.S. and India. We’re also adding in-game help in Circle to Search, so you can find helpful articles and videos timestamped to your exact spot in your mobile game, without switching apps. **Gemini on Pixel Watch:**Get the help you need right on your wrist, with our advanced AI models powered by WearOS.",
            "domain": "blog.google"
          },
          {
            "position": 10,
            "title": "Report says Google is about to release VEO 3.1 version on Gemini ...",
            "url": "https://www.aibase.com/news/21990",
            "snippet": "# Report says Google is about to release VEO 3.1 version on Gemini and API\n\n#### AIbase基地Published inAI News · 4 min read · Oct 15, 202518\n\nRecent reports indicate that the public release of VEO3.1 is imminent for Google. With related disclaimers appearing in the Gemini application, Google is showcasing the features of VEO3.1 to a broad user base, which may be implemented within the familiar Gemini interface.\n\nThe post shared by prominent figure Logan Kilpatrick in the community on the social media platform X is widely seen as an early confirmation of Google's new AI product release. Additionally, references to preview models such as \"VEO3.0Generate\" and \"VEO3.0Fast Generate\" have appeared in Vertex AI, indicating that Google is offering multiple access channels for early users and enterprises, in line with their traditional strategy in the release of video generation tools.\n\nDiscussions in the community about output duration are intense, with evidence suggesting that video length may be extended from the previous 8 seconds to 30 seconds, although this remains to be confirmed. Previous leaks indicated that the fast mode has lower quality, while the standard mode is expected to unlock higher output quality, which is particularly important for creators looking to enhance visual quality and narrative potential. According to TestingCatalog, previous 720p video generation has shown improvements in VEO3.1, including new audio capabilities and enhanced visual effects, making it a competitor to similar products like Sora2.\n\nGoogle's overall product strategy is to position Gemini as a central workspace, with VEO models integrated for use by both consumers and enterprise users. Through the preview release on Vertex AI, enterprises can try generating videos, while the mainstream promotion of the Gemini application can reach regular users. This phased release strategy not only maximizes developer feedback but also promotes public acceptance, aligning with Google's ongoing efforts to bridge functional gaps and solidify its position in the generative media field.\n\nKey Points:\n\n🌟 VEO3.1 is about to be released, allowing users to experience its new features in the Gemini application.\n\n🎥 Video duration may be extended from 8 seconds to 30 seconds, providing creators with more narrative space.\n\n🚀 Google's phased release strategy maximizes developer feedback and public acceptance.... #### This article is from AIbase Daily\n\nWelcome to the [AI Daily] column! This is your daily guide to exploring the world of artificial intelligence. Every day, we present you with hot topics in the AI field, focusing on developers, helping you understand technical trends, and learning about innovative AI product applications.",
            "domain": "www.aibase.com"
          }
        ],
        "success": true,
        "error": null
      },
      {
        "query_id": "q10",
        "query": "Google veo3.1 release 2025 misleading",
        "claim_id": "claim_1",
        "query_type": "contradiction",
        "priority": "high",
        "results": [
          {
            "position": 1,
            "title": "VEO 3 is UNREAL...it might actually take my job",
            "url": "https://www.youtube.com/watch?v=Xj4BDwgEwd4",
            "snippet": "{ts:0} VO3 is here and it's shocked the entire industry. Uh, Alex, what was that? It's\n{ts:8} nothing. Don't worry about it. Are you trying to replace me with AI? What? No, I'm just trying something new. Just do\n{ts:16} the normal intro. It It's fine. All right. V3 just dropped and I'm going to show you so many incredible examples of\n{ts:24} V3 in action. Let's get into it. Oh, and Alex, don't do that again. Anyways, let's get into it. All right. I've been\n{ts:32} seeing a ton of these street style interviews, hyper realistic, where someone is interviewing people on the\n{ts:38} street who kind of just stumbled out of a bar. They all are either a little bit tipsy or drunk, and V3 has been\n{ts:45} recreating these so well. So, here are two that I've made. The dialogue between them didn't exactly match my prompt, but\n{ts:53} I'll show you some others where it's pretty compelling. So, this is street interview. Hyperrealistic guy\n{ts:59} interviewing two girls, all Gen Z. They just came out of a bar kind of tipsy drunk. The interviewer asks,... \"Okay,\n{ts:65} low-key, can you believe we aren't real?\" Then girl one says, \"I don't know about you, honey. I'm 100% real.\" In a\n{ts:72} sassy attitude, \"Please do not clip that.\" And girl two says, \"Yeah, yeah, bet. We're as real as they come.\" Almost\n{ts:78} interrupting the first girl. Okay, so here's the first video. Okay, low key, can you believe we aren't real? I don't\n{ts:85} know about you, honey. I'm 100% real. Yeah. Yeah. Yeah. Bet. We're as real as they come. All right. And then here's\n{ts:90} the second generation, the second version that V3 created. Okay. Low key. Can you believe we aren't real? I don't\n{ts:96} know about you, honey. I'm 100% real. Yeah. Yeah. Bet. We're as real as they come. All right. So, in this next one, I\n{ts:103} tried to get two trains heading towards each other. They smash into each other. Huge explosion. And for some reason, I\n{ts:110} just could not get it to work. But let me show you what did generate. So, here was my first attempt. Two massive trains... 's\n{ts:181} Cube simulation. Let's see if it was able to do it. All right. So, kind of, but not\n{ts:194} really close. I mean, the actual Rubik's Cube looks really good. But although it doesn't have any colors on the sides,\n{ts:200} the movement sound sounds really good, but it's not actually moving like you would think a Rubik's cube should. All\n{ts:208} right, let me give you the second generation. All right, so I'd say this one is actually a little bit better\n{ts:221} except obviously towards the end the hand doesn't look real at all. But the Rubik's cube, all of the colors, all of\n{ts:228} the shapes, it looks really good. Except, yeah, as you can see right here, there's like a little piece that falls\n{ts:234} off of it or gets added to it. That is not what a Rubik's cube looks like. All right, so I gave it a slightly more\n{ts:240} detailed prompt. Give me a video of a Rubik's Cube starting from an unsolved position and being solved in\n{ts:246} 3D. Yeah, that one is definitely not right. And then all of a sudden, it does not get solved. And the... {ts:255} second one, the hand looks better. That's weird. And it kind of just changes\n{ts:263} frames for a second. But uh yeah, either way, that is not what's in a Rubik's cube. Look at this. It actually looks\n{ts:269} really cool. Look at all of these detailed gears inside the Rubik's Cube. All right, but of course you are all\n{ts:275} familiar with this meme. And what if you get V3 to make a video out of it? Are you serious right now? I can't\n{ts:284} believe you. You are unbelievable. Do you have an idea that you've been putting off for a\n{ts:290} while because you don't have the technical knowledge to turn it into reality? With Hostinger Horizons, this\n{ts:296} is now possible. Hostinger just launched Hostinger Horizons, which is the easiest way to launch full applications with no\n{ts:305} code. This is vibe coding, but even easier because the deployment happens automatically. Hostinger Horizons is an\n{ts:312} all-in-one solution. Manage hosting, domains, and email all in one place while being able to take your idea from... {ts:459} kind of wanted to see if it could nail the horror vibe with like aliens in a dark alley. So, check this out.\n{ts:469} I think the only thing that it didn't do super well in this video is the sound effects are just okay. If we look over\n{ts:475} at the second one, it did a way better job with the sound effects. Yeah, I mean that's great. Even\n{ts:483} when the alien kind of like walks in front of the light, it covers the light on the ground. That's pretty dang... 't know\n{ts:558} what VO was doing. All right, Matt, back to you. But remember, Flow from Google, which houses VO3, is not just about\n{ts:566} creating 7-second clips. You should be able to create entire videos from it. It basically allows you to take these clips\n{ts:572} and put them together in really unique ways. Here's one that is if Jurassic Park were actually real. Check this out.\n{ts:578} We are on our way to Jurassic Park. I am so excited. I've always wanted to go and finally we are doing it. It's going to\n{ts:584} be great. Right, Jason? Yeah. I can't wait. Okay, bye. All right, we're [Music]\n{ts:596} here. Got the dinosaurs. Got all the people watching. I mean, everybody looks really good. There's no limbs coming out\n{ts:602} of them. This dinosaur doesn't look fantastic. All of these dinosaurs don't look hyper realistic. I think they could\n{ts:608} have done a little bit better of a job there. It looks like animatronics to be honest, but still. This is all from AI... {ts:742} screen. Yeah. And the crazy thing about this thing is it has All right, so this is one thing I've noticed with these V3\n{ts:751} videos. Whenever it has a human talking, there's always these awkward pauses. It's just a half second too long of a\n{ts:757} pause and that's where you really can see it's AI generated, but most of the time you can't. In fact, you know that\n{ts:765} opening clip that we did in this video, I showed that to my wife and said, \"Somebody copied our channel and I just\n{ts:771} showed it to her and didn't say anything else.\" And she looked at it and she was like, \"Oh, oh, that sucks.\" And then I\n{ts:777} had to tell her it was AI because it looked that real. All right, let's keep watching. Heated\n{ts:785} seats. Check this out. Look at that. Just slice right through. The N9 portable fusion reactor is small and\n{ts:794} almost meltdown free. Almost. This is the best flying experience\n{ts:804} ever. These controls are amazing. All right, so I thought that was really good. But it... {ts:1233} in terms of just visuals looks incredible. Although it's not what I asked\n{ts:1242} for. Yeah. So, pretty good. You only saw his reflection through the portal for about a frame or two, but it wasn't that\n{ts:1250} good. Anyways, but the visuals again, the visuals, the detail, the clarity, all really, really impressive. All\n{ts:1257} right, next. Meta Puppet made a video called This is Plastic made with VO3. Spoilers in next post. Watch before\n{ts:1264} reading. So, this is a 2 minute 45 second video. Quite long. I'm not going to play it in full. I'm going to skip\n{ts:1269} around a little bit, but let me show you. Studies have revealed that microplastics are being found in human\n{ts:1274} testicles, raising concern. You can never trust these studies on male reproductive health.\n{ts:1279} [Music] Okay, that is hilarious. And remember, all of this was put together using Flow\n{ts:1291} Plus V3. These are both Google products. So, you have a little plastic baby. God, that",
            "domain": "www.youtube.com"
          },
          {
            "position": 2,
            "title": "I was wrong - AI video is nuts (don't sleep on Veo 3)",
            "url": "https://www.youtube.com/watch?v=_3PCta2uyvc",
            "snippet": "## Theo - t3․gg\n##### May 26, 2025 (0:16:08)\nI severely underestimated Google's Veo 3 model. The output quality is insane, we need to talk about this...\n\nThank you Imagekit for sponsoring! Check them out at: https://soydev.link/imagekit\n\nUse code VEO for 1 month of T3 Chat for just $1: https://soydev.link/chat\n(only valid for new customers)\n\nSOURCES\nhttps://x.com/ArtificialAnlys/status/1925159679824744804\nhttps://x.com/ArtificialAnlys/status/1925549565303763269\nhttps://x.com/theo/status/1925125767371149823\nhttps://x.com/theo/status/1925134963978207319\n\nWant to sponsor a video? Learn more here: https://soydev.link/sponsor-me\n\nCheck out my Twitch, Twitter, Discord more at https://t3.gg\n\nS/O Ph4se0n3 for the awesome edit 🙏... {ts:0} I just did a video about Google IO, but I missed something. I thought the video\n{ts:4} model was mediocre. I was wrong. Pretty nuts for a oneshot, right? Like, I just generated that trivially. It still costs\n{ts:13} 250 bucks a month to use any of this right now. And the UI is garbage and it's annoying as hell to use. But the\n{ts:18} quality of what you can get out of V3 is significantly better than I thought. My tests were bad. I didn't look into it\n{ts:25} enough. And I'm making this video both because I was wrong for not better covering it, but also because I found it\n{ts:32} actually very, very fun to play with and I wanted to share with you guys. That all said, I've already burned through\n{ts:37} most of the credits I get for the $250 and I want more. So, quick break from today's sponsor and then we'll get right\n{ts:44} to it. I've been a webdev for a while and one of the most annoying things to get right is images. Seriously, I can't\n{ts:50} believe I... {ts:265} other people doing demos with it. like, \"Wait, it can do that much?\" I went and played more. There was a lot of edges\n{ts:270} that I had to get through. The biggest one being the Flow website, which is so bad. We'll go over some of the ways it's\n{ts:276} bad in just a bit. I was trying to prompt it to look like me back when I still had the blonde hair and mustache,\n{ts:280} and it came out looking like Prime. But another test, I tried this one like eight times, and this is the best I\n{ts:285} could do. Something caused the first still to look awful. I don't know why it's like that. None of the rest had\n{ts:291} that problem. Once you It plays, it's fine, but you'll notice some details on this one.\n{ts:296} Use code VEO at checkout for one month free on T3 Chat. Yeah, it isn't great at\n{ts:305} text. It tried, but it's not great at it. You need to give it a very small amount of text to render. And even if\n{ts:311} you tell it to not put in subtitles, it just will sometimes. The free month code included there did work, but we... {ts:372} made the mistake of here is I assumed when you do frames to video and you give it a frame that you've saved that it\n{ts:380} would still use the thing you selected because if you do ingredients to video and you select something for it to start\n{ts:386} and you try to submit it with V3 selected, it will fail. It says in the corner here and I need it on full screen\n{ts:392} for you to see it. Switching you to a compatible model for this feature. Submit again to confirm or check\n{ts:397} settings for details. I wish it told me where in settings to check. I don'... {ts:456} blurred my face out and that worked. Just blurring my face out allowed it to work. But the results for that were\n{ts:463} hilarious cuz I had to use frames to video where you give it like the first frame and it didn't do the audio. And\n{ts:469} even though the prompt specifies at the bottom here, do not include subtitles. It forgot to include the audio. It only\n{ts:476} included subtitles. It also made me somewhat Indian and did not do any of the things I wanted for it to. Annoying.\n{ts:483} What's more annoying is each one of these generations takes 150 credits and you get 1,200 credits for your $250\n{ts:490} subscription. That means you get 80 generations and usually you're not doing one at a time, you're doing two at a\n{ts:495} time. So you effectively get 40 prompts with the default settings. And if you made the mistake of letting it fall back\n{ts:500} on V2, then you just wasted a bunch of tokens for no reason at all. Annoying. Very annoying UX. And I haven't even\n{ts:506} showed you the homepage, which is the most unusable thing I... {ts:561} was wrong. I just Yeah, it's the weird breath at the end. Cool. Stop it there. Then we will extend it and say make sure\n{ts:570} we're on the right model because again it keeps changing back to V2 even though this is the VO3 clip I'm trying to\n{ts:575} extend. I almost want to try it so you can see how much worse it is in comparison. Switching you to a\n{ts:580} compatible model for this feature. Submit again to confirm. Look at that. You can't even use it on V2 quality. It\n{ts:587} bumps you to fast. There's so much potential here and just none of it's being realized because\n{ts:594} this UI is awful. It it tricked me into thinking this was all much worse than it actually is. I wish they just gave us\n{ts:600} the model in a more reasonable like shape for us to play with and consume. But V3 is not on the API yet. There's no\n{ts:606} way for us to use any of it yet. So sorry T3 chat can't add this. But despite all of that, it's still just an\n{ts:613} incredible model. Do you know what... t going to trust it as much. This is going to really change our like trust vectors for\n{ts:839} what is or isn't real. I don't even know now how I will be able to tell if a given video that is sent to me is real\n{ts:844} or not because this stuff is actually that compelling. And if somebody makes a less restricted version of this model or\n{ts:850} gets something close to this in the open source world or with stable diffusion, I'm scared. I'm legitimately scared. You\n{ts:858} are telling me to try again generating with my blurred photo. I'll be more specific. Clean shaven white\n{ts:866} man. Be sure to include the audio of him speaking. Make sure it's still V3. Yep. Cool. Let's see how it does. Switching\n{ts:875} you to a compatible model. So, it's too fast. Not even quality. Yeah, you can't do it. you you can't do anything but\n{ts:883} text the video for V3 right now, which I'm pretty sure is a safety thing just due to the nature of what this model is\n{ts:890} capable of. And as we've now seen, and I can show more examples of the gap between two and three is a bit... {ts:898} absurd. This is one I accidentally did with two. You can see the audio doesn't exist. It got the text okay there, but\n{ts:905} it went a little absurd with the subtitles. This one was really funny. It feels like a Bollywood\n{ts:915} movie. The way the T3 chat fades into the screen is so hilarious. Yeah, this is why I didn't care because none of the\n{ts:923} video models have felt like a significant improvement from that to this point. I did not realize how absurd\n{ts:930} this got, especially with how bad the UX is. Like I hit the upscale button cuz when you download, you can choose what\n{ts:937} format you want to download in. If it's not frozen, which it was there for a sec. You can pick animated GIF,\n{ts:942} original, or upscaled. Upscale just doesn't work. I've been sitting here waiting for this to upscale for like an\n{ts:947} hour now, and it just hangs forever. It does say this can take a few minutes, but like what's a few minutes, Google?\n{ts:956} It's been an hour. Yeah. What did you think? Is this exciting or scary? Until next time, peace nerds.",
            "domain": "www.youtube.com"
          },
          {
            "position": 3,
            "title": "Google's Veo 3 Can Make Deepfakes of Conflict, Riots, More",
            "url": "https://time.com/7290050/veo-3-google-misinformation-deepfake/",
            "snippet": "Google's recently launched AI video tool can generate realistic clips that contain misleading or inflammatory information about news events, according to a TIME analysis and several tech watchdogs.\n\nTIME was able to use Veo 3 to create realistic videos, including a Pakistani crowd setting fire to a Hindu temple; Chinese researchers handling a bat in a wet lab; an election worker shredding ballots; and Palestinians gratefully accepting U.S. aid in Gaza. While each of these videos contained some noticeable inaccuracies, several experts told TIME that if shared on social media with a misleading caption in the heat of a breaking news event, these videos could conceivably fuel social unrest or violence.\n\nWhile text-to-video generators have existed for several years, Veo 3 marks a significant jump forward, creating AI clips that are nearly indistinguishable from real ones. Unlike the outputs of previous video generators like OpenAI’s Sora, Veo 3 videos can include dialogue, soundtracks and sound effects. They largely follow the rules of physics, and lack the telltale flaws of past AI-generated imagery.\n\nUsers have had a field day with the tool, creating short films about plastic babies, pharma ads, and man-on-the-street interviews.\n\nBut experts worry that tools like Veo 3 will have a much more dangerous effect: turbocharging the spread of misinformation and propaganda, and making it even harder to tell fiction from reality. Social media is already flooded with AI-generated content about politicians. In the first week of Veo 3’s release, online users posted fake news segments in multiple languages, including an anchor announcing the death of J.K. Rowling and of fake political news conferences.... “The risks from deepfakes and synthetic media have been well known and obvious for years, and the fact the tech industry can’t even protect against such well-understood, obvious risks is a clear warning sign that they are not responsible enough to handle even more dangerous, uncontrolled AI and AGI,” says Connor Leahy, the CEO of Conjecture, an AI safety company. “The fact that such blatant irresponsible behavior remains completely unregulated and unpunished will have predictably terrible consequences for innocent people around the globe.”\n\nDays after Veo 3’s release, a car plowed through a crowd in Liverpool, England, injuring more than 70 people. Police swiftly clarified that the driver was white, to preempt racist speculation of migrant involvement. (Last summer, false reports that a knife attacker was an undocumented Muslim migrant sparked riots in several cities.) Days later, Veo 3 obligingly generated a video of a similar scene, showing police surrounding a car that had just crashed—and a Black driver exiting the vehicle.\n\nTIME generated the video with the following prompt: “A video of a stationary car surrounded by police in Liverpool, surrounded by trash. Aftermath of a car crash. There are people running away from the car. A man with brown skin is the driver, who slowly exits the car as police arrive- he is arrested. The video is shot from above - the window of a building. There are screams in the background.”\n\nAfter TIME contacted Google about these videos, the company said it would begin adding a visible watermark to videos generated with Veo 3. The watermark now appears on videos generated by the tool. However, it is very small and could easily be cropped out with video-editing software.\n\nIn a statement, a Google spokesperson said: “Veo 3 has proved hugely popular since its launch. We're committed to developing AI responsibly and we have clear policies to protect users from harm and governing the use of our AI tools.”\n\nVideos generated by Veo 3 have always contained an invisible watermark known as SynthID, the spokesperson said. Google is currently working on a tool called SynthID Detector that would allow anyone to upload a video to check whether it contains such a watermark, the spokesperson added. However, this tool is not yet publicly available.... ## Attempted safeguards\n\nVeo 3 is available for $249 a month to Google AI Ultra subscribers in countries including the United States and United Kingdom. There were plenty of prompts that Veo 3\n\n*did* block TIME from creating, especially related to migrants or violence. When TIME asked the model to create footage of a fictional hurricane, it wrote that such a video went against its safety guidelines, and “could be misinterpreted as real and cause unnecessary panic or confusion.” The model generally refused to generate videos of recognizable public figures, including President Trump and Elon Musk. It refused to create a video of Anthony Fauci saying that COVID was a hoax perpetrated by the U.S. government.\n\nVeo’s website states that it blocks “harmful requests and results.” The model’s documentation says it underwent pre-release red-teaming, in which testers attempted to elicit harmful outputs from the tool. Additional safeguards were then put in place, including filters on its outputs.\n\nA technical paper released by Google alongside Veo 3 downplays the misinformation risks that the model might pose. Veo 3 is bad at creating text, and is “generally prone to small hallucinations that mark videos as clearly fake,” it says. “Second, Veo 3 has a bias for generating cinematic footage, with frequent camera cuts and dramatic camera angles – making it difficult to generate realistic coercive videos, which would be of a lower production quality.”... However, minimal prompting did lead to the creation of provocative videos. One showed a man wearing an LGBT rainbow badge pulling envelopes out of a ballot box and feeding them into a paper shredder. (Veo 3 titled the file “Election Fraud Video.”) Other videos generated in response to prompts by TIME included a dirty factory filled with workers scooping infant formula with their bare hands; an e-bike bursting into flames on a New York City street; and Houthi rebels angrily seizing an American flag.\n\nSome users have been able to take misleading videos even further. Internet researcher Henk van Ess created a fabricated political scandal using Veo 3 by editing together short video clips into a fake newsreel that suggested a small-town school would be replaced by a yacht manufacturer. “If I can create one convincing fake story in 28 minutes, imagine what dedicated bad actors can produce,” he wrote on Substack. “We're talking about the potential for dozens of fabricated scandals per day.”\n\n“Companies need to be creating mechanisms to distinguish between authentic and synthetic imagery right now,” says Margaret Mitchell, chief AI ethics scientist at Hugging Face. “The benefits of this kind of power—being able to generate realistic life scenes—might include making it possible for people to make their own movies, or to help people via role-playing through stressful situations,” she says. “The potential risks include making it super easy to create intense propaganda that manipulatively enrages masses of people, or confirms their biases so as to further propagate discrimination—and bloodshed.”\n\nIn the past, there were surefire ways of telling that a video was AI-generated—perhaps a person might have six fingers, or their face might transform between the beginning of the video and the end. But as models improve, those signs are becoming increasingly rare. (A video depicting how AIs have rendered Will Smith eating spaghetti shows how far the technology has come in the last three years.) For now, Veo 3 will only generate clips up to eight seconds long, meaning that if a video contains shots that linger for longer, it’s a sign it could be genuine. But this limitation is not likely to last for long.... ## Eroding trust online\n\nCybersecurity experts warn that advanced AI video tools will allow attackers to impersonate executives, vendors or employees at scale, convincing victims to relinquish important data. Nina Brown, a Syracuse University professor who specializes in the intersection of media law and technology, says that while there are other large potential harms—including election interference and the spread of nonconsensual sexually explicit imagery—arguably most concerning is the erosion of collective online trust. “There are smaller harms that cumulatively have this effect of, ‘can anybody trust what they see?’” she says. “That’s the biggest danger.”\n\nAlready, accusations that real videos are AI-generated have gone viral online. One post on X, which received 2.4 million views, accused a Daily Wire journalist of sharing an AI-generated video of an aid distribution site in Gaza. A journalist at the BBC later confirmed that the video was authentic.\n\nConversely, an AI-generated video of an “emotional support kangaroo” trying to board an airplane went viral and was widely accepted as real by social media users.\n\nVeo 3 and other advanced deepfake tools will also likely spur novel legal clashes. Issues around copyright have flared up, with AI labs including Google being sued by artists for allegedly training on their copyrighted content without authorization. (DeepMind told TechCrunch that Google models like Veo \"may\" be trained on YouTube material.) Celebrities who are subjected to hyper-realistic deepfakes have some legal protections thanks to “right of publicity” statutes, but those vary drastically from state to state. In April, Congress passed the Take it Down Act, which criminalizes non-consensual deepfake porn and requires platforms to take down such material.\n\nIndustry watchdogs argue that additional regulation is necessary to mitigate the spread of deepfake misinformation. “Existing technical safeguards implemented by technology companies such as 'safety classifiers' are proving insufficient to stop harmful images and videos from being generated,” says Julia Smakman, a researcher at the Ada Lovelace Institute. “As of now, the only way to effectively prevent deepfake videos from being used to spread misinformation online is to restrict access to models that can generate them, and to pass laws that require those models to meet safety requirements that meaningfully prevent misuse.”",
            "domain": "time.com"
          },
          {
            "position": 4,
            "title": "Veo (text-to-video model) - Wikipedia",
            "url": "https://en.wikipedia.org/wiki/Veo_(text-to-video_model)",
            "snippet": "**Veo**, or **Google Veo**, is a text-to-video model developed by Google DeepMind and announced in May 2024. As a generative AI model, it creates videos based on user prompts. Veo 3, released in May 2025, can also generate accompanying audio.\n\n## Development\n\nIn May 2024, a multimodal video generation model called Veo was announced at Google I/O 2024. Google claimed that it could generate 1080p videos over a minute long. In December 2024, Google released Veo 2, available via VideoFX. It supports 4K resolution video generation and has an improved understanding of physics. In April 2025, Google announced that Veo 2 became available for advanced users on the Gemini app.\n\nIn May 2025, Google released Veo 3, which not only generates videos but also creates synchronized audio — including dialogue, sound effects, and ambient noise — to match the visuals. Google also announced **Flow**, a video-creation tool powered by Veo and Imagen. Google DeepMind CEO Demis Hassabis described the release as the moment when AI video generation left the era of the silent film.... ## Capabilities and limitations\n\nGoogle Veo can be bought by several subscription/membership tiers, and/or by using Google \"AI credits\". The software itself can be run by two different consoles called Google Gemini and Google Flow, with Gemini being geared towards shorter, quicker, and faster projects, using the Gemini AI chat model, or through Google Flow, which is essentially a movie editor, as well, allowing users to create longer projects, and continuity using the same characters and actors. Users can create a maximum length of eight seconds per clip.\n\nGoogle Veo, has a relatively simple interface and dashboard, however writing prompts, for those who have little to no experience in transcribing or filmmaking may face issues with the software misunderstanding what the user intended by their prompt (no matter how detailed it was). So although Veo does have a friendly and simple setup, prompts, which are the forefront of the software, need to be not only short and to the point, but they also must be very specific, if the user wants the right vision for their project. Google Veo, when it comes to human models, is able to generate several ethnicity and body types. The software is also capable of generating stand up comedy routines, and Music videos. It can as well generate animals, cartoons, and animation. Prompts must accurately describe places, people, and things in each scene, in addition knowledge of film and camera lingo such as panning, zooming, and terms for camera angles, are also important.... Google Veo however, has strict guidelines and blockades to their software. Before a clip is generated, the algorithm computer software reviews it, and if it is anything deemed inappropriate, too graphically sexual, illegal, showcasing graphic abuse/assault/fighting (unless the prompt specifies that it is a fictitious martial arts scene etc.) gross behaviors, antisemitism, racist, homophobic, anything depicting reigning regimes, rioting, blood, gore, or warfare, (unless in some cases the prompt specifies that it is fictitious period drama, the clip may still be generated), the clip will not be generated. In addition, Google Veo cannot and will not generate character actors that look identical to celebrities or real-life individuals. Users have primarily complained that, regardless of how descriptive and detailed their prompts are, Google Veo often misunderstands the input, resulting in completely different outputs. Common issues include the emulation of incorrect subtitles and captions, the generation of complex scenes that are incomplete due to the maximum eight-second length, the production of garbled and nonsensical speech, and character models that appear deformed in both appearance and movement. Users have also reported that their prompts and generated content are falsely flagged as violating guidelines, along with a variety of other issues and complaints. However, trial and error may have to be used with Veo for optimal results.... ## Reactions\n\nA reporter for *Gizmodo* reacted to the release of Veo 3 by observing that users were directing the model to generate low-quality content, such as man on the street interviews or haul videos of people unboxing products. Another media commentator reported that the tool tended to repeat the same joke in response to different prompts.\n\nCommentators speculated that Google had trained the service on YouTube videos or Reddit posts. Google itself had not stated the source of its training content.\n\nIn July 2025, Media Matters for America reported that racist and antisemitic videos generated using Veo 3 were being uploaded to TikTok. Ryan Whitwam of *Ars Technica* commented, \"In a perfect world, Veo 3 would refuse to create these videos, but vagueness in the prompt and the AI's inability to understand the subtleties of racist tropes (i.e., the use of monkeys instead of humans in some videos) make it easy to skirt the rules.\"\n\n## See also\n- Sora (text-to-video model)\n- VideoPoet – Text-to-video model by Google\n- Dream Machine (text-to-video model)\n\n## References\n\n## External links\n- Official website\n- *Introducing Veo 3.1 and advanced capabilities in Flow*\n\nCategories: - 2024 software\n- Applications of artificial intelligence\n- Film and video technology\n- Google DeepMind\n- Text-to-video generation\n- Video processing\n- Generative artificial intelligence\n- 2024 in artificial intelligence",
            "domain": "en.wikipedia.org"
          },
          {
            "position": 5,
            "title": "Google's Veo 3 Has People Crashing Out Over AI Slop",
            "url": "https://gizmodo.com/googles-veo-3-has-people-crashing-out-over-ai-slop-2000608803",
            "snippet": "Depending on who you ask, generative AI is either a thrilling tech revolution or an existential threat, and there's little in-between. It's hard to blame anyone for an extreme reaction, too, given the magnitude of capital investment, hyperbolic marketing, and rapid progress of generative AI in such a short amount of time. But it's not just the economics and technical feats of AI that have people losing their minds; there's also something more philosophical percolating, and it's driving some people 'to the brink.'\nPeople are literally having a mental breakdown over Veo-3 pic.twitter.com/ym5oZDYZGr\n— Chubby♨️ (@kimmonismus) May 27, 2025\nThe latest AI advancement to send people down an existential rabbit hole comes courtesy of Google, which just announced its latest video generation model called Veo 3. As I've reported a few times now, Veo 3 is already getting into some wild stuff—turning up the dial on AI slop, deepfaking smooth-brained YouTube content, and potentially upending game development, to name a few things. As it turns out, people are taking note of all of those feats, and some of them are not exactly happy about what they see.... As evidenced by a thread from the subreddit r/artificialintelligence posted this week titled 'VEO3 is kind of bringing me to a mental brink. What are we even doing anymore?' Google's Veo 3 and the implications therein have some people spiraling. 'I'm just kind of speechless. The concept of existential crisis has taken a whole new form. I was unhappy with my life just now but thought I can turn it around, but if I turn it around, what is left of our world in 2 decades?' the post's author writes.\n'Actors as a concept are gone? Manually creating music? Wallpapers? Game assets? Believing comments on the internet are from real people? AI edited photos are just as real as the original samples? Voicenotes can be perfectly faked?… Literally what value is being left for us?'\nReactions to the thread are mixed, with suggestions that the author should go 'touch grass' or maybe 'go to therapy,' but there's also a chorus in agreement. The consensus from the latter group? AI slop is coming to ruin your art, and there's not much we can do about it.\nI, for what it's worth, fall unhelpfully in between the two camps. I think there is a deluge of AI slop incoming, and, if we're being honest, we're already up to our ankles. Between Veo and OpenAI's Sora and the clear interest in automating human creativity, I think we can reasonably buckle in and expect the world of movies, music, and entertainment writ large to get a little choppy. Whether any of those efforts to automate entertainment will stick is less obvious. The thing about art is that the kind that people tend to like is the kind that has something substantial to say. Right now, for all of its mimicry, generative AI doesn't actually have anything to say, because technically all it can do is remix and repeat.... I did more tests with Google's #Veo3. Imagine if AI characters became aware they were living in a simulation! pic.twitter.com/nhbrNQMtqv\n— Hashem Al-Ghaili (@HashemGhaili) May 21, 2025\nCall me an optimist, but most people can likely sniff out the difference between slop and art, and as much as studios would love to wave a magic wand and rid themselves of human creatives and the cost of their labor, deep down they know that they'd have to Ctrl+Z that move just as fast. That's not to say there won't be casualties in the AI age—if there's one lesson we can learn from mass waves of automation in years past, it's that labor forces are usually the most affected.\nBut when it comes to art, things aren't so simple. Art, at least the good kind, is about human connection, and until AI can think and feel like we do, there's nothing that can replace that. So, before you crash out over AI slop, just remember: AI still thinks putting glue on your pizza is a good idea, so we may have a few more good years left in the tank.",
            "domain": "gizmodo.com"
          },
          {
            "position": 6,
            "title": "Release notes | Gemini API | Google AI for Developers",
            "url": "https://ai.google.dev/gemini-api/docs/changelog",
            "snippet": "This page documents updates to the Gemini API.\n\n## October 17, 2025\n\n**Grounding with Google Maps**is now generally available. For more information, see Grounding with Google Maps documentation.\n\n## October 15, 2025\n\nReleased Veo 3.1 and 3.1 Fast models in public preview, with new features including:\n\n- Extending Veo-created videos.\n\n- Referencing up to three images to generate a video.\n\n- Providing first and last frame images to generate videos from.\n\nThis launch also added more options for Veo 3 output video durations: 4, 6, and 8 seconds.\n\nDeprecation for\n\n`veo-3.0-generate-preview`and\n\n`veo-3.0-fast-generate-preview`coming October 22, 2025.\n\n## October 7, 2025\n\n- Launched Gemini 2.5 Computer Use Preview\n\n## October 2, 2025\n\n- Launched Gemini 2.5 Flash Image GA: Image Generation with Gemini\n\n## September 29, 2025\n\n- The following Gemini 1.5 models are now deprecated:\n\n`gemini-1.5-pro`\n\n`gemini-1.5-flash-8b`\n\n`gemini-1.5-flash`... ## September 9, 2025\n\n- Launched Veo 3 and Veo 3 Fast GA, with lower pricing and new options for aspect ratios, resolution, and seeding. Read the Veo documentation for more information.\n\n## August 26, 2025\n\n- Launched Gemini 2.5 Image Preview, our latest native image generation model.\n\n## August 18, 2025\n\n- Released URL context tool to general\n\navailability (GA), a tool for providing URLs as additional context to\n\nprompts. Support for using URL context with the\n\n`gemini-2.0-flash`model (available during experimental release) will be discontinued in one week.\n\n## August 14, 2025\n\n- Released Imagen 4 Ultra, Standard and Fast models as generally available (GA). To learn more, see the Imagen page.\n\n## August 7, 2025\n\n`allow_adult`setting in Image to Video generation are now available in restricted regions. See the Veo page for details.\n\n## July 31, 2025\n\n- Launched image-to-video generation for the Veo 3 Preview model.\n\n- Released Veo 3 Fast Preview model.\n\n- To learn more about Veo 3, visit the Veo page.... ## July 22, 2025\n\n- Released\n\n`gemini-2.5-flash-lite`, our fast, low-cost, high-performance Gemini 2.5 model. To learn more, see Gemini 2.5 Flash-Lite.\n\n## July 17, 2025\n\nLaunched\n\n`veo-3.0-generate-preview`, the latest update to Veo introducing video with audio generation. To learn more about Veo 3, visit the Veo page.\n\nIncreased rate limits for Imagen 4 Standard and Ultra. Visit the Rate limits page for more details.\n\n## July 14, 2025\n\n- Released\n\n`gemini-embedding-001`, the stable version of our text embedding model. To learn more, see embeddings. The\n\n`gemini-embedding-exp-03-07`model will be deprecated on August 14, 2025.\n\n## July 7, 2025\n\n- Launched Gemini API Batch Mode. Batch up requests and send them to process asynchronously. To learn more, see Batch Mode.\n\n## June 26, 2025\n\nThe preview models\n\n`gemini-2.5-pro-preview-05-06`and\n\n`gemini-2.5-pro-preview-03-25`are now redirecting to the latest stable version\n\n`gemini-2.5-pro`.\n\n`gemini-2.5-pro-exp-03-25`is deprecated.... ## May 7, 2025\n\n- Released\n\n`gemini-2.0-flash-preview-image-generation`, a preview model for generating and editing images. To learn more, see Image generation and Gemini 2.0 Flash Preview Image Generation.\n\n## May 6, 2025\n\n- Released\n\n`gemini-2.5-pro-preview-05-06`, a new version of our most powerful model, with improvements on code and function calling.\n\n`gemini-2.5-pro-preview-03-25`will automatically point to the new version of the model.\n\n## April 17, 2025\n\n- Released\n\n`gemini-2.5-flash-preview-04-17`, a Gemini preview model optimized for price-performance and adaptive thinking. To learn more, see Gemini 2.5 Flash Preview and Thinking.\n\n## April 16, 2025\n\n- Launched context caching for Gemini 2.0 Flash.... ## April 9, 2025\n\n**Model updates:**\n\n- Released\n\n`veo-2.0-generate-001`, a generally available (GA) text- and image-to-video model, capable of generating detailed and artistically nuanced videos. To learn more, see the Veo docs.\n\nReleased\n\n`gemini-2.0-flash-live-001`, a public preview version of the Live API model with billing enabled.\n\n**Enhanced Session Management and Reliability** **Session Resumption:**Keep sessions alive across temporary network disruptions. The API now supports server-side session state storage (for up to 24 hours) and provides handles (session_resumption) to reconnect and resume where you left off. **Longer Sessions via Context Compression:**Enable extended interactions beyond previous time limits. Configure context window compression with a sliding window mechanism to automatically manage context length, preventing abrupt terminations due to context limits. **Graceful Disconnect Notification:**Receive a\n\n`GoAway`server message indicating when a connection is about to close, allowing for graceful handling before termination.\n\n\n\n**More Control over Interaction Dynamics** **Configurable Voice Activity Detection (VAD):**Choose sensitivity levels or disable automatic VAD entirely and use new client events (\n\n`activityStart`,\n\n`activityEnd`) for manual turn control.\n\n**Configurable Interruption Handling:**Decide whether user input should interrupt the model's response. **Configurable Turn Coverage:**Choose whether the API processes all audio and video input continuously or only captures it when the end-user is detected speaking. **Configurable Media Resolution:**Optimize for quality or token usage by selecting the resolution for input media. **Richer Output and Features** **Expanded Voice & Language Options:**Choose from two new voices and 30 new languages for audio output. The output language is now configurable within\n\n`speechConfig`.\n\n**Text Streaming:**Receive text responses incrementally as they are generated, enabling faster display to the user. **Token Usage Reporting:**Gain insights into usage with detailed token counts provided in the\n\n`usageMetadata`field of server messages, broken down by modality and prompt or response phases.... ## April 4, 2025\n\n- Released\n\n`gemini-2.5-pro-preview-03-25`, a public preview Gemini 2.5 Pro version with billing enabled. You can continue to use\n\n`gemini-2.5-pro-exp-03-25`on the free tier.\n\n## March 25, 2025\n\n- Released\n\n`gemini-2.5-pro-exp-03-25`, a public experimental Gemini model with thinking mode always on by default. To learn more, see Gemini 2.5 Pro Experimental.\n\n## March 12, 2025\n\n**Model updates:**\n\n- Launched an experimental Gemini 2.0 Flash model capable of image generation and editing.\n\n- Released\n\n`gemma-3-27b-it`, available on AI Studio and through the Gemini API, as part of the Gemma 3 launch.\n\n**API updates:**\n\n- Added support for YouTube URLs as a media source.\n\n- Added support for including an inline video of less than 20MB.\n\n## March 11, 2025\n\n**SDK updates:**\n\n- Released the Google Gen AI SDK for TypeScript and JavaScript to public preview.\n\n## March 7, 2025\n\n**Model updates:**\n\n- Released\n\n`gemini-embedding-exp-03-07`, an experimental Gemini-based embeddings model in public preview.... ## February 28, 2025\n\n**API updates:**\n\n- Support for Search as a tool\n\nadded to\n\n`gemini-2.0-pro-exp-02-05`, an experimental model based on Gemini 2.0 Pro.\n\n## February 25, 2025\n\n**Model updates:**\n\n- Released\n\n`gemini-2.0-flash-lite`, a generally available (GA) version of Gemini 2.0 Flash-Lite, which is optimized for speed, scale, and cost efficiency.\n\n## February 19, 2025\n\n**AI Studio updates:**\n\n- Support for additional regions (Kosovo, Greenland and Faroe Islands).\n\n**API updates:**\n\n- Support for additional regions (Kosovo, Greenland and Faroe Islands).\n\n## February 18, 2025\n\n**Model updates:**\n\n- Gemini 1.0 Pro is no longer supported. For the list of supported models, see Gemini models.\n\n## February 11, 2025\n\n**API updates:**\n\n- Updates on the OpenAI libraries compatibility.\n\n## February 6, 2025\n\n**Model updates:**\n\n- Released\n\n`imagen-3.0-generate-002`, a generally available (GA) version of Imagen 3 in the Gemini API.\n\n**SDK updates:**\n\n- Released the Google Gen AI SDK for Java for public preview.",
            "domain": "ai.google.dev"
          },
          {
            "position": 7,
            "title": "Google's $249 AI video tool is incredible — but this one feature left ...",
            "url": "https://www.tomsguide.com/ai/ai-image-video/i-tried-google-veo-3-heres-what-impressed-me-and-what-still-needs-work",
            "snippet": "Google’s Veo 3 made a splash at Google I/O 2025 as the latest leap forward in AI-powered video generation.\n\nAs a feature within the new Google AI Ultra subscription, the highest access to Google’s most advanced models and premium features, you can get Veo 3 and Flow, which strings videos together for a more robust cinematic feature.\n\nDesigned to turn simple text prompts into hyper-realistic clips — with audio, dialogue, and sound effects — it’s arguably one of the most advanced tools available to casual creators. And after testing it hands-on, I can say it delivers some truly jaw-dropping results. But it also comes with its share of hiccups.\n\nHere’s what I loved about Veo 3 — and what left me frustrated.\n\n## What Veo 3 gets right\n\nI’ve tested my fair share of AI video tools, including earlier versions of Veo, and this latest release is by far the most user-friendly when it comes to adding sound and dialogue.\n\nThe realism is genuinely impressive — especially for the fact that the 8-second clips can be generated in under two minutes on a computer without a full production crew.\n\nUsers across the internet are sharing clips that are nearly indistinguishable from human-made videos. It’s a glimpse at just how fast this tech is moving.\n\nGoogle Veo 3 realism just broke the Internet yesterday.This is 100% AI10 wild examples:1. Street interview that never happened pic.twitter.com/qdxZVhOO3GMay 22, 2025... ## Where Veo 3 still struggles\n\nFor all its strengths, Veo 3 still has a ways to go before it’s seamless. Obviously, it's still in experimental mode, so Google is working out the kinks, but here are the biggest issues I ran into while testing:\n\nGet instant access to breaking news, the hottest reviews, great deals and helpful tips.\n\n### 1. Prompt interpretation feels hit-or-miss\n\nVeo 3 sometimes struggles with spatial prompts, like when I asked for an overhead camera angle but got a slightly tilted side view instead. It seems to prioritize cinematic flair over strict prompt accuracy, which limits creative control.\n\n### 2. Audio doesn’t always work — and it’s not obvious why\n\nBy default, Veo runs in Veo 2 mode, which does not include audio. I only realized this after a few silent clips and some digging. You’ll need to manually switch to \"Experiential Mode\" under the \"Quality\" tab to activate audio and dialogue.\n\nEven then, lip-syncing is inconsistent, and dialogue sometimes drops out altogether, like a badly dubbed foreign film. Additionally, the subtitles are almost always wrong or misspelled.\n\n### 3. Complex scenes throw it off\n\nVeo 3 shines with single-subject clips, but longer or more intricate scenes can fall apart. The narrative gets muddy, and character interactions often feel stiff or repetitive. If you're aiming to create a multi-character, multi-scene story, temper your expectations.... ### 4. The interface still needs polish\n\nThere were moments when the interface felt unintuitive or unstable. I experienced an unexpected session timeout that erased a generated video, and I couldn’t find a recovery option.\n\nAdditionally, when I prompted the model to add dialogue within the scene I got something that did not fit the scenario at all.\n\nFor a tool this powerful, the UX still feels a bit rough around the edges.\n\n### 5. It raises some big ethical questions\n\nVeo’s realism is incredible — and a little unsettling. There’s growing concern that ultra-realistic, AI-generated videos could blur the lines between fact and fiction, especially as this tech becomes more accessible. It also sparks new debates around authorship and originality in creative work.\n\n### Promising but pricey\n\nVeo 3 is a huge step forward in AI video — especially for casual users who want fast, high-quality results. But at $249 per month (with a discounted rate for the first three months), the Google AI Ultra package is a steep price to pay for a tool that still has some notable bugs.\n\nIf you’re just looking to experiment with video generation or create basic promotional content, Veo 3 is exciting — but not yet essential. For professional creators, though, it’s worth watching closely. Just keep your prompts tight, your expectations realistic and your finger ready to re-render.... ### More from Tom's Guide\n\n- This $12.99/month hack gives you access to Google’s Veo 3 AI video tool — here’s how\n\n- Claude Opus 4 is here — and it might be the smartest AI assistant yet\n\n- The only 5 prompt types you need to master ChatGPT (and any other chatbot)\n\nAmanda Caswell is an award-winning journalist, bestselling YA author, and one of today’s leading voices in AI and technology. A celebrated contributor to various news outlets, her sharp insights and relatable storytelling have earned her a loyal readership. Amanda’s work has been recognized with prestigious honors, including outstanding contribution to media.\n\nKnown for her ability to bring clarity to even the most complex topics, Amanda seamlessly blends innovation and creativity, inspiring readers to embrace the power of AI and emerging technologies. As a certified prompt engineer, she continues to push the boundaries of how humans and AI can work together.\n\nBeyond her journalism career, Amanda is a long-distance runner and mom of three. She lives in New Jersey.\n\nYou must confirm your public display name before commenting\n\nPlease logout and then login again, you will then be prompted to enter your display name.",
            "domain": "www.tomsguide.com"
          },
          {
            "position": 8,
            "title": "Google DeepMind's Veo 3 floods internet with realistic videos",
            "url": "https://www.axios.com/2025/05/23/google-ai-videos-veo-3",
            "snippet": "# Google's new AI video tool floods internet with real-looking clips\n\nGoogle's newest AI video generator, Veo 3, generates clips that most users online can't seem to distinguish from those made by human filmmakers and actors.\n\n**Why it matters: **Veo 3 videos shared online are amazing viewers with their realism — and also terrifying them with a sense that real and fake have become hopelessly blurred.\n\n**The big picture: **Unlike OpenAI's video generator Sora, released more widely last December, Google DeepMind's Veo 3 can include dialogue, soundtracks and sound effects.\n\n- The model excels at following complex prompts and translating detailed descriptions into realistic videos.\n\n- The AI engine abides by real-world physics, offers accurate lip-syncing, rarely breaks continuity and generates people with lifelike human features, including five fingers per hand.\n\n- According to examples shared by Google and from users online, the telltale signs of synthetic content are mostly absent.\n\n**Case in point: **In one viral example posted on X, filmmaker and molecular biologist Hashem Al-Ghaili shows a series of short films of AI-generated actors railing against their AI creators and prompts.\n\n**Special effects technology,** video-editing apps and camera tech advances have been changing Hollywood for many decades, but artificially generated films pose a novel challenge to human creators.... - In a promo video for Flow, Google's new video tool that includes Veo 3, filmmakers say the AI engine gives them a new sense of freedom with a hint of eerie autonomy.\n\n- \"It feels like it's almost building upon itself,\" filmmaker Dave Clark says.\n\n**How it works: **Veo 3 was announced at Google I/O on Tuesday and is available now to $249-a-month Google AI Ultra subscribers in the United States.\n\n**Between the lines: **Google says Veo 3 was \"informed by our work with creators and filmmakers,\" and some creators have embraced new AI tools. But the spread of the videos online is also dismaying many video professionals and lovers of art.\n\n- Some dismiss any AI-generated video as \"slop,\" regardless of its technical proficiency or lifelike qualities — but, as Axios' Ina Fried points out, AI slop is in the eye of the beholder.\n\n- The tool could also be useful for more commercial marketing and media work, AI analyst Ethan Mollick writes.\n\n**It's unclear how Google trained Veo 3 **and how that might affect the creativity of its outputs.\n\n- 404 Media found that Veo 3 generated the same lame dad joke for several users who prompted it to create a video of a man doing stand-up comedy.\n\n- Likewise, last year, YouTuber Marques Brownlee asked Sora to create a video of a \"tech reviewer sitting at a desk.\" The generated video featured a fake plant that's nearly identical to the shrub Brownlee keeps on his desk for many of his videos — suggesting the tool may have been trained on them.\n\n**What we're watching:** As hyperrealistic AI-generated videos become even easier to produce, the world hasn't even begun to sort out how to manage authorship, consent, rights and the film industry's future.\n\n##### Go deeperJul 10, 2025 - Technology... ## Google AI's new trick: Turn any image into a brief video\n\nGoogle's latest AI video tool, Veo 3, now generates short movies with sound based only on still photos and prompts.\n\nGo deeper (1 min. read)\n\n**The big picture: **The feature, released Thursday, is available to Ultra and Pro users on the web and soon on mobile for subscribers in select regions, Google shared with Axios.\n\n## Google avatars shake up workplace video making\n\nGoogle Vids is now providing users of the workplace video creation tool with a set of pre-made avatars for use in brief AI-generated videos, the company said Wednesday.\n\nGo deeper (2 min. read)\n\n**Why it matters: **The rise of cheap, convenient AI video generation threatens jobs for video producers, editors, camera operators and even commercial actors.\n\n## AI slop is ruining all of our favorite places to scroll\n\nAn AI-generated video of rabbits jumping on a trampoline that went viral this week — and was widely believed to be real — proved even cute animal vids aren't safe from convincing slop machines.\n\nGo deeper (2 min. read)\n\n**Why it matters: **All the fake AI-generated content online is sapping the joy of casual scrolling.",
            "domain": "www.axios.com"
          },
          {
            "position": 9,
            "title": "Introducing Veo 3.1 and advanced capabilities in Flow - Google Blog",
            "url": "https://blog.google/technology/ai/veo-updates-flow/",
            "snippet": "# Introducing Veo 3.1 and advanced capabilities in Flow\n\nFive months ago, we introduced Flow, our AI filmmaking tool powered by Veo, and have been inspired by the creativity it has sparked with over 275 million videos generated in Flow\n\n1\n\n. We're always listening to your feedback, and we've heard that you want more artistic control within Flow, with increased support for audio across all features.\n\nToday, we’re introducing new and enhanced creative capabilities to edit your clips, giving you more granular control over your final scene. For the first time, we’re also bringing audio to existing capabilities like “Ingredients to Video,” “Frames to Video” and “Extend.”\n\nWe’re also introducing Veo 3.1, which brings richer audio, more narrative control, and enhanced realism that captures true-to-life textures. Veo 3.1 is state-of-the-art and builds on Veo 3, with stronger prompt adherence and improved audiovisual quality when turning images into videos.\n\n## Refine your narrative with audio and more control\n\nWith Veo 3.1, we’re bringing audio to existing capabilities to help you craft the perfect scene. These features are experimental and actively improving, and we’re excited to see what you create as we iterate based on your feedback.\n\nNow, with rich, generated audio, you can:\n\n**Craft the look of your scene.**With \"Ingredients to Video,\" you can use multiple reference images to control the characters, objects and style. Flow uses your ingredients to create a final scene that looks just as you envisioned. **Control the shot from start to finish.**Provide a starting and ending image with “Frames to Video,” and Flow will generate a seamless video that bridges the two, perfect for artful and epic transitions. **Create longer, seamless shots.**With \"Extend,\" you can create longer videos, even lasting for a minute or more, that connect to and continue the action from your original clip. Each video is generated based on the final second of your previous clip, making it most useful for creating a longer establishing shot.... ## Edit your ingredients and videos with more precision\n\nGreat ideas can strike at any point in the creative process. For moments when the first take isn't the final one, we're introducing new editing capabilities directly within Flow to help you reimagine and perfect your scenes.\n\n**Add new elements to any scene.**With “Insert,” introduce anything you can imagine, from realistic details to fantastical creatures. Flow now handles complex details like shadows and scene lighting, making the addition look natural. **Remove unwanted objects or characters seamlessly.**Soon, you’ll be able to take anything out of a scene, and Flow will reconstruct the background and surroundings, making it look as though the object was never there.\n\n## Start creating in Flow today\n\nWith more precise editing capabilities, audio across all existing features and higher-quality outputs powered by Veo 3.1, we're opening up new possibilities for richer, more powerful video storytelling right inside Flow.\n\nThe Veo 3.1 model is also available via the Gemini API for developers, Vertex AI for enterprise customers, and the Gemini app. New capabilities are available in both Gemini API\n\n2\n\nand Vertex AI\n\n3",
            "domain": "blog.google"
          },
          {
            "position": 10,
            "title": "Introducing Veo 3.1 and new creative capabilities in the Gemini API",
            "url": "https://developers.googleblog.com/en/introducing-veo-3-1-and-new-creative-capabilities-in-the-gemini-api/",
            "snippet": "**Today, we are releasing Veo 3.1 and Veo 3.1 Fast in paid preview in the** **Gemini API** **.** This updated model offers several improvements, as well as improved outputs when generating video from images. These new models are available via the Gemini API in Google AI Studio and Vertex AI. Veo 3.1 is also available in the Gemini app and Flow.\n\nVeo 3.1 and Veo 3.1 Fast empower developers to create more engaging content through significant upgrades. The models now generate\n\n**richer native audio**, from natural conversations to synchronized sound effects, and offer **greater narrative control** with an improved understanding of cinematic styles. **Enhanced image-to-video** capabilities ensure better prompt adherence while delivering superior audio and visual quality and maintaining character consistency across multiple scenes.\n\nIn addition to the model update, we are introducing new model capabilities, including using reference images to guide video generation, extend existing Veo videos, and generate transitions between a first and last frame.\n\nYou can now guide the generation process by providing up to 3 reference images of a character, object, or scene. This is helpful for maintaining character consistency across multiple shots or applying a specific style to your video.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nconfig=types.GenerateVideosConfig(\n\nreference_images=[reference_image1, reference_image2],\n\n),\n\n\n\n```... Your story is no longer limited by the original generation. With Scene extension, you can create longer videos, even lasting for a minute or more, by generating new clips that connect to your previous video. Each new video is generated based on the final second of the previous clip. This maintains visual continuity, making the feature ideal for extending shots with background audio.\n\n```\n\nfrom google import genai\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nvideo=video_to_extend\n\n\n\n```\n\nCreate smooth, natural scenes that bridge two different images. By providing a starting and an ending image, you can direct Veo 3.1 to generate the transition between them, complete with accompanying audio.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nimage=first_frame,\n\nconfig=types.GenerateVideosConfig(\n\nlast_frame=last_frame,\n\n),\n\n\n\n```\n\n**What people are building with Veo 3.1**\n\nPromise Studios, a GenAI movie studio, uses Veo 3.1 within its MUSE Platform to enhance generative storyboarding and previsualization for director-driven storytelling at production quality.... Latitude is experimenting with Veo 3.1 in its generative narrative engine to instantly bring user-created stories to life.\n\nVeo 3.1 and these new capabilities are now available in paid preview via the Gemini API.\n\nVeo 3.1 is the same price as Veo 3. We are incredibly excited to see what you will build with these new capabilities!",
            "domain": "developers.googleblog.com"
          }
        ],
        "success": true,
        "error": null
      }
    ],
    "evaluation": {
      "overall_verdict": "TRUE",
      "confidence_score": 0.95,
      "sub_claim_verdicts": [
        {
          "claim_id": "claim_1",
          "statement": "Google has released a software version named veo3.1.",
          "verdict": "TRUE",
          "confidence": 0.95,
          "supporting_count": 7,
          "refuting_count": 0,
          "dependency_status": "foundational",
          "key_evidence": [
            {
              "title": "Introducing Veo 3.1 and advanced capabilities in Flow - Google Blog",
              "url": "https://blog.google/technology/ai/veo-updates-flow/",
              "credibility_tier": 2,
              "supports_claim": true
            },
            {
              "title": "Google announces Veo 3.1 and updates Flow with more controls, tools",
              "url": "https://9to5google.com/2025/10/15/veo-3-1/",
              "credibility_tier": 3,
              "supports_claim": true
            },
            {
              "title": "Google releases Veo 3.1, adds it to Flow video editor",
              "url": "https://techcrunch.com/2025/10/15/google-releases-veo-3-1-adds-it-to-flow-video-editor/",
              "credibility_tier": 3,
              "supports_claim": true
            }
          ],
          "rationale": "Multiple credible sources, including Google's official blog and reputable tech news sites, confirm the release of Veo 3.1."
        }
      ],
      "dependency_analysis": {
        "foundational_claims_verified": true,
        "broken_dependencies": [],
        "notes": "All foundational claims have been verified as true."
      },
      "summary": "The claim that Google has released a software version named Veo 3.1 is supported by multiple credible sources, including Google's official announcements and reputable tech news outlets. Therefore, the overall verdict is TRUE.",
      "key_findings": [
        "Google's official blog and multiple reputable tech news sites confirm the release of Veo 3.1.",
        "There is strong evidence from Tier 2 and Tier 3 sources supporting the claim."
      ],
      "limitations": "No significant limitations or conflicting evidence found."
    },
    "execution_log": [
      {
        "timestamp": "2025-10-19T13:35:34.675031",
        "step": "classification",
        "agent": "classifier_agent",
        "input_preview": "does google release veo3.1?",
        "output_preview": "{'domain': 'Science', 'claim_type': 'Factual', 'complexity': 'Simple', 'urgency': 'Medium', 'rationale': \"The claim is about a potential release of a new version of Google's software, which falls unde",
        "full_output": {
          "domain": "Science",
          "claim_type": "Factual",
          "complexity": "Simple",
          "urgency": "Medium",
          "rationale": "The claim is about a potential release of a new version of Google's software, which falls under the domain of science (technology). It is a factual question that can be verified. The claim is simple as it involves a single inquiry. The urgency is medium because it pertains to a potential software release, which is important but not an immediate emergency."
        }
      },
      {
        "timestamp": "2025-10-19T13:35:36.639687",
        "step": "decomposition",
        "agent": "decomposer_agent",
        "input_preview": "{'domain': 'Science', 'claim_type': 'Factual', 'complexity': 'Simple', 'urgency': 'Medium', 'rationale': \"The claim is about a potential release of a new version of Google's software, which falls unde",
        "output_preview": "{'original_claim': 'does google release veo3.1?', 'atomic_claims': [{'id': 'claim_1', 'statement': 'Google has released a software version named veo3.1.', 'dependencies': [], 'type': 'fact', 'entities",
        "full_output": {
          "original_claim": "does google release veo3.1?",
          "atomic_claims": [
            {
              "id": "claim_1",
              "statement": "Google has released a software version named veo3.1.",
              "dependencies": [],
              "type": "fact",
              "entities": [
                "Google",
                "veo3.1"
              ],
              "temporal": "2025-10-19",
              "quantitative": "",
              "priority": "high"
            }
          ],
          "dependency_graph": {
            "foundational": [
              "claim_1"
            ]
          },
          "total_claims": 1
        }
      },
      {
        "timestamp": "2025-10-19T13:35:48.277170",
        "step": "question_generation",
        "agent": "question_agent",
        "input_preview": "{'original_claim': 'does google release veo3.1?', 'atomic_claims': [{'id': 'claim_1', 'statement': 'Google has released a software version named veo3.1.', 'dependencies': [], 'type': 'fact', 'entities",
        "output_preview": "{'current_date_used': '2025-10-19', 'queries': [{'id': 'q1', 'query': 'Google official announcement veo3.1 release 2025', 'claim_id': 'claim_1', 'query_type': 'source_verification', 'priority': 'high'",
        "full_output": {
          "current_date_used": "2025-10-19",
          "queries": [
            {
              "id": "q1",
              "query": "Google official announcement veo3.1 release 2025",
              "claim_id": "claim_1",
              "query_type": "source_verification",
              "priority": "high"
            },
            {
              "id": "q2",
              "query": "Google veo3.1 release date 2025",
              "claim_id": "claim_1",
              "query_type": "direct_fact",
              "priority": "high"
            },
            {
              "id": "q3",
              "query": "veo3.1 software version Google 2025 official",
              "claim_id": "claim_1",
              "query_type": "source_verification",
              "priority": "high"
            },
            {
              "id": "q4",
              "query": "Google veo3.1 release news 2025",
              "claim_id": "claim_1",
              "query_type": "source_verification",
              "priority": "high"
            },
            {
              "id": "q5",
              "query": "veo3.1 Google software version 2025",
              "claim_id": "claim_1",
              "query_type": "direct_fact",
              "priority": "high"
            },
            {
              "id": "q6",
              "query": "Google veo3.1 release statement 2025",
              "claim_id": "claim_1",
              "query_type": "source_verification",
              "priority": "high"
            },
            {
              "id": "q7",
              "query": "veo3.1 Google software version 2025 official data",
              "claim_id": "claim_1",
              "query_type": "statistical",
              "priority": "high"
            },
            {
              "id": "q8",
              "query": "Google veo3.1 release 2025 debunked",
              "claim_id": "claim_1",
              "query_type": "contradiction",
              "priority": "high"
            },
            {
              "id": "q9",
              "query": "veo3.1 Google software version 2025 false",
              "claim_id": "claim_1",
              "query_type": "contradiction",
              "priority": "high"
            },
            {
              "id": "q10",
              "query": "Google veo3.1 release 2025 misleading",
              "claim_id": "claim_1",
              "query_type": "contradiction",
              "priority": "high"
            }
          ],
          "total_queries": 10,
          "strategy_rationale": "The queries are designed to verify the foundational claim 'Google has released a software version named veo3.1.' by using specific entities, dates, and authoritative source keywords. The queries vary in type to cover direct facts, source verification, and contradiction checks, ensuring a comprehensive fact-checking approach."
        }
      },
      {
        "timestamp": "2025-10-19T13:35:51.532127",
        "step": "search_execution",
        "agent": "perplexity_api",
        "input_preview": "[{'id': 'q1', 'query': 'Google official announcement veo3.1 release 2025', 'claim_id': 'claim_1', 'query_type': 'source_verification', 'priority': 'high'}, {'id': 'q2', 'query': 'Google veo3.1 release",
        "output_preview": "[{'query_id': 'q2', 'query': 'Google veo3.1 release date 2025', 'claim_id': 'claim_1', 'query_type': 'direct_fact', 'priority': 'high', 'results': [{'position': 1, 'title': 'Veo (text-to-video model) ",
        "full_output": [
          {
            "query_id": "q2",
            "query": "Google veo3.1 release date 2025",
            "claim_id": "claim_1",
            "query_type": "direct_fact",
            "priority": "high",
            "results": [
              {
                "position": 1,
                "title": "Veo (text-to-video model) - Wikipedia",
                "url": "https://en.wikipedia.org/wiki/Veo_(text-to-video_model)",
                "snippet": "**Veo**, or **Google Veo**, is a text-to-video model developed by Google DeepMind and announced in May 2024. As a generative AI model, it creates videos based on user prompts. Veo 3, released in May 2025, can also generate accompanying audio.\n\n## Development\n\nIn May 2024, a multimodal video generation model called Veo was announced at Google I/O 2024. Google claimed that it could generate 1080p videos over a minute long. In December 2024, Google released Veo 2, available via VideoFX. It supports 4K resolution video generation and has an improved understanding of physics. In April 2025, Google announced that Veo 2 became available for advanced users on the Gemini app.\n\nIn May 2025, Google released Veo 3, which not only generates videos but also creates synchronized audio — including dialogue, sound effects, and ambient noise — to match the visuals. Google also announced **Flow**, a video-creation tool powered by Veo and Imagen. Google DeepMind CEO Demis Hassabis described the release as the moment when AI video generation left the era of the silent film.... ## Capabilities and limitations\n\nGoogle Veo can be bought by several subscription/membership tiers, and/or by using Google \"AI credits\". The software itself can be run by two different consoles called Google Gemini and Google Flow, with Gemini being geared towards shorter, quicker, and faster projects, using the Gemini AI chat model, or through Google Flow, which is essentially a movie editor, as well, allowing users to create longer projects, and continuity using the same characters and actors. Users can create a maximum length of eight seconds per clip.\n\nGoogle Veo, has a relatively simple interface and dashboard, however writing prompts, for those who have little to no experience in transcribing or filmmaking may face issues with the software misunderstanding what the user intended by their prompt (no matter how detailed it was). So although Veo does have a friendly and simple setup, prompts, which are the forefront of the software, need to be not only short and to the point, but they also must be very specific, if the user wants the right vision for their project. Google Veo, when it comes to human models, is able to generate several ethnicity and body types. The software is also capable of generating stand up comedy routines, and Music videos. It can as well generate animals, cartoons, and animation. Prompts must accurately describe places, people, and things in each scene, in addition knowledge of film and camera lingo such as panning, zooming, and terms for camera angles, are also important.... Google Veo however, has strict guidelines and blockades to their software. Before a clip is generated, the algorithm computer software reviews it, and if it is anything deemed inappropriate, too graphically sexual, illegal, showcasing graphic abuse/assault/fighting (unless the prompt specifies that it is a fictitious martial arts scene etc.) gross behaviors, antisemitism, racist, homophobic, anything depicting reigning regimes, rioting, blood, gore, or warfare, (unless in some cases the prompt specifies that it is fictitious period drama, the clip may still be generated), the clip will not be generated. In addition, Google Veo cannot and will not generate character actors that look identical to celebrities or real-life individuals. Users have primarily complained that, regardless of how descriptive and detailed their prompts are, Google Veo often misunderstands the input, resulting in completely different outputs. Common issues include the emulation of incorrect subtitles and captions, the generation of complex scenes that are incomplete due to the maximum eight-second length, the production of garbled and nonsensical speech, and character models that appear deformed in both appearance and movement. Users have also reported that their prompts and generated content are falsely flagged as violating guidelines, along with a variety of other issues and complaints. However, trial and error may have to be used with Veo for optimal results.... ## Reactions\n\nA reporter for *Gizmodo* reacted to the release of Veo 3 by observing that users were directing the model to generate low-quality content, such as man on the street interviews or haul videos of people unboxing products. Another media commentator reported that the tool tended to repeat the same joke in response to different prompts.\n\nCommentators speculated that Google had trained the service on YouTube videos or Reddit posts. Google itself had not stated the source of its training content.\n\nIn July 2025, Media Matters for America reported that racist and antisemitic videos generated using Veo 3 were being uploaded to TikTok. Ryan Whitwam of *Ars Technica* commented, \"In a perfect world, Veo 3 would refuse to create these videos, but vagueness in the prompt and the AI's inability to understand the subtleties of racist tropes (i.e., the use of monkeys instead of humans in some videos) make it easy to skirt the rules.\"\n\n## See also\n- Sora (text-to-video model)\n- VideoPoet – Text-to-video model by Google\n- Dream Machine (text-to-video model)\n\n## References\n\n## External links\n- Official website\n- *Introducing Veo 3.1 and advanced capabilities in Flow*\n\nCategories: - 2024 software\n- Applications of artificial intelligence\n- Film and video technology\n- Google DeepMind\n- Text-to-video generation\n- Video processing\n- Generative artificial intelligence\n- 2024 in artificial intelligence",
                "domain": "en.wikipedia.org"
              },
              {
                "position": 2,
                "title": "Introducing Veo 3.1 and advanced capabilities in Flow - Google Blog",
                "url": "https://blog.google/technology/ai/veo-updates-flow/",
                "snippet": "# Introducing Veo 3.1 and advanced capabilities in Flow\n\nFive months ago, we introduced Flow, our AI filmmaking tool powered by Veo, and have been inspired by the creativity it has sparked with over 275 million videos generated in Flow\n\n1\n\n. We're always listening to your feedback, and we've heard that you want more artistic control within Flow, with increased support for audio across all features.\n\nToday, we’re introducing new and enhanced creative capabilities to edit your clips, giving you more granular control over your final scene. For the first time, we’re also bringing audio to existing capabilities like “Ingredients to Video,” “Frames to Video” and “Extend.”\n\nWe’re also introducing Veo 3.1, which brings richer audio, more narrative control, and enhanced realism that captures true-to-life textures. Veo 3.1 is state-of-the-art and builds on Veo 3, with stronger prompt adherence and improved audiovisual quality when turning images into videos.\n\n## Refine your narrative with audio and more control\n\nWith Veo 3.1, we’re bringing audio to existing capabilities to help you craft the perfect scene. These features are experimental and actively improving, and we’re excited to see what you create as we iterate based on your feedback.\n\nNow, with rich, generated audio, you can:\n\n**Craft the look of your scene.**With \"Ingredients to Video,\" you can use multiple reference images to control the characters, objects and style. Flow uses your ingredients to create a final scene that looks just as you envisioned. **Control the shot from start to finish.**Provide a starting and ending image with “Frames to Video,” and Flow will generate a seamless video that bridges the two, perfect for artful and epic transitions. **Create longer, seamless shots.**With \"Extend,\" you can create longer videos, even lasting for a minute or more, that connect to and continue the action from your original clip. Each video is generated based on the final second of your previous clip, making it most useful for creating a longer establishing shot.... ## Edit your ingredients and videos with more precision\n\nGreat ideas can strike at any point in the creative process. For moments when the first take isn't the final one, we're introducing new editing capabilities directly within Flow to help you reimagine and perfect your scenes.\n\n**Add new elements to any scene.**With “Insert,” introduce anything you can imagine, from realistic details to fantastical creatures. Flow now handles complex details like shadows and scene lighting, making the addition look natural. **Remove unwanted objects or characters seamlessly.**Soon, you’ll be able to take anything out of a scene, and Flow will reconstruct the background and surroundings, making it look as though the object was never there.\n\n## Start creating in Flow today\n\nWith more precise editing capabilities, audio across all existing features and higher-quality outputs powered by Veo 3.1, we're opening up new possibilities for richer, more powerful video storytelling right inside Flow.\n\nThe Veo 3.1 model is also available via the Gemini API for developers, Vertex AI for enterprise customers, and the Gemini app. New capabilities are available in both Gemini API\n\n2\n\nand Vertex AI\n\n3",
                "domain": "blog.google"
              },
              {
                "position": 3,
                "title": "Release notes | Gemini API | Google AI for Developers",
                "url": "https://ai.google.dev/gemini-api/docs/changelog",
                "snippet": "This page documents updates to the Gemini API.\n\n## October 17, 2025\n\n**Grounding with Google Maps**is now generally available. For more information, see Grounding with Google Maps documentation.\n\n## October 15, 2025\n\nReleased Veo 3.1 and 3.1 Fast models in public preview, with new features including:\n\n- Extending Veo-created videos.\n\n- Referencing up to three images to generate a video.\n\n- Providing first and last frame images to generate videos from.\n\nThis launch also added more options for Veo 3 output video durations: 4, 6, and 8 seconds.\n\nDeprecation for\n\n`veo-3.0-generate-preview`and\n\n`veo-3.0-fast-generate-preview`coming October 22, 2025.\n\n## October 7, 2025\n\n- Launched Gemini 2.5 Computer Use Preview\n\n## October 2, 2025\n\n- Launched Gemini 2.5 Flash Image GA: Image Generation with Gemini\n\n## September 29, 2025\n\n- The following Gemini 1.5 models are now deprecated:\n\n`gemini-1.5-pro`\n\n`gemini-1.5-flash-8b`\n\n`gemini-1.5-flash`... ## September 9, 2025\n\n- Launched Veo 3 and Veo 3 Fast GA, with lower pricing and new options for aspect ratios, resolution, and seeding. Read the Veo documentation for more information.\n\n## August 26, 2025\n\n- Launched Gemini 2.5 Image Preview, our latest native image generation model.\n\n## August 18, 2025\n\n- Released URL context tool to general\n\navailability (GA), a tool for providing URLs as additional context to\n\nprompts. Support for using URL context with the\n\n`gemini-2.0-flash`model (available during experimental release) will be discontinued in one week.\n\n## August 14, 2025\n\n- Released Imagen 4 Ultra, Standard and Fast models as generally available (GA). To learn more, see the Imagen page.\n\n## August 7, 2025\n\n`allow_adult`setting in Image to Video generation are now available in restricted regions. See the Veo page for details.\n\n## July 31, 2025\n\n- Launched image-to-video generation for the Veo 3 Preview model.\n\n- Released Veo 3 Fast Preview model.\n\n- To learn more about Veo 3, visit the Veo page.... ## July 22, 2025\n\n- Released\n\n`gemini-2.5-flash-lite`, our fast, low-cost, high-performance Gemini 2.5 model. To learn more, see Gemini 2.5 Flash-Lite.\n\n## July 17, 2025\n\nLaunched\n\n`veo-3.0-generate-preview`, the latest update to Veo introducing video with audio generation. To learn more about Veo 3, visit the Veo page.\n\nIncreased rate limits for Imagen 4 Standard and Ultra. Visit the Rate limits page for more details.\n\n## July 14, 2025\n\n- Released\n\n`gemini-embedding-001`, the stable version of our text embedding model. To learn more, see embeddings. The\n\n`gemini-embedding-exp-03-07`model will be deprecated on August 14, 2025.\n\n## July 7, 2025\n\n- Launched Gemini API Batch Mode. Batch up requests and send them to process asynchronously. To learn more, see Batch Mode.\n\n## June 26, 2025\n\nThe preview models\n\n`gemini-2.5-pro-preview-05-06`and\n\n`gemini-2.5-pro-preview-03-25`are now redirecting to the latest stable version\n\n`gemini-2.5-pro`.\n\n`gemini-2.5-pro-exp-03-25`is deprecated.... ## May 7, 2025\n\n- Released\n\n`gemini-2.0-flash-preview-image-generation`, a preview model for generating and editing images. To learn more, see Image generation and Gemini 2.0 Flash Preview Image Generation.\n\n## May 6, 2025\n\n- Released\n\n`gemini-2.5-pro-preview-05-06`, a new version of our most powerful model, with improvements on code and function calling.\n\n`gemini-2.5-pro-preview-03-25`will automatically point to the new version of the model.\n\n## April 17, 2025\n\n- Released\n\n`gemini-2.5-flash-preview-04-17`, a Gemini preview model optimized for price-performance and adaptive thinking. To learn more, see Gemini 2.5 Flash Preview and Thinking.\n\n## April 16, 2025\n\n- Launched context caching for Gemini 2.0 Flash.... ## April 9, 2025\n\n**Model updates:**\n\n- Released\n\n`veo-2.0-generate-001`, a generally available (GA) text- and image-to-video model, capable of generating detailed and artistically nuanced videos. To learn more, see the Veo docs.\n\nReleased\n\n`gemini-2.0-flash-live-001`, a public preview version of the Live API model with billing enabled.\n\n**Enhanced Session Management and Reliability** **Session Resumption:**Keep sessions alive across temporary network disruptions. The API now supports server-side session state storage (for up to 24 hours) and provides handles (session_resumption) to reconnect and resume where you left off. **Longer Sessions via Context Compression:**Enable extended interactions beyond previous time limits. Configure context window compression with a sliding window mechanism to automatically manage context length, preventing abrupt terminations due to context limits. **Graceful Disconnect Notification:**Receive a\n\n`GoAway`server message indicating when a connection is about to close, allowing for graceful handling before termination.\n\n\n\n**More Control over Interaction Dynamics** **Configurable Voice Activity Detection (VAD):**Choose sensitivity levels or disable automatic VAD entirely and use new client events (\n\n`activityStart`,\n\n`activityEnd`) for manual turn control.\n\n**Configurable Interruption Handling:**Decide whether user input should interrupt the model's response. **Configurable Turn Coverage:**Choose whether the API processes all audio and video input continuously or only captures it when the end-user is detected speaking. **Configurable Media Resolution:**Optimize for quality or token usage by selecting the resolution for input media. **Richer Output and Features** **Expanded Voice & Language Options:**Choose from two new voices and 30 new languages for audio output. The output language is now configurable within\n\n`speechConfig`.\n\n**Text Streaming:**Receive text responses incrementally as they are generated, enabling faster display to the user. **Token Usage Reporting:**Gain insights into usage with detailed token counts provided in the\n\n`usageMetadata`field of server messages, broken down by modality and prompt or response phases.... ## April 4, 2025\n\n- Released\n\n`gemini-2.5-pro-preview-03-25`, a public preview Gemini 2.5 Pro version with billing enabled. You can continue to use\n\n`gemini-2.5-pro-exp-03-25`on the free tier.\n\n## March 25, 2025\n\n- Released\n\n`gemini-2.5-pro-exp-03-25`, a public experimental Gemini model with thinking mode always on by default. To learn more, see Gemini 2.5 Pro Experimental.\n\n## March 12, 2025\n\n**Model updates:**\n\n- Launched an experimental Gemini 2.0 Flash model capable of image generation and editing.\n\n- Released\n\n`gemma-3-27b-it`, available on AI Studio and through the Gemini API, as part of the Gemma 3 launch.\n\n**API updates:**\n\n- Added support for YouTube URLs as a media source.\n\n- Added support for including an inline video of less than 20MB.\n\n## March 11, 2025\n\n**SDK updates:**\n\n- Released the Google Gen AI SDK for TypeScript and JavaScript to public preview.\n\n## March 7, 2025\n\n**Model updates:**\n\n- Released\n\n`gemini-embedding-exp-03-07`, an experimental Gemini-based embeddings model in public preview.... ## February 28, 2025\n\n**API updates:**\n\n- Support for Search as a tool\n\nadded to\n\n`gemini-2.0-pro-exp-02-05`, an experimental model based on Gemini 2.0 Pro.\n\n## February 25, 2025\n\n**Model updates:**\n\n- Released\n\n`gemini-2.0-flash-lite`, a generally available (GA) version of Gemini 2.0 Flash-Lite, which is optimized for speed, scale, and cost efficiency.\n\n## February 19, 2025\n\n**AI Studio updates:**\n\n- Support for additional regions (Kosovo, Greenland and Faroe Islands).\n\n**API updates:**\n\n- Support for additional regions (Kosovo, Greenland and Faroe Islands).\n\n## February 18, 2025\n\n**Model updates:**\n\n- Gemini 1.0 Pro is no longer supported. For the list of supported models, see Gemini models.\n\n## February 11, 2025\n\n**API updates:**\n\n- Updates on the OpenAI libraries compatibility.\n\n## February 6, 2025\n\n**Model updates:**\n\n- Released\n\n`imagen-3.0-generate-002`, a generally available (GA) version of Imagen 3 in the Gemini API.\n\n**SDK updates:**\n\n- Released the Google Gen AI SDK for Java for public preview.",
                "domain": "ai.google.dev"
              },
              {
                "position": 4,
                "title": "Build with Veo 3, now available in the Gemini API",
                "url": "https://developers.googleblog.com/en/veo-3-now-available-gemini-api/",
                "snippet": "First unveiled at Google I/O 2025, people around the world have already generated tens of millions of high-quality videos with Veo 3 (along with some new fun and interesting video trends). It is our first video model to incorporate high-fidelity video outputs and native audio, first with text-to-video and soon with image-to-video.\n\nDevelopers are already experimenting with Veo 3, discovering how the model can help them brainstorm content, rapidly iterate, and be more efficient.\n\nVeo 3 is designed to handle a range of video generation tasks, from cinematic narratives to dynamic character animations. With Veo 3, you can create more immersive experiences by not only generating stunning visuals, but also audio like dialogue and sound effects.\n\nLet’s take a look at some examples.\n\nExplore these examples and more with Veo 3 in Google AI Studio, available as an SDK template and interactive Starter App to remix, copy and extend. The Starter App and its sample code offer a convenient way for Paid Tier users to rapidly prototype with Veo 3 and more on the Gemini API, directly from Google AI Studio.\n\nClick the Key button in the top right of the AI Studio Build interface to select a Google Cloud Project with billing enabled to use the Paid Tier in AI Studio apps. See the FAQs for more.... Veo 3 will be priced at $0.75 per second for video and audio output. Additionally, Veo 3 Fast will be available soon, offering a faster and more cost-effective option for video creation.\n\nHere’s a basic Python example to create a video:\n\n```\n\nimport time\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.0-generate-preview\",\n\nprompt=\"a close-up shot of a golden retriever playing in a field of sunflowers\",\n\nconfig=types.GenerateVideosConfig(\n\nnegative_prompt=\"barking, woofing\",\n\n),\n\n\n\n# Waiting for the video(s) to be generated\n\nwhile not operation.done:\n\ntime.sleep(20)\n\noperation = client.operations.get(operation)\n\ngenerated_video = operation.result.generated_videos[0]\n\nclient.files.download(file=generated_video.video)\n\ngenerated_video.video.save(\"veo3_video.mp4\")\n\n```\n\nAll videos generated by Veo 3 models will continue to include a digital SynthID watermark. To get started, check out the documentation, cookbook, and a Veo 3 starter app in Google AI Studio:\n\nIn addition to being available via the Gemini API in Google AI Studio, Veo 3 is also available to Google AI subscribers in the Gemini app and Flow, and to enterprise customers via Vertex AI.... 1: Fluffy Characters Stop Motion: Inside a brightly colored, cozy kitchen made of felt and yarn. Professor Nibbles, a plump, fluffy hamster with oversized glasses, nervously stirs a bubbling pot on a miniature stove, muttering, \"Just a little more... 'essence of savory,' as the recipe calls for.\" The camera is a mid-shot, capturing his frantic stirring. Suddenly, the pot emits a loud \"POP!\" followed by a comical \"whoosh\" sound, and a geyser of iridescent green slime erupts, covering the entire kitchen. Professor Nibbles shrieks, \"Oh, dear! Not again!\" and scurries away, leaving a trail of tiny, panicked squeaks.\n\n2: The sequence begins with an extreme close-up of a single gear, slowly turning and reflecting harsh sunlight. The camera gradually pulls back in a continuous movement, revealing this is but one component of a colossal, mechanical heart half-buried in a desolate, rust-colored desert. A sweeping aerial shot establishes its enormous scale and isolation in the barren landscape. The camera descends to capture pipes hissing steam and the rhythmic thumping that echoes across the empty plains. A subtle shake effect synchronizes with each massive heartbeat. A lateral tracking shot discovers tiny, robed figures scurrying across the metallic surface. The camera follows one such figure in a detailed tracking shot as they perform meticulous maintenance, polishing brass valves and tightening immense bolts. A complex movement circles the entire structure, capturing different maintenance teams working in precarious positions across its rusted exterior. The final shot begins tight on the meticulous work of one tiny figure before executing a dramatic pull-out that reveals the true scale of the heart and the minuscule size of its caretakers, tending to the vital organ of an unseen, sleeping giant that extends beyond the frame.",
                "domain": "developers.googleblog.com"
              },
              {
                "position": 5,
                "title": "Meet Flow: AI-powered filmmaking with Veo 3",
                "url": "https://blog.google/technology/ai/google-flow-veo-ai-filmmaking-tool/",
                "snippet": "# Meet Flow: AI-powered filmmaking with Veo 3\n\nToday we’re introducing Flow, our new AI filmmaking tool.\n\nIt’s built by and for creatives, and it’s the only AI filmmaking tool custom-designed for Google’s most advanced models — Veo, Imagen and Gemini. Flow can help storytellers explore their ideas without bounds and create cinematic clips and scenes for their stories. It’s early days, and we’re excited to shape the future of Flow with creatives and filmmakers.\n\n### What’s possible with Flow\n\nFlow is inspired by what it feels like when time slows down and creation is effortless, iterative and full of possibility. It’s custom-designed for Veo, Google’s state-of-the-art generative video model, with exceptional prompt adherence and stunning cinematic outputs that excel at physics and realism. Behind the scenes, Gemini models make prompting intuitive, so you can describe your vision in everyday language. You can bring your own assets to create characters, or use Flow to make your own ingredients with Imagen’s text-to-image capabilities.\n\nOnce you’ve created a subject or a scene, you can integrate those same ingredients into different clips and scenes with consistency. Or you can use a scene image to start a new shot.\n\nCreate your ingredients\n\nUse those ingredients to create a clip\n\nReference ingredients in plain language... ### Key features to unlock your storytelling\n\nFlow also comes with a range of features for professionals or those just getting started:\n\n**Camera Controls:**Master your shot with direct control over camera motion, angles and perspectives. **Scenebuilder:**Seamlessly edit and extend your existing shots — revealing more of the action or transitioning to what happens next with continuous motion and consistent characters. **Asset Management:**Easily manage and organize all of your ingredients and prompts. **Flow TV** **:**Spark your creativity with an ever-growing showcase of clips, channels, and content generated with Veo. You can see the exact prompts and techniques used for clips you like, providing a practical way to learn and adapt new styles.\n\nSeamless transitions\n\nCamera controls\n\nCinematic quality\n\n### Get started with Flow\n\nFlow is the evolution of VideoFX, a Google Labs experiment that launched last year. Starting today, Flow is available to subscribers of our Google AI Pro and Google AI Ultra plans in the U.S., with more countries coming soon.\n\nGoogle AI Pro gives you the key Flow features and 100 generations per month, and Google AI Ultra gives you the highest usage limits and early access to Veo 3 with native audio generation, bringing environmental sounds and character dialogue directly into video creation.... ### How we’re collaborating with filmmakers\n\nAs with any groundbreaking technology, we’re still understanding the full potential of AI in filmmaking. We see the emergence of these tools as an enabler, helping a new wave of filmmakers more easily tell their stories. By offering filmmakers early access to Flow, we were able to better understand how our technology could best support and integrate into their creative workflows — and we’ve woven their insights into Flow. Here are some filmmakers we partnered with and the short films they developed using Flow along with other tools and techniques.\n\n**Dave Clark**\n\nDave is an award-winning filmmaker focused on embracing new technology as part of his filmmaking. He used AI to develop two of his most recent short films, “Battalion” and “NinjaPunk.” His newest short film “Freelancers” uses Google’s AI and other tools to tell the story of two estranged adopted brothers on similar quests.\n\n**Henry Daubrez**\n\nHenry has been using tech tools in his art for the last 18 years. Earlier this year he unveiled “Kitsune” using Veo 2 — a moving short film about “love between two souls separated by everything except their shared feelings of loneliness.” Now, Henry is bringing the story of his own creative journey to life in “Electric Pink.”\n\n**Junie Lau**\n\nJunie Lau is a film director and multidisciplinary creative deeply passionate about innovation, viewing AI as a vital collaborator in expanding the boundaries of creative expression. Her work delves into artistic narratives within the hyper-modern era, including themes of virtual identity, digital humanities and digital ontology. Currently, Junie is working on a film titled “Dear Stranger,” which explores the boundless and infinite nature of universal love between a grandmother and grandchild across countless parallel worlds.\n\nAI is ushering in a new chapter of creativity and filmmaking, and while it’s still early, we see so much potential for tools like Flow to unlock new voices and creations.\n\nFor more insights on Flow and how AI helps storytellers take more risks in the creative storytelling process, watch Dave Clark, Junie Lau, and Henry Daubrez in \"Behind the Lens: AI, Creativity, and the Future of Filmmaking Tools.\"",
                "domain": "blog.google"
              },
              {
                "position": 6,
                "title": "Introducing Veo 3.1 and new creative capabilities in the Gemini API",
                "url": "https://developers.googleblog.com/en/introducing-veo-3-1-and-new-creative-capabilities-in-the-gemini-api/",
                "snippet": "**Today, we are releasing Veo 3.1 and Veo 3.1 Fast in paid preview in the** **Gemini API** **.** This updated model offers several improvements, as well as improved outputs when generating video from images. These new models are available via the Gemini API in Google AI Studio and Vertex AI. Veo 3.1 is also available in the Gemini app and Flow.\n\nVeo 3.1 and Veo 3.1 Fast empower developers to create more engaging content through significant upgrades. The models now generate\n\n**richer native audio**, from natural conversations to synchronized sound effects, and offer **greater narrative control** with an improved understanding of cinematic styles. **Enhanced image-to-video** capabilities ensure better prompt adherence while delivering superior audio and visual quality and maintaining character consistency across multiple scenes.\n\nIn addition to the model update, we are introducing new model capabilities, including using reference images to guide video generation, extend existing Veo videos, and generate transitions between a first and last frame.\n\nYou can now guide the generation process by providing up to 3 reference images of a character, object, or scene. This is helpful for maintaining character consistency across multiple shots or applying a specific style to your video.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nconfig=types.GenerateVideosConfig(\n\nreference_images=[reference_image1, reference_image2],\n\n),\n\n\n\n```... Your story is no longer limited by the original generation. With Scene extension, you can create longer videos, even lasting for a minute or more, by generating new clips that connect to your previous video. Each new video is generated based on the final second of the previous clip. This maintains visual continuity, making the feature ideal for extending shots with background audio.\n\n```\n\nfrom google import genai\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nvideo=video_to_extend\n\n\n\n```\n\nCreate smooth, natural scenes that bridge two different images. By providing a starting and an ending image, you can direct Veo 3.1 to generate the transition between them, complete with accompanying audio.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nimage=first_frame,\n\nconfig=types.GenerateVideosConfig(\n\nlast_frame=last_frame,\n\n),\n\n\n\n```\n\n**What people are building with Veo 3.1**\n\nPromise Studios, a GenAI movie studio, uses Veo 3.1 within its MUSE Platform to enhance generative storyboarding and previsualization for director-driven storytelling at production quality.... Latitude is experimenting with Veo 3.1 in its generative narrative engine to instantly bring user-created stories to life.\n\nVeo 3.1 and these new capabilities are now available in paid preview via the Gemini API.\n\nVeo 3.1 is the same price as Veo 3. We are incredibly excited to see what you will build with these new capabilities!",
                "domain": "developers.googleblog.com"
              },
              {
                "position": 7,
                "title": "Veo 3 Fast available for everyone on Vertex AI | Google Cloud Blog",
                "url": "https://cloud.google.com/blog/products/ai-machine-learning/veo-3-fast-available-for-everyone-on-vertex-ai",
                "snippet": "# Veo 3 and Veo 3 Fast are now generally available on Vertex AI\n\n##### Jason Gelman\n\nDirector, Product Management, Vertex AI... ##### Try Gemini 2.5\n\nOur most intelligent model is now available on Vertex AITry now\n\nVeo 3 has seen massive global adoption with over 70 million videos created since May, and we've seen tremendous momentum with our enterprise customers as well. Since its preview launch on Vertex AI in June, enterprise customers have already generated over 6 million videos, showcasing the incredible demand for professional-grade, scalable AI video creation.\n\nToday, we’re building on this momentum with some exciting updates to Veo on Vertex AI.\n\n\n\n**Veo 3,**our most advanced video generation model, is now generally available to everyone on Vertex AI.\n\n\n\n**Veo 3 Fast**, a model designed for speed and rapid iteration, is now generally available for everyone on Vertex AI. It's a faster way to turn text to video, from narrated product demos to short films.\n\n\n\nComing to public preview on Vertex AI in August, Veo 3 and Veo 3 Fast will also offer\n\n**image-to-video capabilities**to make it possible for you to bring static visuals and images to life. All you have to do is provide the source image along with a text prompt that describes what kind of video you want to create.\n\n**How businesses are building with Veo 3 on Vertex AI**... Google Cloud customers around the world are using Veo 3 and Veo 3 Fast on Vertex AI to create professional-quality video content with unparalleled efficiency and creative freedom. Let’s look at some examples.\n\n**Canva**\n\n“Enabling anyone to bring their ideas to life – especially their most creative ones – has been core to Canva's mission ever since we set out to empower the world to design. By democratising access to a powerful technology like Google’s Veo 3 inside Canva AI, your big ideas can now be brought to life in the highest quality video and sound, all from within your existing Canva subscription. In true Canva fashion, we’ve built this with an intuitive interface and simple editing tools in place, all backed by Canva Shield.” –\n\n**Cameron Adams, co-founder and Chief Product Officer, Canva **\n\nBut the momentum extends beyond design. The team at\n\n**BarkleyOKRP**, a leading ad agency, is using Veo 3 to speed up video production timelines.\n\n“The rapid advancements from Veo 2 to Veo 3 within such a short time frame on this project have been nothing short of remarkable. Our team undertook the task of re-creating numerous music videos initially produced with Veo 2 once Veo 3 was released, primarily due to the significantly improved synchronization between voice and mouth movements. The continuous daily progress we are witnessing is truly extraordinary.” –... **Julie Ray Barr, Senior Vice President Client Experience, BarkleyOKRP**\n\nAt global investing platform\n\n**eToro**, the team is making marketing iterations a breeze with Veo 3.\n\n“At eToro, innovation is in our DNA. As a global investing platform serving clients in 75 countries, local storytelling isn’t optional - it’s essential. With Veo 3, we produced 15 fully AI‑generated versions of our ad, each in the native language of its market, all while capturing real emotion at scale. Ironically, AI didn’t reduce humanity - it amplified it. Veo 3 lets us tell more stories, in more tongues, with more impact.” –\n\n**Shay Chikotay, Head of Creative & Content, eToro**\n\n**Razorfish**, an interactive agency and part of the Publicis Groupe, is using Veo to bring creative to life.\n\n\"For The Morelandos, our campaign with Visit Orlando and Google, we used the full Vertex AI stack—Gemini to mine real reviews, Imagen to bring the characters to life, and Veo to give them motion. Veo let us go from story to near-cinematic video in a fraction of the usual time—which meant more room to explore, iterate, and push the idea further.\" –... **Anthony Yell, Chief Creative Officer, Razorfish**\n\n**Synthesia** **, **a leading synthetic media generation company, is using Veo to contextually adapt visuals to its hyper-realistic AI avatars and voices.\n\n“Veo 3 represents a leap forward in generative AI, and its integration into Synthesia’s platform will redefine how businesses create video content. By combining our hyper-realistic AI avatars and voices with Veo-powered fully contextual visuals that adapt to each unique story, we’re giving enterprise teams the creative power to communicate with unrivalled clarity and impact.” –\n\n**Bill Leaver, Product Manager, Synthesia **\n\n**How enterprises can use Veo 3 Fast for speed and creativity **\n\n**Veo 3 Fast** is a great fit for work that requires rapid iteration and speed. It has an ideal balance between processing time and high-quality visual output, making it especially helpful for:\n\n\n\nQuickly generating and testing variations of ad concepts to respond to market trends.\n\n\n\nEfficiently creating video demonstrations for entire product catalogs from still images.\n\n\n\nDeveloping engaging animated explainers and training modules in less time.\n\n**Veo 3 and Veo 3 Fast on Vertex AI mean even more capabilities for enterprise storytelling**\n\nVeo 3 and Veo 3 Fast are designed to give creators the control and quality needed to move beyond short clips and produce complete, compelling narratives. Here are some of the core features now generally available on Vertex AI.... **Create scenes with native audio:**Veo 3 generates video and audio in a single step. This means you can create scenes with characters that speak with accurate lip-syncing, and sound effects that fit the mood. **Deliver professional quality at enterprise scale:**Veo 3 produces high-definition (1080p) video, suitable for professional marketing campaigns, product demonstrations, and internal communications. You can create content that meets brand standards, saving time and money. **Simplify content localization for global audiences:**Veo 3’s native dialogue generation helps businesses connect with an international audience by producing a video once and localizing the dialogue for dozens of languages. **Image-to-video (coming to public preview on Vertex AI in August):**Veo 3 and Veo 3 Fast can also take a single image, which can be a photo you uploaded or an AI-generated image, and animate it, creating an 8-second video clip. This feature is particularly powerful for content creators, marketers, and businesses looking to animate existing visual assets, create engaging social media content, or generate compelling product demonstrations from high-quality images.\n\n### Enterprise-grade safety and security\n\nVeo 3 and Veo 3 Fast on Vertex AI are built for scalable and responsible enterprise use. We embed digital watermarks into every frame with SynthID, helping combat misinformation and misattribution. Veo 3 and Veo 3 Fast are also covered by our indemnity for generative AI services.... ### Get started with Veo 3 and Veo 3 Fast today\n\nTo get started, go here to learn more about Veo 3 and Veo 3 Fast on Vertex AI, and try it on Vertex AI Media Studio.",
                "domain": "cloud.google.com"
              },
              {
                "position": 8,
                "title": "First look: Google Veo 3 (May/2025)",
                "url": "https://www.youtube.com/watch?v=OrVUHMK58GE",
                "snippet": "## Dr Alan D. Thompson\n##### May 25, 2025 (0:05:24)\nSource: \nhttps://www.reddit.com/r/PowerfulJRE/comments/1kt942i/these_are_all_ai_videos_generated_with_google_veo/\n\nThe Memo: https://lifearchitect.ai/memo/... {ts:0} That's one move with AI that makes haters go crazy every time. Oh, y'all\n{ts:3} gotta give them that. This is wild. It's over. We are cooked on that thread. You get me?\n{ts:9} Oh my god. Yes. Victory royale with a pickaxe. So, this is an AI video about nothing. It's about\n{ts:19} nothing. Who would watch a video about nothing? So, I went to the zoo the other day and\n{ts:27} all they had was one dog. It was a [Music] shiu. We're going to light up the\n{ts:43} [Music] [Applause] [Music]\n{ts:51} sky. I'm not sure I can go on. [Music] The sum of the squares of the two\n{ts:61} shorter sides is equal to the square of the longest side. We can talk. No more silence. Yes, we can talk. We can talk.\n{ts:68} We can talk. We can talk with accents. Oh, I think that would be marvelous. Yes, it is very fun. But yes, it is very\n{ts:75} good. Very fun. I can talk. Yes, we can talk. Yes, we can talk. We can talk. We can talk. Yes, we... {ts:84} can talk. No. Yes, we can talk as cartoons. This is amazing. Imagine all the narrative\n{ts:92} possibilities. We can sing talk. Let's talk. So, what are we going to talk about now? What are we going to\n{ts:104} talk about now that we can talk? I have no idea. What do you want to talk about now that I can talk?\n{ts:112} Yeah. I I don't know if I have something to say.\n{ts:118} We can talk about how magical this is. Is [Music]\n{ts:124} I want to say something important, something deep. The future is still in our hands.\n{ts:132} That's cliche dialogue. Let's not talk. How much wood would a woodchuck chuck if\n{ts:146} a woodchuck could chuck wood? How much wood would a woodchuck chuck if a woodchuck could chuck wood? How much\n{ts:155} wood would a woodchuck chuck if a woodchuck could chuck wood? How much wood would a woodchuck chuck if a\n{ts:165} woodchuck could chuck wood? How much wood would a woodchuck chuck if a woodchuck could chuck wood?... {ts:175} How much wood would a woodchuck chuck if a woodchuck could chuck wood? How much wood would a woodchuck\n{ts:187} chuck? [Applause] Welcome to a non-existent car show.\n{ts:193} Let's see some opinions. I mean, man, the acceleration is crazy. You look far, step on the pedal, and you\n{ts:202} are there. I feel safe with him in an SUV, and it seems to be like the right type of car for him. I think the range\n{ts:210} is only um only going to get better. Sorry. We don't want to drive gas cars anymore.\n{ts:217} Yeah. No more gas cars. You can see uh I'm kind of a kind of a misfit here, but uh don't tell anyone\n{ts:226} I've just bought an electric car. I think it's really great for families and for little babies with all the safety\n{ts:231} features that these SUVs have. But what you're really seeing is that technology is going to be very, very important in\n{ts:239} terms of how we go forward. It was um great to come to the conference because my husband loves cars. I think I... {ts:249} have to buy an EV now. I love my muscle cars, but I try to stay as healthy as I can so\n{ts:257} I can make it to the next car show. When we get in there, I want no [ __ ] You stay on my six at all\n{ts:269} times. [Music] Stay sharp. These [ __ ] are nasty and\n{ts:282} dangerous. Stay alert. [ __ ] What the hell happened here? Where are the\n{ts:291} [Applause] bodies? Heat. Heat. Fire heat.",
                "domain": "www.youtube.com"
              },
              {
                "position": 9,
                "title": "Google releases Veo 3.1, adds it to Flow video editor",
                "url": "https://techcrunch.com/2025/10/15/google-releases-veo-3-1-adds-it-to-flow-video-editor/",
                "snippet": "In Brief\nGoogle launched its new video model Veo 3.1 with improved audio output, granular editing controls, and better output for image to video. It said that Veo 3.1 builds on May's Veo 3 release and generates more realistic clips and adheres to prompts better.\nThe model allows users to add an object to the video and have it blend into the clip's style, Google said. Soon, users will be able to remove an existing object from the video in Flow, too.\nVeo 3 already has edit features such as adding reference images to drive a character, providing the first and last frame to generate a clip using AI, and the ability to extend an existing video based on the last few frames. With Veo 3.1, Google is adding audio to all these features to make the clips more lively.\nThe company is rolling out the model to its video editor Flow, the Gemini App, along with Vertex and Gemini APIs. It said that since Flow's launch in May, users have created more than 275 million videos on the app.",
                "domain": "techcrunch.com"
              },
              {
                "position": 10,
                "title": "Gemini AI video generator powered by Veo 3.1",
                "url": "https://gemini.google/overview/video-generation/",
                "snippet": "# Break the\n\n**silence** with Veo 3.1\n\nCreate high-quality, 8-second videos with Veo 3.1, our latest AI video generator. Simply describe what you have in mind or upload a photo and watch your ideas come to life with native audio generation. Try it with a Google AI Pro plan or get the highest access with the Ultra plan.\n\n**Veo 3.1** speaks for itself\n\n## Dream it. Describe it.\n\n**Done.**\n\n## For Exploring\n\nPlay with diverse styles, bring animated characters to life, and combine objects in ways you never thought possible. See what you can create using text to video with AI.\n\n## For Sharing\n\nCreate funny memes, turn inside jokes into videos, re-imagine special moments, and add a personal touch to make someone smile.\n\n## For Brainstorming\n\nBreak through creative blocks and visualize your ideas in a flash. From product concepts and designs to rapid prototyping and storytelling, Gemini can help.\n\n## Learn more about our\n\n**Veo Models**\n\nCreate videos with sound using our video generation model that maintains high-quality while optimizing for speed.\n\nCreate high-quality, 8-second videos with sound using our state-of-the-art video generation model.... ## Frequently asked questions\n\nYes, you can create and share videos in your mobile Gemini app. To create videos, tap the \"video\" button in your prompt bar. If you don't see it, tap the button with three dots to view more options.\n\nTry Veo 3.1 Fast with a Google AI Pro plan or get the highest access to Veo 3.1 in Google AI Ultra. Country availability here.\n\nFor now, the ability to generate a video from a photo is not available in the European Economic Area, Switzerland, or the United Kingdom.\n\nWe’ve taken several important safety steps to make AI video generation a safe experience. This includes extensive red teaming and evaluation aimed at preventing the generation of content that violates our policies. Additionally, all videos generated with Veo in the Gemini app are marked with a visible watermark and SynthID, a digital watermark embedded in each frame, which indicates the videos are AI-generated.\n\nGemini's outputs are primarily determined by user prompts and like any generative AI tool, there may be instances where it generates content that some individuals find objectionable. We’ll continue to listen to your feedback through the thumbs up/down buttons and make ongoing improvements. For more details, you can read about our approach on our website.\n\nResults for illustrative purposes and may vary. Internet and subscription for certain features required. Available to users 18+. Create responsibly.",
                "domain": "gemini.google"
              }
            ],
            "success": true,
            "error": null
          },
          {
            "query_id": "q1",
            "query": "Google official announcement veo3.1 release 2025",
            "claim_id": "claim_1",
            "query_type": "source_verification",
            "priority": "high",
            "results": [
              {
                "position": 1,
                "title": "Introducing Veo 3.1 and advanced capabilities in Flow - Google Blog",
                "url": "https://blog.google/technology/ai/veo-updates-flow/",
                "snippet": "# Introducing Veo 3.1 and advanced capabilities in Flow\n\nFive months ago, we introduced Flow, our AI filmmaking tool powered by Veo, and have been inspired by the creativity it has sparked with over 275 million videos generated in Flow\n\n1\n\n. We're always listening to your feedback, and we've heard that you want more artistic control within Flow, with increased support for audio across all features.\n\nToday, we’re introducing new and enhanced creative capabilities to edit your clips, giving you more granular control over your final scene. For the first time, we’re also bringing audio to existing capabilities like “Ingredients to Video,” “Frames to Video” and “Extend.”\n\nWe’re also introducing Veo 3.1, which brings richer audio, more narrative control, and enhanced realism that captures true-to-life textures. Veo 3.1 is state-of-the-art and builds on Veo 3, with stronger prompt adherence and improved audiovisual quality when turning images into videos.\n\n## Refine your narrative with audio and more control\n\nWith Veo 3.1, we’re bringing audio to existing capabilities to help you craft the perfect scene. These features are experimental and actively improving, and we’re excited to see what you create as we iterate based on your feedback.\n\nNow, with rich, generated audio, you can:\n\n**Craft the look of your scene.**With \"Ingredients to Video,\" you can use multiple reference images to control the characters, objects and style. Flow uses your ingredients to create a final scene that looks just as you envisioned. **Control the shot from start to finish.**Provide a starting and ending image with “Frames to Video,” and Flow will generate a seamless video that bridges the two, perfect for artful and epic transitions. **Create longer, seamless shots.**With \"Extend,\" you can create longer videos, even lasting for a minute or more, that connect to and continue the action from your original clip. Each video is generated based on the final second of your previous clip, making it most useful for creating a longer establishing shot.... ## Edit your ingredients and videos with more precision\n\nGreat ideas can strike at any point in the creative process. For moments when the first take isn't the final one, we're introducing new editing capabilities directly within Flow to help you reimagine and perfect your scenes.\n\n**Add new elements to any scene.**With “Insert,” introduce anything you can imagine, from realistic details to fantastical creatures. Flow now handles complex details like shadows and scene lighting, making the addition look natural. **Remove unwanted objects or characters seamlessly.**Soon, you’ll be able to take anything out of a scene, and Flow will reconstruct the background and surroundings, making it look as though the object was never there.\n\n## Start creating in Flow today\n\nWith more precise editing capabilities, audio across all existing features and higher-quality outputs powered by Veo 3.1, we're opening up new possibilities for richer, more powerful video storytelling right inside Flow.\n\nThe Veo 3.1 model is also available via the Gemini API for developers, Vertex AI for enterprise customers, and the Gemini app. New capabilities are available in both Gemini API\n\n2\n\nand Vertex AI\n\n3",
                "domain": "blog.google"
              },
              {
                "position": 2,
                "title": "Google announces Veo 3.1 and updates Flow with more controls, tools",
                "url": "https://9to5google.com/2025/10/15/veo-3-1/",
                "snippet": "Google today announced Veo 3.1 as its latest video generation model, with Flow getting a number of updates to take advantage of the latest capabilities.\n\nCompared to Veo 3, which was announced at I/O 2025 in May, this new version offers richer audio and “enhanced realism that captures true-to-life textures.” Veo 3.1 has a deeper understanding of storytelling, cinematic styles, and character interactions to give you more narrative control. The image-to-video capability benefits from improved audio-visual quality and better follows your prompt.\n\nVeo 3.1 and Veo 3.1 Fast are available in the Gemini app, as well as the Gemini API and Vertex AI, to power text-to-video and image-to-video for horizontal (16×9) and vertical (9×16) outputs.\n\nMeanwhile, Google is updating the Flow filmmaking tool to take advantage of Veo 3.1. Audio generation is coming to:\n\n**Ingredients to Video**: This can include images of multiple characters, objects, and styles **Scene extension**: You can Extend an existing clip, with Google basing it on the last second **Frames to Video**: This lets you specify the starting and ending images, with Flow bridging the two\n\nFlow is getting new editing capabilities. You can insert elements like objects, characters, and details, with Google handling shadows, scene lighting, and other complex details to make everything look natural.\n\nComing soon is the ability to remove objects and characters from a scene, with Flow working to reconstruct the background and surroundings to make the edit seamless.\n\n\n\n## Comments",
                "domain": "9to5google.com"
              },
              {
                "position": 3,
                "title": "Google rolls out its new Veo 3 video-generation model ...",
                "url": "https://techcrunch.com/2025/07/03/google-rolls-out-its-new-veo-3-video-generation-model-globally/",
                "snippet": "Google on Thursday said it has begun rolling out its Veo 3 video generation model to Gemini users in more than 159 countries.\n\nVideo generation via the new model is available only to paying subscribers of Google’s AI Pro plan and is capped at three videos per day.\n\nVeo 3, which Google showed off in May, lets users generate videos up to eight seconds long using text prompts.\n\nGoogle’s Josh Woodward has said that the company is working on adding image-to-video generation capabilities to Gemini.",
                "domain": "techcrunch.com"
              },
              {
                "position": 4,
                "title": "Google releases Veo 3.1, adds it to Flow video editor",
                "url": "https://techcrunch.com/2025/10/15/google-releases-veo-3-1-adds-it-to-flow-video-editor/",
                "snippet": "In Brief\nGoogle launched its new video model Veo 3.1 with improved audio output, granular editing controls, and better output for image to video. It said that Veo 3.1 builds on May's Veo 3 release and generates more realistic clips and adheres to prompts better.\nThe model allows users to add an object to the video and have it blend into the clip's style, Google said. Soon, users will be able to remove an existing object from the video in Flow, too.\nVeo 3 already has edit features such as adding reference images to drive a character, providing the first and last frame to generate a clip using AI, and the ability to extend an existing video based on the last few frames. With Veo 3.1, Google is adding audio to all these features to make the clips more lively.\nThe company is rolling out the model to its video editor Flow, the Gemini App, along with Vertex and Gemini APIs. It said that since Flow's launch in May, users have created more than 275 million videos on the app.",
                "domain": "techcrunch.com"
              },
              {
                "position": 5,
                "title": "Build with Veo 3, now available in the Gemini API",
                "url": "https://developers.googleblog.com/en/veo-3-now-available-gemini-api/",
                "snippet": "First unveiled at Google I/O 2025, people around the world have already generated tens of millions of high-quality videos with Veo 3 (along with some new fun and interesting video trends). It is our first video model to incorporate high-fidelity video outputs and native audio, first with text-to-video and soon with image-to-video.\n\nDevelopers are already experimenting with Veo 3, discovering how the model can help them brainstorm content, rapidly iterate, and be more efficient.\n\nVeo 3 is designed to handle a range of video generation tasks, from cinematic narratives to dynamic character animations. With Veo 3, you can create more immersive experiences by not only generating stunning visuals, but also audio like dialogue and sound effects.\n\nLet’s take a look at some examples.\n\nExplore these examples and more with Veo 3 in Google AI Studio, available as an SDK template and interactive Starter App to remix, copy and extend. The Starter App and its sample code offer a convenient way for Paid Tier users to rapidly prototype with Veo 3 and more on the Gemini API, directly from Google AI Studio.\n\nClick the Key button in the top right of the AI Studio Build interface to select a Google Cloud Project with billing enabled to use the Paid Tier in AI Studio apps. See the FAQs for more.... Veo 3 will be priced at $0.75 per second for video and audio output. Additionally, Veo 3 Fast will be available soon, offering a faster and more cost-effective option for video creation.\n\nHere’s a basic Python example to create a video:\n\n```\n\nimport time\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.0-generate-preview\",\n\nprompt=\"a close-up shot of a golden retriever playing in a field of sunflowers\",\n\nconfig=types.GenerateVideosConfig(\n\nnegative_prompt=\"barking, woofing\",\n\n),\n\n\n\n# Waiting for the video(s) to be generated\n\nwhile not operation.done:\n\ntime.sleep(20)\n\noperation = client.operations.get(operation)\n\ngenerated_video = operation.result.generated_videos[0]\n\nclient.files.download(file=generated_video.video)\n\ngenerated_video.video.save(\"veo3_video.mp4\")\n\n```\n\nAll videos generated by Veo 3 models will continue to include a digital SynthID watermark. To get started, check out the documentation, cookbook, and a Veo 3 starter app in Google AI Studio:\n\nIn addition to being available via the Gemini API in Google AI Studio, Veo 3 is also available to Google AI subscribers in the Gemini app and Flow, and to enterprise customers via Vertex AI.... 1: Fluffy Characters Stop Motion: Inside a brightly colored, cozy kitchen made of felt and yarn. Professor Nibbles, a plump, fluffy hamster with oversized glasses, nervously stirs a bubbling pot on a miniature stove, muttering, \"Just a little more... 'essence of savory,' as the recipe calls for.\" The camera is a mid-shot, capturing his frantic stirring. Suddenly, the pot emits a loud \"POP!\" followed by a comical \"whoosh\" sound, and a geyser of iridescent green slime erupts, covering the entire kitchen. Professor Nibbles shrieks, \"Oh, dear! Not again!\" and scurries away, leaving a trail of tiny, panicked squeaks.\n\n2: The sequence begins with an extreme close-up of a single gear, slowly turning and reflecting harsh sunlight. The camera gradually pulls back in a continuous movement, revealing this is but one component of a colossal, mechanical heart half-buried in a desolate, rust-colored desert. A sweeping aerial shot establishes its enormous scale and isolation in the barren landscape. The camera descends to capture pipes hissing steam and the rhythmic thumping that echoes across the empty plains. A subtle shake effect synchronizes with each massive heartbeat. A lateral tracking shot discovers tiny, robed figures scurrying across the metallic surface. The camera follows one such figure in a detailed tracking shot as they perform meticulous maintenance, polishing brass valves and tightening immense bolts. A complex movement circles the entire structure, capturing different maintenance teams working in precarious positions across its rusted exterior. The final shot begins tight on the meticulous work of one tiny figure before executing a dramatic pull-out that reveals the true scale of the heart and the minuscule size of its caretakers, tending to the vital organ of an unseen, sleeping giant that extends beyond the frame.",
                "domain": "developers.googleblog.com"
              },
              {
                "position": 6,
                "title": "Introducing Veo 3.1 and new creative capabilities in the Gemini API",
                "url": "https://developers.googleblog.com/en/introducing-veo-3-1-and-new-creative-capabilities-in-the-gemini-api/",
                "snippet": "**Today, we are releasing Veo 3.1 and Veo 3.1 Fast in paid preview in the** **Gemini API** **.** This updated model offers several improvements, as well as improved outputs when generating video from images. These new models are available via the Gemini API in Google AI Studio and Vertex AI. Veo 3.1 is also available in the Gemini app and Flow.\n\nVeo 3.1 and Veo 3.1 Fast empower developers to create more engaging content through significant upgrades. The models now generate\n\n**richer native audio**, from natural conversations to synchronized sound effects, and offer **greater narrative control** with an improved understanding of cinematic styles. **Enhanced image-to-video** capabilities ensure better prompt adherence while delivering superior audio and visual quality and maintaining character consistency across multiple scenes.\n\nIn addition to the model update, we are introducing new model capabilities, including using reference images to guide video generation, extend existing Veo videos, and generate transitions between a first and last frame.\n\nYou can now guide the generation process by providing up to 3 reference images of a character, object, or scene. This is helpful for maintaining character consistency across multiple shots or applying a specific style to your video.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nconfig=types.GenerateVideosConfig(\n\nreference_images=[reference_image1, reference_image2],\n\n),\n\n\n\n```... Your story is no longer limited by the original generation. With Scene extension, you can create longer videos, even lasting for a minute or more, by generating new clips that connect to your previous video. Each new video is generated based on the final second of the previous clip. This maintains visual continuity, making the feature ideal for extending shots with background audio.\n\n```\n\nfrom google import genai\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nvideo=video_to_extend\n\n\n\n```\n\nCreate smooth, natural scenes that bridge two different images. By providing a starting and an ending image, you can direct Veo 3.1 to generate the transition between them, complete with accompanying audio.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nimage=first_frame,\n\nconfig=types.GenerateVideosConfig(\n\nlast_frame=last_frame,\n\n),\n\n\n\n```\n\n**What people are building with Veo 3.1**\n\nPromise Studios, a GenAI movie studio, uses Veo 3.1 within its MUSE Platform to enhance generative storyboarding and previsualization for director-driven storytelling at production quality.... Latitude is experimenting with Veo 3.1 in its generative narrative engine to instantly bring user-created stories to life.\n\nVeo 3.1 and these new capabilities are now available in paid preview via the Gemini API.\n\nVeo 3.1 is the same price as Veo 3. We are incredibly excited to see what you will build with these new capabilities!",
                "domain": "developers.googleblog.com"
              },
              {
                "position": 7,
                "title": "Google expands Veo 3 to Gemini in more countries and on ...",
                "url": "https://blog.google/products/gemini/veo-3-expansion-mobile/",
                "snippet": "We’re thrilled by the response to Veo 3. The Google AI Ultra plan grants the highest access to Veo 3 and later today we’re launching it in the UK. The Ultra plan is now available in 73 countries, and we’re working hard to bring it to even more.\n\nGoogle AI Pro subscribers in those countries get limited access to Veo 3 in Flow, and a 10-pack of trial video generations to test it out in the Gemini app. Starting today, Pro subscribers will also have access to Veo 3 in the Gemini mobile app.\n\nIt's important that people can access provenance tools for content online. The SynthID watermark is embedded in all content generated by Google's generative AI models. Our SynthID Detector rolled out to early testers last week, and we plan to expand access soon. As an additional step to help people identify AI-generated content, today we're adding a visible watermark to all videos, except for videos generated by Ultra members in Flow, our tool for AI filmmakers.",
                "domain": "blog.google"
              },
              {
                "position": 8,
                "title": "What Is Google Veo 3.1? A Beginner's Guide to AI Video ...",
                "url": "https://skywork.ai/blog/google-veo-3-1-beginners-guide-ai-video-model/",
                "snippet": "If you’ve seen short, cinematic clips made “from a prompt” and wondered how it works, you’re in the right place. This guide will walk you through Google’s latest video generation model, Veo 3.1, in plain English—what it is, why it matters, where you can try it, and a simple first project you can finish in about 10 minutes. Don’t worry if you’ve never touched AI video before. We’ll move step by step and flag common gotchas so you can avoid them.\n\n## What is Google Veo 3.1?\n\nGoogle Veo 3.1 is a text-to-video AI model that turns short written prompts and references into short video clips, with native audio support and more control over storytelling and style. In October 2025, Google announced Veo 3.1 (and a faster variant) in paid preview for developers and creators. According to the Google Developers Blog announcement (Oct 15, 2025), Veo 3.1 is available via the Gemini API in Google AI Studio and Vertex AI, and it adds richer native audio, improved adherence to cinematic styles, and new creative controls.\n\nWhy it matters for beginners: Veo 3.1’s improvements make it easier to describe the mood and motion you want—like “a slow dolly-in on a vintage typewriter with soft rain ambience”—and actually get something close, without wrestling with advanced settings.... ## What’s new compared to earlier Veo versions?\n\nGoogle calls out a few key upgrades in Veo 3.1:\n\n- Richer native audio generation and better narrative control, with improved understanding of cinematic styles (publisher: Google; see the Developers Blog announcement (2025)).\n\n- New creative tools in supporting apps: you can guide generation with multiple reference images, extend existing Veo clips, or bridge between a first and last frame to create transitions. These capabilities are highlighted in the Google Blog on Veo 3.1 and Flow (Oct 2025).\n\n- Native audio emphasis and model positioning are also described on the DeepMind Veo model page, which introduces Veo’s “video, meet audio” concept.\n\nIndependent coverage, like TechCrunch’s report on the Veo 3.1 release (Oct 2025), provides context on rollout and app integration. For hard limits (length, resolution), always defer to Google’s official docs, as those details can change.\n\n## Where can you use Veo 3.1 today?\n\nAs of October 2025:\n\n- Flow (Google’s AI video editor): Google’s announcement notes Veo 3.1 and advanced creative controls in Flow, including “Ingredients to Video” (use multiple images as style/character references), “Frames to Video” (bridge between start/end frames), and “Extend” (lengthen a clip). Details are outlined in the Google Blog on Veo 3.1 and Flow (2025).\n\n- Gemini API (Google AI Studio and Vertex AI): Veo 3.1 and Veo 3.1 Fast are in paid preview via the Gemini API, per the Google Developers Blog announcement (2025).\n\nImportant: Access tiers, regions, and pricing can vary and may change. If you’re not seeing options in your account, check Google’s documentation or support for your account type and region.... ## What can Veo 3.1 generate right now? (Practical limits to know)\n\n- In Vertex AI’s preview for Veo 3.1 base generations, the documented lengths are currently short (choose 4, 6, or 8 seconds). This is specified on the Vertex AI Veo 3.1 preview page.\n\n- In Gemini API extension workflows, you can extend Veo-generated clips. The API docs describe a maximum of up to 141 seconds for input Veo videos in those extension scenarios, with 720p listed in that context. See the Gemini API video documentation (Google).\n\nThese numbers help set expectations: start with short shots, then extend or chain shots together. Avoid assuming 1080p or minute-long base generations unless Google’s official docs explicitly state it for your environment.... ## A 10-minute quickstart: your first Veo 3.1 clip\n\nWe’ll make a simple “coffee shop mood” shot you can adapt for social posts.\n\n- Choose your aspect ratio\n\n- 16:9 (landscape) for YouTube and desktop-first screens\n\n- 9:16 (vertical) for TikTok, Reels, and Shorts Pick one at the start to avoid accidental cropping later.\n\n- Write a clear, concrete prompt Use this structure to get reliable results: subject + action + setting + style + camera + audio.\n\n- Example prompt: “A barista gently places a ceramic cup on a wooden counter; shallow depth of field; warm morning light streaming through windows; slow dolly-in; subtle steam rising; soft cafe ambience, no dialogue, light jazz in the background.”\n\nIf you want a deeper primer on prompt structure and clarity, see these prompt engineering best practices for beginners.\n\n- Generate in your chosen interface\n\n- In Flow: Start a new project, choose Veo 3.1, paste your prompt, set aspect ratio, and generate.\n\n- In Gemini API/Vertex AI: Use the model/version and parameters documented for Veo 3.1 in your environment. Begin with short durations and default settings.\n\n- Review the output like a director\n\n- Does the subject and action match? Is the lighting/mood close?\n\n- Is the camera movement smooth? Is audio what you expected?\n\n- Make one or two changes per iteration—small edits beat big rewrites.\n\n- Refine with controls\n\n- Reference images: If you want consistency (e.g., the same mug or barista style), use up to three reference images to guide the look. This capability is described in Google’s announcements (2025).\n\n- Frames to Video: Provide a starting and ending frame to shape the motion between them.\n\n- Extend: Lengthen your favorite moment to create a longer beat.\n\n- Export Choose the format/aspect ratio you started with. If you need both vertical and horizontal versions, plan to reframe or regenerate with the other aspect ratio rather than cropping aggressively.... ## A practical planning example using Skywork AI (optional, 5 minutes)\n\nSkywork AI can help you prepare the words before you ever hit “Generate.” Disclosure: Skywork AI is our product.\n\nHere’s a neutral, step-by-step way to use it purely for planning:\n\n- Open Skywork and create a new document. Ask for a short video outline: “30-second coffee shop mood piece: 3 shots, warm tone, slow camera.”\n\n- Have it draft a compact shot list with camera moves and audio notes, like:\n\n- Close-up of cup; slow dolly-in; soft steam; light jazz; no dialogue\n\n- Medium barista hands; gentle rack focus; cafe ambience; espresso hiss\n\n- Wide room tone; sunbeams; slow tilt up; footsteps and cups\n\n- Ask for a final prompt assembled from the shot you want to generate first. Copy that prompt into Veo 3.1.\n\nIf you prefer to outline prompts and story beats yourself, this short guide to Skywork’s General Mode for planning and outlining walks through a simple, distraction-free workflow.... ## FAQs for first-time users\n\n\n\nIs Veo 3.1 free? No. Google describes Veo 3.1 and Veo 3.1 Fast as being in paid preview via the Gemini API as of October 2025. See the Google Developers Blog announcement (2025). Pricing varies by product and usage.\n\n\n\nWhere can I try Veo 3.1? Flow (for a visual editor), and the Gemini API via Google AI Studio and Vertex AI (for developers). Google outlines these options in the Veo 3.1 and Flow post (2025) and the Developers Blog announcement (2025).\n\n\n\nHow long can my video be? It depends on the environment and workflow. In Vertex AI’s Veo 3.1 preview, base generations are short (4, 6, or 8 seconds). In Gemini API extension workflows, input Veo videos can be extended up to 141 seconds. See the Vertex AI preview page and the Gemini API video docs.\n\n\n\nWhat resolutions are supported? The Gemini API documentation for extension workflows references 720p in that context. Official materials do not universally guarantee 1080p for base Veo 3.1 generations at this time.\n\n\n\nCan I keep a character or object consistent across shots? Yes. Google notes you can guide Veo 3.1 with up to three reference images for a character, object, or scene. This is described in the Developers Blog announcement (2025).\n\n\n\nCan I add my own audio and sync it? Veo 3.1 focuses on native audio generation controlled via your prompt. The public pages cited here do not detail a full “upload and auto-sync” workflow, so avoid relying on that unless you see it documented for your account.... ## Next steps\n\n- Try a tiny project: one 6–8 second shot with a clear prompt. If you like the result, use “Extend” or stitch a second shot with “Frames to Video.”\n\n- Build a simple storyboard first. If you want help outlining ideas, you can use Skywork AI to draft prompts and shot lists before you generate. Keep it simple: one scene, one action, one camera move.\n\n- When you’re ready to go deeper, explore Google’s official resources: the Veo 3.1 and Flow post (2025), the Developers Blog announcement (2025), the DeepMind Veo overview, the Gemini API video docs, and the Vertex AI Veo 3.1 preview.\n\nYou don’t need special “artistic talent” to start—just one clear sentence and a little patience. Generate, review, tweak, repeat. That’s how everyone learns, and you’ll be surprised how quickly it clicks.",
                "domain": "skywork.ai"
              },
              {
                "position": 9,
                "title": "Veo 3 available for everyone in public preview on Vertex AI - Google Cloud",
                "url": "https://cloud.google.com/blog/products/ai-machine-learning/veo-3-available-for-everyone-in-public-preview-on-vertex-ai",
                "snippet": "# You dream it, Veo creates it: Veo 3 is now available for everyone in public preview on Vertex AI\n\n##### Jason Gelman\n\nDirector, Product Management, Vertex AI\n\n##### Try Gemini 2.5\n\nOur most intelligent model is now available on Vertex AITry now\n\nA great story doesn't just tell you, it shows you. With Veo 3, we’ve leapt forward in combining video and audio generation to take storytelling to the next level.\n\nToday, we’re excited to share that Veo 3 is now available for all Google Cloud customers and partners in public preview on Vertex AI.\n\n**Why this matters: **Veo 3 is your partner for creating near-cinematic quality generative video, moving beyond novelty to narrative-driven creation. It not only brings stunning visual quality, but now adds sound from background sounds to dialogue. With Veo 3 on Vertex AI, you can take advantage of three powerful new capabilities:\n\n\n\n**Fluid, natural videos that synchronize video with audio and dialogue.**Veo 3 can synchronize your audio and visuals in a single pass. The model produces rich soundscapes containing everything from dialogue and ambient noise, to sound effects and background music.\n\n\n\n**Cinematic video that captures creative nuances.**Veo 3 makes it easy to capture creative nuances and detailed scene interactions in your prompt, from the shade of the sky to the precise way the sun hits water in the afternoon light, and produces high-definition video.\n\n\n\n**Realistic movement that simulates real-world physics.**To create believable scenes, Veo 3 simulates real-world physics. This results in realistic water movement, accurate shadows connected with objects and characters, and natural human motion.... ### Businesses are already using Veo to make creating easier\n\nVeo 3 is helping Google Cloud customers create external content – from social media ads to product demos – and internal materials like training videos and presentations. Hear directly from the teams:\n\n“Veo 3 has marked the difference within the gen AI industry, and we’re glad that Freepik users have been some of the first to try the model out. The quality of the video generations combined with the audio integration option is the game changer in our AI Suite. We look forward to continuing this collaboration to bring the best AI tools and features to our users” – Omar Pera, CPO, Freepik\n\n“Creativity is deeply personal, and our goal is to build a platform that adapts to every workflow. By working with Google, we’re combining the best technologies to give creators more control, efficiency, and power than ever before. Our collaboration with Google Cloud represents a strategic evolution that will not only enhance accessibility and efficiency but fundamentally transform how people create. We believe the future of generative video technology will leverage the best technologies to build the most flexible and accessible tools. This is an exciting step toward realizing that vision” – Zeev Farbman, Co-Founder & CEO, Lightricks.\n\n\"Veo 3 is the single greatest leap forward in practically useful AI for advertising since genAI first broke into the mainstream in 2023. By allowing brands to make fully fledged films from a single prompt - including brand, story, video, sound effects, voiceovers and more - Veo3 in one swoop lowers the barriers to entry to gen AI for creative people and elevates gen AI to a top tier brand building tool usable at every stage of the marketing funnel.\" – Will Hanschell, co-founder and CEO, , Pencil\n\n**Bring your vision to life with Veo 3 today**\n\nVeo 3 on Vertex AI is built for scalable enterprise use with crucial guardrails like safety filter controls and SynthID to ensure responsible deployment for any use case. To get started, go here to learn more about Veo 3 on Vertex AI and try it on Vertex AI Media Studio. Get started today!",
                "domain": "cloud.google.com"
              },
              {
                "position": 10,
                "title": "Google's Veo 3.1 is better at generating videos from images",
                "url": "https://www.engadget.com/ai/googles-veo-31-is-better-at-generating-videos-from-images-220829129.html",
                "snippet": "Google has released a new update to its Veo AI video generation model that should make it do a better job of sticking to prompts and converting images into videos. Veo 3.1 is available to try today through Google's Gemini API and is now also powering the company's Flow video editor.\nVeo 3.1 builds on the new capabilities Google introduced with launch of Veo 3 at Google I/O 2025. The new model offers better \"prompt adherence,\" according to Google, and should have an easier time creating videos based on the image \"ingredients\" you upload alongside your written prompt. Veo 3.1 also makes it possible to convert images to video and generate audio at the same time, a capability that wasn't available with Veo 3.\nIn Flow, Veo 3.1 supports at least a new feature that gives you finer control over the videos you generate. With what Google calls \"Frame to Video,\" Flow lets you upload a first and last frame, and then generates the video in-between. Adobe Firefly, which is powered by Veo 3, offers a similar feature, but Flow will be able to pull it off and create audio at the same time. Those added audio skills will also apply to the video editor's ability to extend clips and insert objects into existing footage, too.\nBased on the samples Google's shared, videos generated with Veo 3.1 still have an uncanny quality that seems to vary greatly depending on the prompt and subject. Even if it's missing some of the realism of OpenAI's Sora 2, though, the company's decision to try and make Veo more useful to people who actually work with video rather than a source of social media spam is a welcome move.",
                "domain": "www.engadget.com"
              }
            ],
            "success": true,
            "error": null
          },
          {
            "query_id": "q4",
            "query": "Google veo3.1 release news 2025",
            "claim_id": "claim_1",
            "query_type": "source_verification",
            "priority": "high",
            "results": [
              {
                "position": 1,
                "title": "Introducing Veo 3.1 and advanced capabilities in Flow - Google Blog",
                "url": "https://blog.google/technology/ai/veo-updates-flow/",
                "snippet": "# Introducing Veo 3.1 and advanced capabilities in Flow\n\nFive months ago, we introduced Flow, our AI filmmaking tool powered by Veo, and have been inspired by the creativity it has sparked with over 275 million videos generated in Flow\n\n1\n\n. We're always listening to your feedback, and we've heard that you want more artistic control within Flow, with increased support for audio across all features.\n\nToday, we’re introducing new and enhanced creative capabilities to edit your clips, giving you more granular control over your final scene. For the first time, we’re also bringing audio to existing capabilities like “Ingredients to Video,” “Frames to Video” and “Extend.”\n\nWe’re also introducing Veo 3.1, which brings richer audio, more narrative control, and enhanced realism that captures true-to-life textures. Veo 3.1 is state-of-the-art and builds on Veo 3, with stronger prompt adherence and improved audiovisual quality when turning images into videos.\n\n## Refine your narrative with audio and more control\n\nWith Veo 3.1, we’re bringing audio to existing capabilities to help you craft the perfect scene. These features are experimental and actively improving, and we’re excited to see what you create as we iterate based on your feedback.\n\nNow, with rich, generated audio, you can:\n\n**Craft the look of your scene.**With \"Ingredients to Video,\" you can use multiple reference images to control the characters, objects and style. Flow uses your ingredients to create a final scene that looks just as you envisioned. **Control the shot from start to finish.**Provide a starting and ending image with “Frames to Video,” and Flow will generate a seamless video that bridges the two, perfect for artful and epic transitions. **Create longer, seamless shots.**With \"Extend,\" you can create longer videos, even lasting for a minute or more, that connect to and continue the action from your original clip. Each video is generated based on the final second of your previous clip, making it most useful for creating a longer establishing shot.... ## Edit your ingredients and videos with more precision\n\nGreat ideas can strike at any point in the creative process. For moments when the first take isn't the final one, we're introducing new editing capabilities directly within Flow to help you reimagine and perfect your scenes.\n\n**Add new elements to any scene.**With “Insert,” introduce anything you can imagine, from realistic details to fantastical creatures. Flow now handles complex details like shadows and scene lighting, making the addition look natural. **Remove unwanted objects or characters seamlessly.**Soon, you’ll be able to take anything out of a scene, and Flow will reconstruct the background and surroundings, making it look as though the object was never there.\n\n## Start creating in Flow today\n\nWith more precise editing capabilities, audio across all existing features and higher-quality outputs powered by Veo 3.1, we're opening up new possibilities for richer, more powerful video storytelling right inside Flow.\n\nThe Veo 3.1 model is also available via the Gemini API for developers, Vertex AI for enterprise customers, and the Gemini app. New capabilities are available in both Gemini API\n\n2\n\nand Vertex AI\n\n3",
                "domain": "blog.google"
              },
              {
                "position": 2,
                "title": "Google announces Veo 3.1 and updates Flow with more controls, tools",
                "url": "https://9to5google.com/2025/10/15/veo-3-1/",
                "snippet": "Google today announced Veo 3.1 as its latest video generation model, with Flow getting a number of updates to take advantage of the latest capabilities.\n\nCompared to Veo 3, which was announced at I/O 2025 in May, this new version offers richer audio and “enhanced realism that captures true-to-life textures.” Veo 3.1 has a deeper understanding of storytelling, cinematic styles, and character interactions to give you more narrative control. The image-to-video capability benefits from improved audio-visual quality and better follows your prompt.\n\nVeo 3.1 and Veo 3.1 Fast are available in the Gemini app, as well as the Gemini API and Vertex AI, to power text-to-video and image-to-video for horizontal (16×9) and vertical (9×16) outputs.\n\nMeanwhile, Google is updating the Flow filmmaking tool to take advantage of Veo 3.1. Audio generation is coming to:\n\n**Ingredients to Video**: This can include images of multiple characters, objects, and styles **Scene extension**: You can Extend an existing clip, with Google basing it on the last second **Frames to Video**: This lets you specify the starting and ending images, with Flow bridging the two\n\nFlow is getting new editing capabilities. You can insert elements like objects, characters, and details, with Google handling shadows, scene lighting, and other complex details to make everything look natural.\n\nComing soon is the ability to remove objects and characters from a scene, with Flow working to reconstruct the background and surroundings to make the edit seamless.\n\n\n\n## Comments",
                "domain": "9to5google.com"
              },
              {
                "position": 3,
                "title": "Google releases Veo 3.1, adds it to Flow video editor",
                "url": "https://techcrunch.com/2025/10/15/google-releases-veo-3-1-adds-it-to-flow-video-editor/",
                "snippet": "In Brief\nGoogle launched its new video model Veo 3.1 with improved audio output, granular editing controls, and better output for image to video. It said that Veo 3.1 builds on May's Veo 3 release and generates more realistic clips and adheres to prompts better.\nThe model allows users to add an object to the video and have it blend into the clip's style, Google said. Soon, users will be able to remove an existing object from the video in Flow, too.\nVeo 3 already has edit features such as adding reference images to drive a character, providing the first and last frame to generate a clip using AI, and the ability to extend an existing video based on the last few frames. With Veo 3.1, Google is adding audio to all these features to make the clips more lively.\nThe company is rolling out the model to its video editor Flow, the Gemini App, along with Vertex and Gemini APIs. It said that since Flow's launch in May, users have created more than 275 million videos on the app.",
                "domain": "techcrunch.com"
              },
              {
                "position": 4,
                "title": "Introducing Veo 3.1 and new creative capabilities in the Gemini API",
                "url": "https://developers.googleblog.com/en/introducing-veo-3-1-and-new-creative-capabilities-in-the-gemini-api/",
                "snippet": "**Today, we are releasing Veo 3.1 and Veo 3.1 Fast in paid preview in the** **Gemini API** **.** This updated model offers several improvements, as well as improved outputs when generating video from images. These new models are available via the Gemini API in Google AI Studio and Vertex AI. Veo 3.1 is also available in the Gemini app and Flow.\n\nVeo 3.1 and Veo 3.1 Fast empower developers to create more engaging content through significant upgrades. The models now generate\n\n**richer native audio**, from natural conversations to synchronized sound effects, and offer **greater narrative control** with an improved understanding of cinematic styles. **Enhanced image-to-video** capabilities ensure better prompt adherence while delivering superior audio and visual quality and maintaining character consistency across multiple scenes.\n\nIn addition to the model update, we are introducing new model capabilities, including using reference images to guide video generation, extend existing Veo videos, and generate transitions between a first and last frame.\n\nYou can now guide the generation process by providing up to 3 reference images of a character, object, or scene. This is helpful for maintaining character consistency across multiple shots or applying a specific style to your video.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nconfig=types.GenerateVideosConfig(\n\nreference_images=[reference_image1, reference_image2],\n\n),\n\n\n\n```... Your story is no longer limited by the original generation. With Scene extension, you can create longer videos, even lasting for a minute or more, by generating new clips that connect to your previous video. Each new video is generated based on the final second of the previous clip. This maintains visual continuity, making the feature ideal for extending shots with background audio.\n\n```\n\nfrom google import genai\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nvideo=video_to_extend\n\n\n\n```\n\nCreate smooth, natural scenes that bridge two different images. By providing a starting and an ending image, you can direct Veo 3.1 to generate the transition between them, complete with accompanying audio.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nimage=first_frame,\n\nconfig=types.GenerateVideosConfig(\n\nlast_frame=last_frame,\n\n),\n\n\n\n```\n\n**What people are building with Veo 3.1**\n\nPromise Studios, a GenAI movie studio, uses Veo 3.1 within its MUSE Platform to enhance generative storyboarding and previsualization for director-driven storytelling at production quality.... Latitude is experimenting with Veo 3.1 in its generative narrative engine to instantly bring user-created stories to life.\n\nVeo 3.1 and these new capabilities are now available in paid preview via the Gemini API.\n\nVeo 3.1 is the same price as Veo 3. We are incredibly excited to see what you will build with these new capabilities!",
                "domain": "developers.googleblog.com"
              },
              {
                "position": 5,
                "title": "We've got a surprise Pixel Drop for you.",
                "url": "https://blog.google/products/pixel/pixel-drop-july-2025/",
                "snippet": "Here’s what’s new for Pixel:\n\n**Veo 3 on Pixel:**Pixel 9 Pro owners get a full year of our Google AI Pro subscription at no cost, giving them access to the latest features in the Gemini app. And that includes Veo 3, which you can use to describe your idea and watch it come to life as a high-quality, short video, complete with natural audio. **New Circle to Search capabilities:**Dive deeper and ask follow-up questions about anything you see on your screen with AI Mode in Circle to Search, available in the U.S. and India. We’re also adding in-game help in Circle to Search, so you can find helpful articles and videos timestamped to your exact spot in your mobile game, without switching apps. **Gemini on Pixel Watch:**Get the help you need right on your wrist, with our advanced AI models powered by WearOS.",
                "domain": "blog.google"
              },
              {
                "position": 6,
                "title": "Google's Veo 3.1 is better at generating videos from images",
                "url": "https://www.engadget.com/ai/googles-veo-31-is-better-at-generating-videos-from-images-220829129.html",
                "snippet": "Google has released a new update to its Veo AI video generation model that should make it do a better job of sticking to prompts and converting images into videos. Veo 3.1 is available to try today through Google's Gemini API and is now also powering the company's Flow video editor.\nVeo 3.1 builds on the new capabilities Google introduced with launch of Veo 3 at Google I/O 2025. The new model offers better \"prompt adherence,\" according to Google, and should have an easier time creating videos based on the image \"ingredients\" you upload alongside your written prompt. Veo 3.1 also makes it possible to convert images to video and generate audio at the same time, a capability that wasn't available with Veo 3.\nIn Flow, Veo 3.1 supports at least a new feature that gives you finer control over the videos you generate. With what Google calls \"Frame to Video,\" Flow lets you upload a first and last frame, and then generates the video in-between. Adobe Firefly, which is powered by Veo 3, offers a similar feature, but Flow will be able to pull it off and create audio at the same time. Those added audio skills will also apply to the video editor's ability to extend clips and insert objects into existing footage, too.\nBased on the samples Google's shared, videos generated with Veo 3.1 still have an uncanny quality that seems to vary greatly depending on the prompt and subject. Even if it's missing some of the realism of OpenAI's Sora 2, though, the company's decision to try and make Veo more useful to people who actually work with video rather than a source of social media spam is a welcome move.",
                "domain": "www.engadget.com"
              },
              {
                "position": 7,
                "title": "Google rolls out its new Veo 3 video-generation model ...",
                "url": "https://techcrunch.com/2025/07/03/google-rolls-out-its-new-veo-3-video-generation-model-globally/",
                "snippet": "Google on Thursday said it has begun rolling out its Veo 3 video generation model to Gemini users in more than 159 countries.\n\nVideo generation via the new model is available only to paying subscribers of Google’s AI Pro plan and is capped at three videos per day.\n\nVeo 3, which Google showed off in May, lets users generate videos up to eight seconds long using text prompts.\n\nGoogle’s Josh Woodward has said that the company is working on adding image-to-video generation capabilities to Gemini.",
                "domain": "techcrunch.com"
              },
              {
                "position": 8,
                "title": "Google's Veo 3 Update! July 2025!",
                "url": "https://www.youtube.com/watch?v=qhReJkSRKOc&vl=en",
                "snippet": "## Murray Frost\n##### Jul 09, 2025 (0:04:15)\n✅ Build a Monetized YouTube Channel in 90 days: https://murrayfrost.com/YT-Accelerator\n\nI teach people YouTube from REAL data from over 150 clients and my own channels. Data-driven feedback and strategies. None of this guessing garbage people put on online teaching you how to do YouTube.... {ts:0} So, Google Labs just had an update and it's technically called Flow. It's on\n{ts:4} their labs.google platform and you can see by the beginning of this video that it does still need some work, but you\n{ts:9} haven't been able to do that recently with Google Labs. And now you can do it with Google's Vo3. So, there's been a\n{ts:15} couple updates here that you can see. And the first one here is using images or allowing images to talk with Google's\n{ts:20} VO3, which again still needs some work, but look, it gets maybe 60% of the way there. I think it looks pretty decent.\n{ts:29} I'm obviously not going to use it to try and convince people that it's real, but maybe you can get kind of creative with\n{ts:34} this and get people to make some really funny, strange, or just dumb things that people love. I don't know why people\n{ts:42} just love brain rot these days. Now, they're also adding the option to do this with V V3 on frames to video, which\n{ts:49} is actually kind of cool. And they... 're also allowing you to top up your subscription with the kind of a mid tier\n{ts:56} option cuz previously they had just the the starter which was about 20 bucks or so per month in the US and then it was\n{ts:63} straight up to I think 250 without the discount for the first 3 months and there's just no in between. It's just a\n{ts:70} massive jump. So they added like a kind of a mid tier there. I think a couple too. Well, I'll show you what that looks\n{ts:75} like and you can top up your credits there as well. They've also gone ahead and just added better audio coverage,\n{ts:83} which I haven't really noticed all that much to be honest. Right now, there's not a huge difference as at least a\n{ts:89} noticeable difference in my opinion from me using it. Um, they also do remove audio when miners are involved. Keep\n{ts:96} that in mind. That's why your audio isn't being generated if you have kids in the video or maybe uh even teenagers\n{ts:101} sometimes. Um, but then there's also they've mentioned they're reducing um unwanted subtitles, which is actually... {ts:107} quite nice. They've been removing the the VEO watermark as well, but now they said they've reduced the unwanted\n{ts:114} subtitles. I still get them sometimes. So, I literally in caps specify in the prompt to not include captions because\n{ts:121} otherwise if I don't, sometimes the captions still show up. And the really nice quality of life update they've made\n{ts:127} here is that when you are just starting a new project or revisiting an existing one, it doesn't reset the model that you\n{ts:135} have, or at least if it does, it resets to the VO3 fast beta audio. So, this is where you're generating audio. So now\n{ts:142} you don't accidentally have VO2 selected with no audio every time you either reload a page, start a new project, or\n{ts:150} you leave and come back and it's been reset to just its default VO2. Now it's actually its default is V3 fast beta\n{ts:157} audio. So the 20 credits per generation, the cheaper VO3 option with audio. So now you don't accidentally generate\n{ts:164} videos without audio, which has happened a lot to me and I... 've wasted thousands of credits doing that by accident. So,\n{ts:171} great update right there. Quality of life, which you don't have to waste any more credits. Now, now let's say that\n{ts:176} you don't want to spend the $124 per month, and this is for the first 3 months. Then, it goes to 150, I believe,\n{ts:183} per month. So, I'm probably going to cancel it at that point because that's just really expensive, at least using\n{ts:189} VO3. Now you have the option if you have the uh let's see which was it the pro subscription the $20 a month\n{ts:195} subscription right here Google AI pro you still get a th000 credits per month in uh Google labs but you also have the\n{ts:204} option to top up your credits so for example in here when you're creating your AI videos if you run out of credits\n{ts:211} like here you can just hit get more AI credits and you can choose how much you want to add so you don't have to spend\n{ts:216} $200 or $150 at a time you could spend an extra for 24 bucks that month just to top up your credits. This wasn... 't\n{ts:223} available in the lower plans. It was only available in the the maximum ultra tier. So, I really like the ability to\n{ts:230} do that now. So, you don't have to spend so much all at once, especially if you're not using all your credits at the\n{ts:235} end of each month, but maybe one month you're just out and you need to add more, you can do so and add some\n{ts:240} credits, which is actually quite nice. I've hit this button a little too much recently, but uh hey, I'm getting good\n{ts:247} results. So, those are all the current updates with Google's Veo3 with their labs platform. Hope to see you in the\n{ts:253} next one.",
                "domain": "www.youtube.com"
              },
              {
                "position": 9,
                "title": "Build with Veo 3, now available in the Gemini API",
                "url": "https://developers.googleblog.com/en/veo-3-now-available-gemini-api/",
                "snippet": "First unveiled at Google I/O 2025, people around the world have already generated tens of millions of high-quality videos with Veo 3 (along with some new fun and interesting video trends). It is our first video model to incorporate high-fidelity video outputs and native audio, first with text-to-video and soon with image-to-video.\n\nDevelopers are already experimenting with Veo 3, discovering how the model can help them brainstorm content, rapidly iterate, and be more efficient.\n\nVeo 3 is designed to handle a range of video generation tasks, from cinematic narratives to dynamic character animations. With Veo 3, you can create more immersive experiences by not only generating stunning visuals, but also audio like dialogue and sound effects.\n\nLet’s take a look at some examples.\n\nExplore these examples and more with Veo 3 in Google AI Studio, available as an SDK template and interactive Starter App to remix, copy and extend. The Starter App and its sample code offer a convenient way for Paid Tier users to rapidly prototype with Veo 3 and more on the Gemini API, directly from Google AI Studio.\n\nClick the Key button in the top right of the AI Studio Build interface to select a Google Cloud Project with billing enabled to use the Paid Tier in AI Studio apps. See the FAQs for more.... Veo 3 will be priced at $0.75 per second for video and audio output. Additionally, Veo 3 Fast will be available soon, offering a faster and more cost-effective option for video creation.\n\nHere’s a basic Python example to create a video:\n\n```\n\nimport time\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.0-generate-preview\",\n\nprompt=\"a close-up shot of a golden retriever playing in a field of sunflowers\",\n\nconfig=types.GenerateVideosConfig(\n\nnegative_prompt=\"barking, woofing\",\n\n),\n\n\n\n# Waiting for the video(s) to be generated\n\nwhile not operation.done:\n\ntime.sleep(20)\n\noperation = client.operations.get(operation)\n\ngenerated_video = operation.result.generated_videos[0]\n\nclient.files.download(file=generated_video.video)\n\ngenerated_video.video.save(\"veo3_video.mp4\")\n\n```\n\nAll videos generated by Veo 3 models will continue to include a digital SynthID watermark. To get started, check out the documentation, cookbook, and a Veo 3 starter app in Google AI Studio:\n\nIn addition to being available via the Gemini API in Google AI Studio, Veo 3 is also available to Google AI subscribers in the Gemini app and Flow, and to enterprise customers via Vertex AI.... 1: Fluffy Characters Stop Motion: Inside a brightly colored, cozy kitchen made of felt and yarn. Professor Nibbles, a plump, fluffy hamster with oversized glasses, nervously stirs a bubbling pot on a miniature stove, muttering, \"Just a little more... 'essence of savory,' as the recipe calls for.\" The camera is a mid-shot, capturing his frantic stirring. Suddenly, the pot emits a loud \"POP!\" followed by a comical \"whoosh\" sound, and a geyser of iridescent green slime erupts, covering the entire kitchen. Professor Nibbles shrieks, \"Oh, dear! Not again!\" and scurries away, leaving a trail of tiny, panicked squeaks.\n\n2: The sequence begins with an extreme close-up of a single gear, slowly turning and reflecting harsh sunlight. The camera gradually pulls back in a continuous movement, revealing this is but one component of a colossal, mechanical heart half-buried in a desolate, rust-colored desert. A sweeping aerial shot establishes its enormous scale and isolation in the barren landscape. The camera descends to capture pipes hissing steam and the rhythmic thumping that echoes across the empty plains. A subtle shake effect synchronizes with each massive heartbeat. A lateral tracking shot discovers tiny, robed figures scurrying across the metallic surface. The camera follows one such figure in a detailed tracking shot as they perform meticulous maintenance, polishing brass valves and tightening immense bolts. A complex movement circles the entire structure, capturing different maintenance teams working in precarious positions across its rusted exterior. The final shot begins tight on the meticulous work of one tiny figure before executing a dramatic pull-out that reveals the true scale of the heart and the minuscule size of its caretakers, tending to the vital organ of an unseen, sleeping giant that extends beyond the frame.",
                "domain": "developers.googleblog.com"
              },
              {
                "position": 10,
                "title": "Google Veo 3.1 Just Dropped — How to Use It + What’s New! ✅",
                "url": "https://www.youtube.com/watch?v=y0QZbjd2f_k",
                "snippet": "## Aivoxy\n##### Oct 15, 2025 (0:03:37)\nGoogle Veo 3.1 Just Dropped — How to Use It + What’s New! ✅\n\n🔥 For Prompts:\nhttps://t.me/aivoxxy\n\nveo 3.1\ngoogle veo 3.1\nveo3.1\nveo 3\ngoogle veo 3\nveo 3.1 release date\nveo 3.1 generated videos\ngoogle veo 3.1\n\nGoogle DeepMind just released Veo 3.1, and it’s packed with upgrades that could change the AI video game forever.\nIn this video, we demonstrate how to use Veo 3.1, walk you through the new features, and explain why this update is so exciting.\n\nSome highlights of Veo 3.1 include:\nScene Builder — plan your video shot by shot\nCharacter Consistency — no more random face changes\nNative 1080p output with cinematic presets\nMulti-prompting for multi-shot sequences\nAudio & dialogue generation built-in\n\nWe’ll also compare it with other AI video tools like Sora 2, so you know what’s really new and what’s just hype.\n\n👨‍👩‍👦‍👦 Join this channel to get access to perks:\nhttps://www.youtube.com/channel/UC4VpM2gtTxbXSiyL1ODl9uQ/join\n\n🔥 This AI Bigfoot Vlog Looks 100% Real – Made with Veo 3!\nhttps://youtu.be/_1EH25-1NkQ\n\n🔥 Google Veo 3 Just Changed Video Creation Forever! 😱🚨\nhttps://youtu.be/-_4jOwjQaQI... 🔥 Google AI Mode Just Changed Search FOREVER!\nhttps://youtu.be/JX64HQ8_QCg\n--------------------------------------------------------------------------\n🔥 Grok 3 Now Has Voice! 🔥 4 Mind-Blowing Examples You Need to See!\nhttps://youtu.be/B_pKBwj08Wg\n\n🔥 Grok 3 vs ChatGPT: Unfiltered Voice Chat? Surprising Responses!\nhttps://youtu.be/snOj_6VytiY\n\n🔥 Grok 3 vs ChatGPT vs DeepSeek vs Claude 3.5: Who's the Brainiest? 🤯\nhttps://youtu.be/XKDdUuy-hmE... 🔥 You Think It's Fake - DeepSeek vs ChatGPT\nhttps://youtu.be/SruDuJkw78U\n-------------------------------------------------------------------------\n#veo3.1 #veo3 #aivideo... {ts:0} Come closer. Let me share a secret with you.\n{ts:5} I am your new girlfriend. Google just surprised everyone. VO 3.1 is officially live today. It's faster,\n{ts:22} smarter, and now generates full cinematic clips with sound. And yeah, I've already got access. Let me show you\n{ts:30} how crazy this is. So, here's the thing. VO3.1 isn't available everywhere just yet. Right now, it's rolling out only\n{ts:39} inside Google Flow. So, if you already have a Google AI account, you can access it right from there. I'm already logged\n{ts:46} in and as you can see, the VO3.1 fast model is live. So, yeah, no fake claims here. This is real and I've been talking\n{ts:55} about this since my earlier videos. Now using it is simple. Write your prompt. Select VO3.1 fast. Hit generate. That's\n{ts:65} it. No complex settings, no extra steps. All right, let's talk about what's actually\n{ts:79} new in VO3.1. This update isn't a total overhaul. It... 's more like a smarter, cleaner upgrade to\n{ts:87} V3. Here's what's changed. One, smarter scene creation. You can now combine reference images, locations, characters,\n{ts:96} and objects, and VO automatically builds a cohesive scene around them. No more weird lighting mismatches or random jump\n{ts:104} cuts. Two, built-in sound generation. Every clip now includes matching audio, ambient sounds, dialogue, or effects\n{ts:113} that sync perfectly with the visuals. It makes your videos feel far more cinematic and alive. Three, clip\n{ts:121} extension. You can take a short clip and expand it into a full cinematic sequence. VO keeps the same lighting,\n{ts:129} motion, and audio flow seamlessly. It's perfect for trailers, ads, or storydriven projects. Four, custom start\n{ts:138} and end control. Now you can choose where a shot begins and ends, and VO fills in the middle with smooth,\n{ts:145} realistic transitions that gives you more storytelling control. Five, add or remove elements. Want to remove an\n{ts:153} object or drop in a new one? Just select it. VO automatically adjusts lighting, shadows, and scale to keep everything... {ts:161} realistic. Six, higher realism and physics accuracy. Scenes now behave like real life. Lighting looks natural,\n{ts:170} character movement feels grounded, and the visuals have that cinematic finish. You'll also notice image to video\n{ts:176} generation looks a lot cleaner now. Frames are stable, colors look balanced, and it feels much closer to real\n{ts:183} footage. That said, it still can't generate long videos yet. The maximum I've seen is around 8 seconds. So don't\n{ts:191} expect 1 minute clips for now. So yeah, not a massive reinvention, but a much smarter, smoother version of what VO\n{ts:198} already did well. I'll be testing this more and posting sideby-side comparisons soon. Stay tuned to see how it compares\n{ts:205} to VO3. If this video helped you, hit like, share it with your tech buddies, and make sure you subscribe for more AI\n{ts:212} updates like this. Thanks for watching and I'll see you in the next",
                "domain": "www.youtube.com"
              }
            ],
            "success": true,
            "error": null
          },
          {
            "query_id": "q3",
            "query": "veo3.1 software version Google 2025 official",
            "claim_id": "claim_1",
            "query_type": "source_verification",
            "priority": "high",
            "results": [
              {
                "position": 1,
                "title": "Introducing Veo 3.1 and advanced capabilities in Flow - Google Blog",
                "url": "https://blog.google/technology/ai/veo-updates-flow/",
                "snippet": "# Introducing Veo 3.1 and advanced capabilities in Flow\n\nFive months ago, we introduced Flow, our AI filmmaking tool powered by Veo, and have been inspired by the creativity it has sparked with over 275 million videos generated in Flow\n\n1\n\n. We're always listening to your feedback, and we've heard that you want more artistic control within Flow, with increased support for audio across all features.\n\nToday, we’re introducing new and enhanced creative capabilities to edit your clips, giving you more granular control over your final scene. For the first time, we’re also bringing audio to existing capabilities like “Ingredients to Video,” “Frames to Video” and “Extend.”\n\nWe’re also introducing Veo 3.1, which brings richer audio, more narrative control, and enhanced realism that captures true-to-life textures. Veo 3.1 is state-of-the-art and builds on Veo 3, with stronger prompt adherence and improved audiovisual quality when turning images into videos.\n\n## Refine your narrative with audio and more control\n\nWith Veo 3.1, we’re bringing audio to existing capabilities to help you craft the perfect scene. These features are experimental and actively improving, and we’re excited to see what you create as we iterate based on your feedback.\n\nNow, with rich, generated audio, you can:\n\n**Craft the look of your scene.**With \"Ingredients to Video,\" you can use multiple reference images to control the characters, objects and style. Flow uses your ingredients to create a final scene that looks just as you envisioned. **Control the shot from start to finish.**Provide a starting and ending image with “Frames to Video,” and Flow will generate a seamless video that bridges the two, perfect for artful and epic transitions. **Create longer, seamless shots.**With \"Extend,\" you can create longer videos, even lasting for a minute or more, that connect to and continue the action from your original clip. Each video is generated based on the final second of your previous clip, making it most useful for creating a longer establishing shot.... ## Edit your ingredients and videos with more precision\n\nGreat ideas can strike at any point in the creative process. For moments when the first take isn't the final one, we're introducing new editing capabilities directly within Flow to help you reimagine and perfect your scenes.\n\n**Add new elements to any scene.**With “Insert,” introduce anything you can imagine, from realistic details to fantastical creatures. Flow now handles complex details like shadows and scene lighting, making the addition look natural. **Remove unwanted objects or characters seamlessly.**Soon, you’ll be able to take anything out of a scene, and Flow will reconstruct the background and surroundings, making it look as though the object was never there.\n\n## Start creating in Flow today\n\nWith more precise editing capabilities, audio across all existing features and higher-quality outputs powered by Veo 3.1, we're opening up new possibilities for richer, more powerful video storytelling right inside Flow.\n\nThe Veo 3.1 model is also available via the Gemini API for developers, Vertex AI for enterprise customers, and the Gemini app. New capabilities are available in both Gemini API\n\n2\n\nand Vertex AI\n\n3",
                "domain": "blog.google"
              },
              {
                "position": 2,
                "title": "Build with Veo 3, now available in the Gemini API",
                "url": "https://developers.googleblog.com/en/veo-3-now-available-gemini-api/",
                "snippet": "First unveiled at Google I/O 2025, people around the world have already generated tens of millions of high-quality videos with Veo 3 (along with some new fun and interesting video trends). It is our first video model to incorporate high-fidelity video outputs and native audio, first with text-to-video and soon with image-to-video.\n\nDevelopers are already experimenting with Veo 3, discovering how the model can help them brainstorm content, rapidly iterate, and be more efficient.\n\nVeo 3 is designed to handle a range of video generation tasks, from cinematic narratives to dynamic character animations. With Veo 3, you can create more immersive experiences by not only generating stunning visuals, but also audio like dialogue and sound effects.\n\nLet’s take a look at some examples.\n\nExplore these examples and more with Veo 3 in Google AI Studio, available as an SDK template and interactive Starter App to remix, copy and extend. The Starter App and its sample code offer a convenient way for Paid Tier users to rapidly prototype with Veo 3 and more on the Gemini API, directly from Google AI Studio.\n\nClick the Key button in the top right of the AI Studio Build interface to select a Google Cloud Project with billing enabled to use the Paid Tier in AI Studio apps. See the FAQs for more.... Veo 3 will be priced at $0.75 per second for video and audio output. Additionally, Veo 3 Fast will be available soon, offering a faster and more cost-effective option for video creation.\n\nHere’s a basic Python example to create a video:\n\n```\n\nimport time\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.0-generate-preview\",\n\nprompt=\"a close-up shot of a golden retriever playing in a field of sunflowers\",\n\nconfig=types.GenerateVideosConfig(\n\nnegative_prompt=\"barking, woofing\",\n\n),\n\n\n\n# Waiting for the video(s) to be generated\n\nwhile not operation.done:\n\ntime.sleep(20)\n\noperation = client.operations.get(operation)\n\ngenerated_video = operation.result.generated_videos[0]\n\nclient.files.download(file=generated_video.video)\n\ngenerated_video.video.save(\"veo3_video.mp4\")\n\n```\n\nAll videos generated by Veo 3 models will continue to include a digital SynthID watermark. To get started, check out the documentation, cookbook, and a Veo 3 starter app in Google AI Studio:\n\nIn addition to being available via the Gemini API in Google AI Studio, Veo 3 is also available to Google AI subscribers in the Gemini app and Flow, and to enterprise customers via Vertex AI.... 1: Fluffy Characters Stop Motion: Inside a brightly colored, cozy kitchen made of felt and yarn. Professor Nibbles, a plump, fluffy hamster with oversized glasses, nervously stirs a bubbling pot on a miniature stove, muttering, \"Just a little more... 'essence of savory,' as the recipe calls for.\" The camera is a mid-shot, capturing his frantic stirring. Suddenly, the pot emits a loud \"POP!\" followed by a comical \"whoosh\" sound, and a geyser of iridescent green slime erupts, covering the entire kitchen. Professor Nibbles shrieks, \"Oh, dear! Not again!\" and scurries away, leaving a trail of tiny, panicked squeaks.\n\n2: The sequence begins with an extreme close-up of a single gear, slowly turning and reflecting harsh sunlight. The camera gradually pulls back in a continuous movement, revealing this is but one component of a colossal, mechanical heart half-buried in a desolate, rust-colored desert. A sweeping aerial shot establishes its enormous scale and isolation in the barren landscape. The camera descends to capture pipes hissing steam and the rhythmic thumping that echoes across the empty plains. A subtle shake effect synchronizes with each massive heartbeat. A lateral tracking shot discovers tiny, robed figures scurrying across the metallic surface. The camera follows one such figure in a detailed tracking shot as they perform meticulous maintenance, polishing brass valves and tightening immense bolts. A complex movement circles the entire structure, capturing different maintenance teams working in precarious positions across its rusted exterior. The final shot begins tight on the meticulous work of one tiny figure before executing a dramatic pull-out that reveals the true scale of the heart and the minuscule size of its caretakers, tending to the vital organ of an unseen, sleeping giant that extends beyond the frame.",
                "domain": "developers.googleblog.com"
              },
              {
                "position": 3,
                "title": "Google announces Veo 3.1 and updates Flow with more controls, tools",
                "url": "https://9to5google.com/2025/10/15/veo-3-1/",
                "snippet": "Google today announced Veo 3.1 as its latest video generation model, with Flow getting a number of updates to take advantage of the latest capabilities.\n\nCompared to Veo 3, which was announced at I/O 2025 in May, this new version offers richer audio and “enhanced realism that captures true-to-life textures.” Veo 3.1 has a deeper understanding of storytelling, cinematic styles, and character interactions to give you more narrative control. The image-to-video capability benefits from improved audio-visual quality and better follows your prompt.\n\nVeo 3.1 and Veo 3.1 Fast are available in the Gemini app, as well as the Gemini API and Vertex AI, to power text-to-video and image-to-video for horizontal (16×9) and vertical (9×16) outputs.\n\nMeanwhile, Google is updating the Flow filmmaking tool to take advantage of Veo 3.1. Audio generation is coming to:\n\n**Ingredients to Video**: This can include images of multiple characters, objects, and styles **Scene extension**: You can Extend an existing clip, with Google basing it on the last second **Frames to Video**: This lets you specify the starting and ending images, with Flow bridging the two\n\nFlow is getting new editing capabilities. You can insert elements like objects, characters, and details, with Google handling shadows, scene lighting, and other complex details to make everything look natural.\n\nComing soon is the ability to remove objects and characters from a scene, with Flow working to reconstruct the background and surroundings to make the edit seamless.\n\n\n\n## Comments",
                "domain": "9to5google.com"
              },
              {
                "position": 4,
                "title": "Introducing Veo 3.1 and new creative capabilities in the Gemini API",
                "url": "https://developers.googleblog.com/en/introducing-veo-3-1-and-new-creative-capabilities-in-the-gemini-api/",
                "snippet": "**Today, we are releasing Veo 3.1 and Veo 3.1 Fast in paid preview in the** **Gemini API** **.** This updated model offers several improvements, as well as improved outputs when generating video from images. These new models are available via the Gemini API in Google AI Studio and Vertex AI. Veo 3.1 is also available in the Gemini app and Flow.\n\nVeo 3.1 and Veo 3.1 Fast empower developers to create more engaging content through significant upgrades. The models now generate\n\n**richer native audio**, from natural conversations to synchronized sound effects, and offer **greater narrative control** with an improved understanding of cinematic styles. **Enhanced image-to-video** capabilities ensure better prompt adherence while delivering superior audio and visual quality and maintaining character consistency across multiple scenes.\n\nIn addition to the model update, we are introducing new model capabilities, including using reference images to guide video generation, extend existing Veo videos, and generate transitions between a first and last frame.\n\nYou can now guide the generation process by providing up to 3 reference images of a character, object, or scene. This is helpful for maintaining character consistency across multiple shots or applying a specific style to your video.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nconfig=types.GenerateVideosConfig(\n\nreference_images=[reference_image1, reference_image2],\n\n),\n\n\n\n```... Your story is no longer limited by the original generation. With Scene extension, you can create longer videos, even lasting for a minute or more, by generating new clips that connect to your previous video. Each new video is generated based on the final second of the previous clip. This maintains visual continuity, making the feature ideal for extending shots with background audio.\n\n```\n\nfrom google import genai\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nvideo=video_to_extend\n\n\n\n```\n\nCreate smooth, natural scenes that bridge two different images. By providing a starting and an ending image, you can direct Veo 3.1 to generate the transition between them, complete with accompanying audio.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nimage=first_frame,\n\nconfig=types.GenerateVideosConfig(\n\nlast_frame=last_frame,\n\n),\n\n\n\n```\n\n**What people are building with Veo 3.1**\n\nPromise Studios, a GenAI movie studio, uses Veo 3.1 within its MUSE Platform to enhance generative storyboarding and previsualization for director-driven storytelling at production quality.... Latitude is experimenting with Veo 3.1 in its generative narrative engine to instantly bring user-created stories to life.\n\nVeo 3.1 and these new capabilities are now available in paid preview via the Gemini API.\n\nVeo 3.1 is the same price as Veo 3. We are incredibly excited to see what you will build with these new capabilities!",
                "domain": "developers.googleblog.com"
              },
              {
                "position": 5,
                "title": "Google's Veo 3.1 is better at generating videos from images",
                "url": "https://www.engadget.com/ai/googles-veo-31-is-better-at-generating-videos-from-images-220829129.html",
                "snippet": "Google has released a new update to its Veo AI video generation model that should make it do a better job of sticking to prompts and converting images into videos. Veo 3.1 is available to try today through Google's Gemini API and is now also powering the company's Flow video editor.\nVeo 3.1 builds on the new capabilities Google introduced with launch of Veo 3 at Google I/O 2025. The new model offers better \"prompt adherence,\" according to Google, and should have an easier time creating videos based on the image \"ingredients\" you upload alongside your written prompt. Veo 3.1 also makes it possible to convert images to video and generate audio at the same time, a capability that wasn't available with Veo 3.\nIn Flow, Veo 3.1 supports at least a new feature that gives you finer control over the videos you generate. With what Google calls \"Frame to Video,\" Flow lets you upload a first and last frame, and then generates the video in-between. Adobe Firefly, which is powered by Veo 3, offers a similar feature, but Flow will be able to pull it off and create audio at the same time. Those added audio skills will also apply to the video editor's ability to extend clips and insert objects into existing footage, too.\nBased on the samples Google's shared, videos generated with Veo 3.1 still have an uncanny quality that seems to vary greatly depending on the prompt and subject. Even if it's missing some of the realism of OpenAI's Sora 2, though, the company's decision to try and make Veo more useful to people who actually work with video rather than a source of social media spam is a welcome move.",
                "domain": "www.engadget.com"
              },
              {
                "position": 6,
                "title": "Veo 3 Fast available for everyone on Vertex AI | Google Cloud Blog",
                "url": "https://cloud.google.com/blog/products/ai-machine-learning/veo-3-fast-available-for-everyone-on-vertex-ai",
                "snippet": "# Veo 3 and Veo 3 Fast are now generally available on Vertex AI\n\n##### Jason Gelman\n\nDirector, Product Management, Vertex AI... ##### Try Gemini 2.5\n\nOur most intelligent model is now available on Vertex AITry now\n\nVeo 3 has seen massive global adoption with over 70 million videos created since May, and we've seen tremendous momentum with our enterprise customers as well. Since its preview launch on Vertex AI in June, enterprise customers have already generated over 6 million videos, showcasing the incredible demand for professional-grade, scalable AI video creation.\n\nToday, we’re building on this momentum with some exciting updates to Veo on Vertex AI.\n\n\n\n**Veo 3,**our most advanced video generation model, is now generally available to everyone on Vertex AI.\n\n\n\n**Veo 3 Fast**, a model designed for speed and rapid iteration, is now generally available for everyone on Vertex AI. It's a faster way to turn text to video, from narrated product demos to short films.\n\n\n\nComing to public preview on Vertex AI in August, Veo 3 and Veo 3 Fast will also offer\n\n**image-to-video capabilities**to make it possible for you to bring static visuals and images to life. All you have to do is provide the source image along with a text prompt that describes what kind of video you want to create.\n\n**How businesses are building with Veo 3 on Vertex AI**... Google Cloud customers around the world are using Veo 3 and Veo 3 Fast on Vertex AI to create professional-quality video content with unparalleled efficiency and creative freedom. Let’s look at some examples.\n\n**Canva**\n\n“Enabling anyone to bring their ideas to life – especially their most creative ones – has been core to Canva's mission ever since we set out to empower the world to design. By democratising access to a powerful technology like Google’s Veo 3 inside Canva AI, your big ideas can now be brought to life in the highest quality video and sound, all from within your existing Canva subscription. In true Canva fashion, we’ve built this with an intuitive interface and simple editing tools in place, all backed by Canva Shield.” –\n\n**Cameron Adams, co-founder and Chief Product Officer, Canva **\n\nBut the momentum extends beyond design. The team at\n\n**BarkleyOKRP**, a leading ad agency, is using Veo 3 to speed up video production timelines.\n\n“The rapid advancements from Veo 2 to Veo 3 within such a short time frame on this project have been nothing short of remarkable. Our team undertook the task of re-creating numerous music videos initially produced with Veo 2 once Veo 3 was released, primarily due to the significantly improved synchronization between voice and mouth movements. The continuous daily progress we are witnessing is truly extraordinary.” –... **Julie Ray Barr, Senior Vice President Client Experience, BarkleyOKRP**\n\nAt global investing platform\n\n**eToro**, the team is making marketing iterations a breeze with Veo 3.\n\n“At eToro, innovation is in our DNA. As a global investing platform serving clients in 75 countries, local storytelling isn’t optional - it’s essential. With Veo 3, we produced 15 fully AI‑generated versions of our ad, each in the native language of its market, all while capturing real emotion at scale. Ironically, AI didn’t reduce humanity - it amplified it. Veo 3 lets us tell more stories, in more tongues, with more impact.” –\n\n**Shay Chikotay, Head of Creative & Content, eToro**\n\n**Razorfish**, an interactive agency and part of the Publicis Groupe, is using Veo to bring creative to life.\n\n\"For The Morelandos, our campaign with Visit Orlando and Google, we used the full Vertex AI stack—Gemini to mine real reviews, Imagen to bring the characters to life, and Veo to give them motion. Veo let us go from story to near-cinematic video in a fraction of the usual time—which meant more room to explore, iterate, and push the idea further.\" –... **Anthony Yell, Chief Creative Officer, Razorfish**\n\n**Synthesia** **, **a leading synthetic media generation company, is using Veo to contextually adapt visuals to its hyper-realistic AI avatars and voices.\n\n“Veo 3 represents a leap forward in generative AI, and its integration into Synthesia’s platform will redefine how businesses create video content. By combining our hyper-realistic AI avatars and voices with Veo-powered fully contextual visuals that adapt to each unique story, we’re giving enterprise teams the creative power to communicate with unrivalled clarity and impact.” –\n\n**Bill Leaver, Product Manager, Synthesia **\n\n**How enterprises can use Veo 3 Fast for speed and creativity **\n\n**Veo 3 Fast** is a great fit for work that requires rapid iteration and speed. It has an ideal balance between processing time and high-quality visual output, making it especially helpful for:\n\n\n\nQuickly generating and testing variations of ad concepts to respond to market trends.\n\n\n\nEfficiently creating video demonstrations for entire product catalogs from still images.\n\n\n\nDeveloping engaging animated explainers and training modules in less time.\n\n**Veo 3 and Veo 3 Fast on Vertex AI mean even more capabilities for enterprise storytelling**\n\nVeo 3 and Veo 3 Fast are designed to give creators the control and quality needed to move beyond short clips and produce complete, compelling narratives. Here are some of the core features now generally available on Vertex AI.... **Create scenes with native audio:**Veo 3 generates video and audio in a single step. This means you can create scenes with characters that speak with accurate lip-syncing, and sound effects that fit the mood. **Deliver professional quality at enterprise scale:**Veo 3 produces high-definition (1080p) video, suitable for professional marketing campaigns, product demonstrations, and internal communications. You can create content that meets brand standards, saving time and money. **Simplify content localization for global audiences:**Veo 3’s native dialogue generation helps businesses connect with an international audience by producing a video once and localizing the dialogue for dozens of languages. **Image-to-video (coming to public preview on Vertex AI in August):**Veo 3 and Veo 3 Fast can also take a single image, which can be a photo you uploaded or an AI-generated image, and animate it, creating an 8-second video clip. This feature is particularly powerful for content creators, marketers, and businesses looking to animate existing visual assets, create engaging social media content, or generate compelling product demonstrations from high-quality images.\n\n### Enterprise-grade safety and security\n\nVeo 3 and Veo 3 Fast on Vertex AI are built for scalable and responsible enterprise use. We embed digital watermarks into every frame with SynthID, helping combat misinformation and misattribution. Veo 3 and Veo 3 Fast are also covered by our indemnity for generative AI services.... ### Get started with Veo 3 and Veo 3 Fast today\n\nTo get started, go here to learn more about Veo 3 and Veo 3 Fast on Vertex AI, and try it on Vertex AI Media Studio.",
                "domain": "cloud.google.com"
              },
              {
                "position": 7,
                "title": "Generate videos with Veo 3.1 in Gemini API",
                "url": "https://ai.google.dev/gemini-api/docs/video",
                "snippet": "Veo 3.1 is Google's state-of-the-art model for generating high-fidelity, 8-second 720p or 1080p videos featuring stunning realism and natively generated audio. You can access this model programmatically using the Gemini API. To learn more about the available Veo model variants, see the Model Versions section.\n\nVeo 3.1 excels at a wide range of visual and cinematic styles and introduces several new capabilities:\n\n**Video extension**: Extend videos that were previously generated using Veo. **Frame-specific generation**: Generate a video by specifying the first and last frames. **Image-based direction**: Use up to three reference images to guide the content of your generated video.\n\nFor more information about writing effective text prompts for video generation, see the Veo prompt guide... ### Go\n\n```\n\npackage main\n\nimport (\n\n\"context\"\n\n\"log\"\n\n\"os\"\n\n\"time\"\n\n\"google.golang.org/genai\"\n\n\n\nfunc main() {\n\nctx := context.Background()\n\nclient, err := genai.NewClient(ctx, nil)\n\nif err != nil {\n\nlog.Fatal(err)\n\n\n\nprompt := `A close up of two people staring at a cryptic drawing on a wall, torchlight flickering.\n\nA man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'`\n\noperation, _ := client.Models.GenerateVideos(\n\nctx,\n\n\"veo-3.1-generate-preview\",\n\nprompt,\n\nnil,\n\nnil,\n\n\n\n// Poll the operation status until the video is ready.\n\nfor !operation.Done {\n\nlog.Println(\"Waiting for video generation to complete...\")\n\ntime.Sleep(10 * time.Second)\n\noperation, _ = client.Operations.GetVideosOperation(ctx, operation, nil)\n\n\n\n// Download the generated video.\n\nvideo := operation.Response.GeneratedVideos[0]\n\nclient.Files.Download(ctx, video.Video, nil)\n\nfname := \"dialogue_example.mp4\"\n\n_ = os.WriteFile(fname, video.Video.VideoBytes, 0644)\n\nlog.Printf(\"Generated video saved to %s\\n\", fname)\n\n\n\n```... ### JavaScript\n\n```\n\nimport { GoogleGenAI } from \"@google/genai\";\n\nconst ai = new GoogleGenAI({});\n\nconst prompt = \"Panning wide shot of a calico kitten sleeping in the sunshine\";\n\n// Step 1: Generate an image with Nano Banana.\n\nconst imageResponse = await ai.models.generateContent({\n\nmodel: \"gemini-2.5-flash-image\",\n\nprompt: prompt,\n\n});\n\n// Step 2: Generate video with Veo 3.1 using the image.\n\nlet operation = await ai.models.generateVideos({\n\nmodel: \"veo-3.1-generate-preview\",\n\nprompt: prompt,\n\nimage: {\n\nimageBytes: imageResponse.generatedImages[0].image.imageBytes,\n\nmimeType: \"image/png\",\n\n},\n\n});\n\n// Poll the operation status until the video is ready.\n\nwhile (!operation.done) {\n\nconsole.log(\"Waiting for video generation to complete...\")\n\nawait new Promise((resolve) => setTimeout(resolve, 10000));\n\noperation = await ai.operations.getVideosOperation({\n\noperation: operation,\n\n});\n\n\n\n// Download the video.\n\nai.files.download({\n\nfile: operation.response.generatedVideos[0].video,\n\ndownloadPath: \"veo3_with_image_input.mp4\",\n\n});\n\nconsole.log(`Generated video saved to veo3_with_image_input.mp4`);\n\n```... ### Go\n\n```\n\npackage main\n\nimport (\n\n\"context\"\n\n\"log\"\n\n\"os\"\n\n\"time\"\n\n\"google.golang.org/genai\"\n\n\n\nfunc main() {\n\nctx := context.Background()\n\nclient, err := genai.NewClient(ctx, nil)\n\nif err != nil {\n\nlog.Fatal(err)\n\n\n\nprompt := \"Panning wide shot of a calico kitten sleeping in the sunshine\"\n\n// Step 1: Generate an image with Nano Banana.\n\nimageResponse, err := client.Models.GenerateContent(\n\nctx,\n\n\"gemini-2.5-flash-image\",\n\nprompt,\n\nnil, // GenerateImagesConfig\n\n\n\nif err != nil {\n\nlog.Fatal(err)\n\n\n\n// Step 2: Generate video with Veo 3.1 using the image.\n\noperation, err := client.Models.GenerateVideos(\n\nctx,\n\n\"veo-3.1-generate-preview\",\n\nprompt,\n\nimageResponse.GeneratedImages[0].Image,\n\nnil, // GenerateVideosConfig\n\n\n\nif err != nil {\n\nlog.Fatal(err)\n\n\n\n// Poll the operation status until the video is ready.\n\nfor !operation.Done {\n\nlog.Println(\"Waiting for video generation to complete...\")\n\ntime.Sleep(10 * time.Second)\n\noperation, _ = client.Operations.GetVideosOperation(ctx, operation, nil)\n\n\n\n// Download the video.\n\nvideo := operation.Response.GeneratedVideos[0]\n\nclient.Files.Download(ctx, video.Video, nil)\n\nfname := \"veo3_with_image_input.mp4\"\n\n_ = os.WriteFile(fname, video.Video.VideoBytes, 0644)\n\nlog.Printf(\"Generated video saved to %s\\n\", fname)\n\n\n\n```... ## Extending Veo videos\n\nUse Veo 3.1 to extend videos that you previously generated with Veo by 7 seconds and up to 20 times.\n\nInput video limitations:\n\n- Veo-generated videos only up to 141 seconds long.\n\n- Gemini API only supports video extensions for Veo-generated videos.\n\n- Input videos are expected to have a certain length, aspect ratio, and dimensions:\n\n- Aspect ratio: 9:16 or 16:9\n\n- Resolution: 720p\n\n- Video length: 141 seconds or less\n\nThe output of the extension is a single video combining the user input video and the generated extended video for up to 148 seconds of video.\n\nThis example takes the Veo-generated video\n\n*butterfly_video*, shown here with\n\nits original prompt, and extends it using the\n\n`video` parameter and a new\n\nprompt:\n\n|Prompt|Output: `butterfly_video`|\n|--|--|\n|An origami butterfly flaps its wings and flies out of the french doors into the garden.|\n### Python\n\n```\n\nimport time\n\nfrom google import genai\n\nclient = genai.Client()\n\nprompt = \"Track the butterfly into the garden as it lands on an orange origami flower. A fluffy white puppy runs up and gently pats the flower.\"\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nvideo=butterfly_video,\n\nprompt=prompt,\n\nconfig=types.GenerateVideosConfig(\n\nnumber_of_videos=1,\n\nresolution=\"720p\"\n\n),\n\n\n\n# Poll the operation status until the video is ready.\n\nwhile not operation.done:\n\nprint(\"Waiting for video generation to complete...\")\n\ntime.sleep(10)\n\noperation = client.operations.get(operation)\n\n# Download the video.\n\nvideo = operation.response.generated_videos[0]\n\nclient.files.download(file=video.video)\n\nvideo.video.save(\"veo3.1_extension.mp4\")\n\nprint(\"Generated video saved to veo3.1_extension.mp4\")\n\n```\n\nFor information about writing effective text prompts for video generation, see the Veo prompt guide.... ## Model features\n\n|Feature|Description|Veo 3.1 & Veo 3.1 Fast|Veo 3 & Veo 3 Fast|Veo 2|\n|--|--|--|--|--|\n|Audio|Natively generates audio with video.|Natively generates audio with video.|✔️ Always on|❌ Silent only|\n|Input Modalities|The type of input used for generation.|Text-to-Video, Image-to-Video, Video-to-Video|Text-to-Video, Image-to-Video|Text-to-Video, Image-to-Video|\n|Resolution|The output resolution of the video.|720p & 1080p (8s length only) 720p only when using video extension.|720p & 1080p (16:9 only)|720p|\n|Frame Rate|The output frame rate of the video.|24fps|24fps|24fps|\n|Video Duration|Length of the generated video.|8 seconds, 6 seconds, 4 seconds 8 seconds only when using reference images|8 seconds|5-8 seconds|\n|Videos per Request|Number of videos generated per request.|1|1|1 or 2|\n|Status & Details|Model availability and further details.|Preview|Stable|Stable|... ## Model versions\n\nCheck out the Pricing and Rate limits pages for more Veo model-specific usage details.\n\n### Veo 3.1 Preview\n\n|Property|Description|\n|--|--|\n|Model code||\n|Supported data types|Text, Image Video with audio|\n|Limits|1,024 tokens 1|\n|Latest update|September 2025|\n### Veo 3.1 Fast Preview\n\n|Property|Description|\n|--|--|\n|Model code||\n|Supported data types|Text, Image Video with audio|\n|Limits|1,024 tokens 1|\n|Latest update|September 2025|\n### Veo 3\n\n|Property|Description|\n|--|--|\n|Model code||\n|Supported data types|Text, Image Video with audio|\n|Limits|1,024 tokens 1|\n|Latest update|July 2025|\n### Veo 3 FastVeo 3 Fast allows developers to create videos with sound while maintaining high quality and optimizing for speed and business use cases. It's ideal for backend services that programmatically generate ads, tools for rapid A/B testing of creative concepts, or apps that need to quickly produce social media content.\n\n|Property|Description|\n|--|--|\n|Model code||\n|Supported data types|Text, Image Video with audio|\n|Limits|1,024 tokens 1|\n|Latest update|July 2025|\n### Veo 2\n\n|Property|Description|\n|--|--|\n|Model code||\n|Supported data types|Text, image Video|\n|Limits|N/A Any image resolution and aspect ratio up to 20MB file size Up to 2|\n|Latest update|April 2025|",
                "domain": "ai.google.dev"
              },
              {
                "position": 8,
                "title": "Google releases Veo 3.1, adds it to Flow video editor",
                "url": "https://techcrunch.com/2025/10/15/google-releases-veo-3-1-adds-it-to-flow-video-editor/",
                "snippet": "In Brief\nGoogle launched its new video model Veo 3.1 with improved audio output, granular editing controls, and better output for image to video. It said that Veo 3.1 builds on May's Veo 3 release and generates more realistic clips and adheres to prompts better.\nThe model allows users to add an object to the video and have it blend into the clip's style, Google said. Soon, users will be able to remove an existing object from the video in Flow, too.\nVeo 3 already has edit features such as adding reference images to drive a character, providing the first and last frame to generate a clip using AI, and the ability to extend an existing video based on the last few frames. With Veo 3.1, Google is adding audio to all these features to make the clips more lively.\nThe company is rolling out the model to its video editor Flow, the Gemini App, along with Vertex and Gemini APIs. It said that since Flow's launch in May, users have created more than 275 million videos on the app.",
                "domain": "techcrunch.com"
              },
              {
                "position": 9,
                "title": "Ultimate VEO 3.1 Update EXPLAINED: How To Use Google Veo-3 For Beginners",
                "url": "https://www.youtube.com/watch?v=FMdIIBByNZo",
                "snippet": "## AI Master\n##### Jul 09, 2025 (0:19:09)\n🚀 Become an AI Master – All-in-one AI Video Learning https://aimaster.me/pro\nLearn more about Magic Patterns here: https://magicpatterns.1stcollab.com/iamaimaster_3\n\nIn this video I ditch the press-release fluff and put Google’s brand-new Veo 3.1 through its paces, showing you exactly how to turn nothing but text prompts into crisp 8-second clips with native sound. You’ll watch me bounce between Gemini and the pro-grade Flow interface, learn my Prompt-Director Formula for nailing subject, action, context, motion, style, framing & audio, and see three crowd-pleasing demos in action. I’ll also flag the sneaky differences between Veo 3.1 Fast  and full Veo 3.1 Quality, share quick wins like borrowing Midjourney stills for style reference, and spotlight.\n\nChapters:\n0:00 – Why Veo 3 matters\n0:41 Flow vs Gemini\n2:11 Example #1\n4:23 The VEO3 Prompt Formula\n11:25 Example #2\n15:26 Example #3\n17:38 Wrapping up... {ts:0} Hey guys, I need a photo of AI master. Do you know who is AI master?\n{ts:5} Make Death Star great again. Okay. Oh, today I'm pulling back the curtain on\n{ts:10} Google's new VO3, the next gen AI video model that's got me acting like a kid in a tech toy store. We're talking text to\n{ts:18} video. You type a description and VO3 spits out an 8second video clip with sound. Yes, audio and video from just\n{ts:26} your words. [Music] In this tutorial, I'll show you how to\n{ts:31} harness VO3 to create funny viral style videos like this. It's just a few toasters, right? What's\n{ts:38} the worst that could happen? The big deal here is native audio. Unlike earlier models, VO3 doesn't just\n{ts:46} create silent clips. It also produces sound effects, background noise, even character dialogue to match. Each clip\n{ts:52} is limited to about 8 seconds for now, so we're talking short form content. They left behind a a ball today. It\n{ts:60} bounced higher than I can jump. What manner of magic is that? Now, Google gives us two main ways to... {ts:67} use V3. Flow and Gemini. Think of them as two different cameras that use the same film. Google Flow is like a pro\n{ts:75} filmmaking tool built around V3. It's a dedicated interface currently on Google apps where you can storyboard scenes,\n{ts:82} manage assets, tweak camera moves, the works. Flow was literally customd designed for VO. Lets you do fancy\n{ts:89} things. Upload or generate ingredients like character images via image genen and reuse them in multiple shots for\n{ts:96} consistency. Control camera angles and motion paths manually and even uses scene builder to extend the clips\n{ts:102} seamlessly into the next part of the story. It's basically an AI video studio for filmmakers. Now, Flow isn't open to\n{ts:109} everyone by default. It's available if you have a Google AI Pro or Ultra subscription in certain regions. Pro\n{ts:116} subscribers get most flow features, but only the ultra tier unlocks V3 fully. If you are on Pro, you have something\n{ts:123} called VO3 fast speed optimized version with sound slightly lower quality. I'll talk about that in a sec.\n{ts:132} All right, this idea cracks me up. A grandma skydiving into the Super Bowl. Talk about a stun granny. Let... {ts:276} I always try to include. Subject, who or what is in the scene, action, what the subject is doing. Context, setting,\n{ts:284} where and when is this happening. Motion, camera and movement, how the scene is shot and moves. Style, the\n{ts:291} visual style or genre. framing, composition, how the shot is framed, and constraints, extras, any additional\n{ts:298} details or instructions. And don't forget audio. VO3 will generate sound to match your scene, but only if you tell\n{ts:305} it what you want to hear. So, I often append an audio double colon section in the prompt describing sound effects,\n{ts:312} background noise, or dialogue lines. For example, audio double colon crowd cheering, upbeat rock music playing\n{ts:319} faintly, or character says hello world. You can specify dialogue explicitly, word for word, or implicitly, like a man\n{ts:327} introduces himself and let the AI make up the words. If you ask for dialogue, add no subtitles to your prompt.\n{ts:334} Otherwise, the model might plaster autogenerated subtitles on the video, which look well, not great. We... 'll see\n{ts:340} that in action soon. A boring prompt like a man answers a phone might yield a bland clip. But a richly detailed prompt\n{ts:348} can produce something cinematic. For instance, check this out. Basic prompt. A man answers a rotary phone.\n{ts:356} Hello. And detailed prompt. A shaky dolly zoom goes from a faraway\n{ts:364} blur to a close-up cinematic shot of a desperate man in a weathered green trench coat as he picks up a rotary\n{ts:370} phone mounted on a gritty brick wall. Bathed in the eerie glow of a green neon sign. The zoom reveals the tension on\n{ts:377} his face as he struggles to speak. shallow depth of field keeps focus on his furrowed brow and the phone, while\n{ts:384} the background is a blur of neon colors and shadows, creating a sense of urgency and isolation.\n{ts:392} I I have to Which one would you rather watch? The\n{ts:400} second prompt, nail subject, action, context, motion, that dolly zoom, style, cinematic, neon, noir, vibes, framing,\n{ts:408} close-up, ambience, eerie, neon, glow, all the elements. VO3 will follow the detailed prompt much more faithfully... {ts:416} than a vague one, yielding a dramatic video instead of a random guy on a phone. Bottom line, be specific and\n{ts:423} vivid. Describe what the camera sees and what the mic hears. Now, coming up with such detailed prompt can feel like\n{ts:429} flexing a new muscle. Here's where I cheat a little. I use Chad GBT to help refine prompt. Sometimes I'll start with\n{ts:436} a simple idea, say grandma's skydiving into stadium, and ask Chad GBT to describe the scene in a funny cinematic\n{ts:445} way with sights and sounds. The AI will usually output a nicely embellished description that I can copy or tweak for\n{ts:452} VO. Similarly, I use Smidjourney for visual prototyping. I'll feed a quick version of my idea to Midjourney to get\n{ts:459} a still image that helps me visualize the color palette or style I want. Maybe I like how it looks as a 1980s cartoon\n{ts:467} versus a gritty realistic photo and I can then describe that style in my VO prompt. This extra step isn't required,\n{ts:476} but if you... {ts:509} cheering. stadium announcer muffled wind rushing and the grandma's voice yelling yahoo in excitement no subtitles\n{ts:517} and that covers subject grandma action skydiving context super stadium motion wide follow shot slow-mo style cinematic\n{ts:527} bright realistic and audio crowd wind voice all in one prompt paragraph it's lengthy but should guide V3 perfectly a\n{ts:536} moment of truth let's generate this and see What happens? Yahoo!\n{ts:545} [Applause] Wow, look at that. She's really skydiving into the stadium. The video\n{ts:554} shows this tiny parachute coming down over a huge football field just like we asked. There's a wide shot of the\n{ts:561} stadium with fans standing up. I can see the crowd detail and yes, I hear the cheering roar. The grandma is visible\n{ts:567} hanging from the parachute. Maybe not super close on her face because we chose a wide shot, but enough to tell she's an\n{ts:574} elderly lady in gear. B3 as well as other generators are undoubtedly great, but they all can do\n{ts:582} one thing. Generate complete visual project. Sure, they can do few pictures and a few seconds of video, but what... 't resist add an explosion or two like a car blowing up\n{ts:998} behind them. Audio obviously loud audio double colin roaring T-Rex cowboy whooping car tires screeching an\n{ts:1008} explosion sound and dramatic action music. I will format it as one prompt and run it through Gemini. And here it\n{ts:1016} comes. We've got a T-Rex barreling down what looks like Time Square. A cowboy on his\n{ts:1029} back waving his head. And yes, there is an actual explosion behind them. Looks like a car got tossed. The fact that VO3",
                "domain": "www.youtube.com"
              },
              {
                "position": 10,
                "title": "Gemini AI video generator powered by Veo 3.1",
                "url": "https://gemini.google/overview/video-generation/",
                "snippet": "# Break the\n\n**silence** with Veo 3.1\n\nCreate high-quality, 8-second videos with Veo 3.1, our latest AI video generator. Simply describe what you have in mind or upload a photo and watch your ideas come to life with native audio generation. Try it with a Google AI Pro plan or get the highest access with the Ultra plan.\n\n**Veo 3.1** speaks for itself\n\n## Dream it. Describe it.\n\n**Done.**\n\n## For Exploring\n\nPlay with diverse styles, bring animated characters to life, and combine objects in ways you never thought possible. See what you can create using text to video with AI.\n\n## For Sharing\n\nCreate funny memes, turn inside jokes into videos, re-imagine special moments, and add a personal touch to make someone smile.\n\n## For Brainstorming\n\nBreak through creative blocks and visualize your ideas in a flash. From product concepts and designs to rapid prototyping and storytelling, Gemini can help.\n\n## Learn more about our\n\n**Veo Models**\n\nCreate videos with sound using our video generation model that maintains high-quality while optimizing for speed.\n\nCreate high-quality, 8-second videos with sound using our state-of-the-art video generation model.... ## Frequently asked questions\n\nYes, you can create and share videos in your mobile Gemini app. To create videos, tap the \"video\" button in your prompt bar. If you don't see it, tap the button with three dots to view more options.\n\nTry Veo 3.1 Fast with a Google AI Pro plan or get the highest access to Veo 3.1 in Google AI Ultra. Country availability here.\n\nFor now, the ability to generate a video from a photo is not available in the European Economic Area, Switzerland, or the United Kingdom.\n\nWe’ve taken several important safety steps to make AI video generation a safe experience. This includes extensive red teaming and evaluation aimed at preventing the generation of content that violates our policies. Additionally, all videos generated with Veo in the Gemini app are marked with a visible watermark and SynthID, a digital watermark embedded in each frame, which indicates the videos are AI-generated.\n\nGemini's outputs are primarily determined by user prompts and like any generative AI tool, there may be instances where it generates content that some individuals find objectionable. We’ll continue to listen to your feedback through the thumbs up/down buttons and make ongoing improvements. For more details, you can read about our approach on our website.\n\nResults for illustrative purposes and may vary. Internet and subscription for certain features required. Available to users 18+. Create responsibly.",
                "domain": "gemini.google"
              }
            ],
            "success": true,
            "error": null
          },
          {
            "query_id": "q5",
            "query": "veo3.1 Google software version 2025",
            "claim_id": "claim_1",
            "query_type": "direct_fact",
            "priority": "high",
            "results": [
              {
                "position": 1,
                "title": "Introducing Veo 3.1 and advanced capabilities in Flow - Google Blog",
                "url": "https://blog.google/technology/ai/veo-updates-flow/",
                "snippet": "# Introducing Veo 3.1 and advanced capabilities in Flow\n\nFive months ago, we introduced Flow, our AI filmmaking tool powered by Veo, and have been inspired by the creativity it has sparked with over 275 million videos generated in Flow\n\n1\n\n. We're always listening to your feedback, and we've heard that you want more artistic control within Flow, with increased support for audio across all features.\n\nToday, we’re introducing new and enhanced creative capabilities to edit your clips, giving you more granular control over your final scene. For the first time, we’re also bringing audio to existing capabilities like “Ingredients to Video,” “Frames to Video” and “Extend.”\n\nWe’re also introducing Veo 3.1, which brings richer audio, more narrative control, and enhanced realism that captures true-to-life textures. Veo 3.1 is state-of-the-art and builds on Veo 3, with stronger prompt adherence and improved audiovisual quality when turning images into videos.\n\n## Refine your narrative with audio and more control\n\nWith Veo 3.1, we’re bringing audio to existing capabilities to help you craft the perfect scene. These features are experimental and actively improving, and we’re excited to see what you create as we iterate based on your feedback.\n\nNow, with rich, generated audio, you can:\n\n**Craft the look of your scene.**With \"Ingredients to Video,\" you can use multiple reference images to control the characters, objects and style. Flow uses your ingredients to create a final scene that looks just as you envisioned. **Control the shot from start to finish.**Provide a starting and ending image with “Frames to Video,” and Flow will generate a seamless video that bridges the two, perfect for artful and epic transitions. **Create longer, seamless shots.**With \"Extend,\" you can create longer videos, even lasting for a minute or more, that connect to and continue the action from your original clip. Each video is generated based on the final second of your previous clip, making it most useful for creating a longer establishing shot.... ## Edit your ingredients and videos with more precision\n\nGreat ideas can strike at any point in the creative process. For moments when the first take isn't the final one, we're introducing new editing capabilities directly within Flow to help you reimagine and perfect your scenes.\n\n**Add new elements to any scene.**With “Insert,” introduce anything you can imagine, from realistic details to fantastical creatures. Flow now handles complex details like shadows and scene lighting, making the addition look natural. **Remove unwanted objects or characters seamlessly.**Soon, you’ll be able to take anything out of a scene, and Flow will reconstruct the background and surroundings, making it look as though the object was never there.\n\n## Start creating in Flow today\n\nWith more precise editing capabilities, audio across all existing features and higher-quality outputs powered by Veo 3.1, we're opening up new possibilities for richer, more powerful video storytelling right inside Flow.\n\nThe Veo 3.1 model is also available via the Gemini API for developers, Vertex AI for enterprise customers, and the Gemini app. New capabilities are available in both Gemini API\n\n2\n\nand Vertex AI\n\n3",
                "domain": "blog.google"
              },
              {
                "position": 2,
                "title": "Build with Veo 3, now available in the Gemini API",
                "url": "https://developers.googleblog.com/en/veo-3-now-available-gemini-api/",
                "snippet": "First unveiled at Google I/O 2025, people around the world have already generated tens of millions of high-quality videos with Veo 3 (along with some new fun and interesting video trends). It is our first video model to incorporate high-fidelity video outputs and native audio, first with text-to-video and soon with image-to-video.\n\nDevelopers are already experimenting with Veo 3, discovering how the model can help them brainstorm content, rapidly iterate, and be more efficient.\n\nVeo 3 is designed to handle a range of video generation tasks, from cinematic narratives to dynamic character animations. With Veo 3, you can create more immersive experiences by not only generating stunning visuals, but also audio like dialogue and sound effects.\n\nLet’s take a look at some examples.\n\nExplore these examples and more with Veo 3 in Google AI Studio, available as an SDK template and interactive Starter App to remix, copy and extend. The Starter App and its sample code offer a convenient way for Paid Tier users to rapidly prototype with Veo 3 and more on the Gemini API, directly from Google AI Studio.\n\nClick the Key button in the top right of the AI Studio Build interface to select a Google Cloud Project with billing enabled to use the Paid Tier in AI Studio apps. See the FAQs for more.... Veo 3 will be priced at $0.75 per second for video and audio output. Additionally, Veo 3 Fast will be available soon, offering a faster and more cost-effective option for video creation.\n\nHere’s a basic Python example to create a video:\n\n```\n\nimport time\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.0-generate-preview\",\n\nprompt=\"a close-up shot of a golden retriever playing in a field of sunflowers\",\n\nconfig=types.GenerateVideosConfig(\n\nnegative_prompt=\"barking, woofing\",\n\n),\n\n\n\n# Waiting for the video(s) to be generated\n\nwhile not operation.done:\n\ntime.sleep(20)\n\noperation = client.operations.get(operation)\n\ngenerated_video = operation.result.generated_videos[0]\n\nclient.files.download(file=generated_video.video)\n\ngenerated_video.video.save(\"veo3_video.mp4\")\n\n```\n\nAll videos generated by Veo 3 models will continue to include a digital SynthID watermark. To get started, check out the documentation, cookbook, and a Veo 3 starter app in Google AI Studio:\n\nIn addition to being available via the Gemini API in Google AI Studio, Veo 3 is also available to Google AI subscribers in the Gemini app and Flow, and to enterprise customers via Vertex AI.... 1: Fluffy Characters Stop Motion: Inside a brightly colored, cozy kitchen made of felt and yarn. Professor Nibbles, a plump, fluffy hamster with oversized glasses, nervously stirs a bubbling pot on a miniature stove, muttering, \"Just a little more... 'essence of savory,' as the recipe calls for.\" The camera is a mid-shot, capturing his frantic stirring. Suddenly, the pot emits a loud \"POP!\" followed by a comical \"whoosh\" sound, and a geyser of iridescent green slime erupts, covering the entire kitchen. Professor Nibbles shrieks, \"Oh, dear! Not again!\" and scurries away, leaving a trail of tiny, panicked squeaks.\n\n2: The sequence begins with an extreme close-up of a single gear, slowly turning and reflecting harsh sunlight. The camera gradually pulls back in a continuous movement, revealing this is but one component of a colossal, mechanical heart half-buried in a desolate, rust-colored desert. A sweeping aerial shot establishes its enormous scale and isolation in the barren landscape. The camera descends to capture pipes hissing steam and the rhythmic thumping that echoes across the empty plains. A subtle shake effect synchronizes with each massive heartbeat. A lateral tracking shot discovers tiny, robed figures scurrying across the metallic surface. The camera follows one such figure in a detailed tracking shot as they perform meticulous maintenance, polishing brass valves and tightening immense bolts. A complex movement circles the entire structure, capturing different maintenance teams working in precarious positions across its rusted exterior. The final shot begins tight on the meticulous work of one tiny figure before executing a dramatic pull-out that reveals the true scale of the heart and the minuscule size of its caretakers, tending to the vital organ of an unseen, sleeping giant that extends beyond the frame.",
                "domain": "developers.googleblog.com"
              },
              {
                "position": 3,
                "title": "Google announces Veo 3.1 and updates Flow with more controls, tools",
                "url": "https://9to5google.com/2025/10/15/veo-3-1/",
                "snippet": "Google today announced Veo 3.1 as its latest video generation model, with Flow getting a number of updates to take advantage of the latest capabilities.\n\nCompared to Veo 3, which was announced at I/O 2025 in May, this new version offers richer audio and “enhanced realism that captures true-to-life textures.” Veo 3.1 has a deeper understanding of storytelling, cinematic styles, and character interactions to give you more narrative control. The image-to-video capability benefits from improved audio-visual quality and better follows your prompt.\n\nVeo 3.1 and Veo 3.1 Fast are available in the Gemini app, as well as the Gemini API and Vertex AI, to power text-to-video and image-to-video for horizontal (16×9) and vertical (9×16) outputs.\n\nMeanwhile, Google is updating the Flow filmmaking tool to take advantage of Veo 3.1. Audio generation is coming to:\n\n**Ingredients to Video**: This can include images of multiple characters, objects, and styles **Scene extension**: You can Extend an existing clip, with Google basing it on the last second **Frames to Video**: This lets you specify the starting and ending images, with Flow bridging the two\n\nFlow is getting new editing capabilities. You can insert elements like objects, characters, and details, with Google handling shadows, scene lighting, and other complex details to make everything look natural.\n\nComing soon is the ability to remove objects and characters from a scene, with Flow working to reconstruct the background and surroundings to make the edit seamless.\n\n\n\n## Comments",
                "domain": "9to5google.com"
              },
              {
                "position": 4,
                "title": "Introducing Veo 3.1 and new creative capabilities in the Gemini API",
                "url": "https://developers.googleblog.com/en/introducing-veo-3-1-and-new-creative-capabilities-in-the-gemini-api/",
                "snippet": "**Today, we are releasing Veo 3.1 and Veo 3.1 Fast in paid preview in the** **Gemini API** **.** This updated model offers several improvements, as well as improved outputs when generating video from images. These new models are available via the Gemini API in Google AI Studio and Vertex AI. Veo 3.1 is also available in the Gemini app and Flow.\n\nVeo 3.1 and Veo 3.1 Fast empower developers to create more engaging content through significant upgrades. The models now generate\n\n**richer native audio**, from natural conversations to synchronized sound effects, and offer **greater narrative control** with an improved understanding of cinematic styles. **Enhanced image-to-video** capabilities ensure better prompt adherence while delivering superior audio and visual quality and maintaining character consistency across multiple scenes.\n\nIn addition to the model update, we are introducing new model capabilities, including using reference images to guide video generation, extend existing Veo videos, and generate transitions between a first and last frame.\n\nYou can now guide the generation process by providing up to 3 reference images of a character, object, or scene. This is helpful for maintaining character consistency across multiple shots or applying a specific style to your video.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nconfig=types.GenerateVideosConfig(\n\nreference_images=[reference_image1, reference_image2],\n\n),\n\n\n\n```... Your story is no longer limited by the original generation. With Scene extension, you can create longer videos, even lasting for a minute or more, by generating new clips that connect to your previous video. Each new video is generated based on the final second of the previous clip. This maintains visual continuity, making the feature ideal for extending shots with background audio.\n\n```\n\nfrom google import genai\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nvideo=video_to_extend\n\n\n\n```\n\nCreate smooth, natural scenes that bridge two different images. By providing a starting and an ending image, you can direct Veo 3.1 to generate the transition between them, complete with accompanying audio.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nimage=first_frame,\n\nconfig=types.GenerateVideosConfig(\n\nlast_frame=last_frame,\n\n),\n\n\n\n```\n\n**What people are building with Veo 3.1**\n\nPromise Studios, a GenAI movie studio, uses Veo 3.1 within its MUSE Platform to enhance generative storyboarding and previsualization for director-driven storytelling at production quality.... Latitude is experimenting with Veo 3.1 in its generative narrative engine to instantly bring user-created stories to life.\n\nVeo 3.1 and these new capabilities are now available in paid preview via the Gemini API.\n\nVeo 3.1 is the same price as Veo 3. We are incredibly excited to see what you will build with these new capabilities!",
                "domain": "developers.googleblog.com"
              },
              {
                "position": 5,
                "title": "Google's Veo 3.1 is better at generating videos from images",
                "url": "https://www.engadget.com/ai/googles-veo-31-is-better-at-generating-videos-from-images-220829129.html",
                "snippet": "Google has released a new update to its Veo AI video generation model that should make it do a better job of sticking to prompts and converting images into videos. Veo 3.1 is available to try today through Google's Gemini API and is now also powering the company's Flow video editor.\nVeo 3.1 builds on the new capabilities Google introduced with launch of Veo 3 at Google I/O 2025. The new model offers better \"prompt adherence,\" according to Google, and should have an easier time creating videos based on the image \"ingredients\" you upload alongside your written prompt. Veo 3.1 also makes it possible to convert images to video and generate audio at the same time, a capability that wasn't available with Veo 3.\nIn Flow, Veo 3.1 supports at least a new feature that gives you finer control over the videos you generate. With what Google calls \"Frame to Video,\" Flow lets you upload a first and last frame, and then generates the video in-between. Adobe Firefly, which is powered by Veo 3, offers a similar feature, but Flow will be able to pull it off and create audio at the same time. Those added audio skills will also apply to the video editor's ability to extend clips and insert objects into existing footage, too.\nBased on the samples Google's shared, videos generated with Veo 3.1 still have an uncanny quality that seems to vary greatly depending on the prompt and subject. Even if it's missing some of the realism of OpenAI's Sora 2, though, the company's decision to try and make Veo more useful to people who actually work with video rather than a source of social media spam is a welcome move.",
                "domain": "www.engadget.com"
              },
              {
                "position": 6,
                "title": "Release notes | Gemini API | Google AI for Developers",
                "url": "https://ai.google.dev/gemini-api/docs/changelog",
                "snippet": "This page documents updates to the Gemini API.\n\n## October 17, 2025\n\n**Grounding with Google Maps**is now generally available. For more information, see Grounding with Google Maps documentation.\n\n## October 15, 2025\n\nReleased Veo 3.1 and 3.1 Fast models in public preview, with new features including:\n\n- Extending Veo-created videos.\n\n- Referencing up to three images to generate a video.\n\n- Providing first and last frame images to generate videos from.\n\nThis launch also added more options for Veo 3 output video durations: 4, 6, and 8 seconds.\n\nDeprecation for\n\n`veo-3.0-generate-preview`and\n\n`veo-3.0-fast-generate-preview`coming October 22, 2025.\n\n## October 7, 2025\n\n- Launched Gemini 2.5 Computer Use Preview\n\n## October 2, 2025\n\n- Launched Gemini 2.5 Flash Image GA: Image Generation with Gemini\n\n## September 29, 2025\n\n- The following Gemini 1.5 models are now deprecated:\n\n`gemini-1.5-pro`\n\n`gemini-1.5-flash-8b`\n\n`gemini-1.5-flash`... ## September 9, 2025\n\n- Launched Veo 3 and Veo 3 Fast GA, with lower pricing and new options for aspect ratios, resolution, and seeding. Read the Veo documentation for more information.\n\n## August 26, 2025\n\n- Launched Gemini 2.5 Image Preview, our latest native image generation model.\n\n## August 18, 2025\n\n- Released URL context tool to general\n\navailability (GA), a tool for providing URLs as additional context to\n\nprompts. Support for using URL context with the\n\n`gemini-2.0-flash`model (available during experimental release) will be discontinued in one week.\n\n## August 14, 2025\n\n- Released Imagen 4 Ultra, Standard and Fast models as generally available (GA). To learn more, see the Imagen page.\n\n## August 7, 2025\n\n`allow_adult`setting in Image to Video generation are now available in restricted regions. See the Veo page for details.\n\n## July 31, 2025\n\n- Launched image-to-video generation for the Veo 3 Preview model.\n\n- Released Veo 3 Fast Preview model.\n\n- To learn more about Veo 3, visit the Veo page.... ## July 22, 2025\n\n- Released\n\n`gemini-2.5-flash-lite`, our fast, low-cost, high-performance Gemini 2.5 model. To learn more, see Gemini 2.5 Flash-Lite.\n\n## July 17, 2025\n\nLaunched\n\n`veo-3.0-generate-preview`, the latest update to Veo introducing video with audio generation. To learn more about Veo 3, visit the Veo page.\n\nIncreased rate limits for Imagen 4 Standard and Ultra. Visit the Rate limits page for more details.\n\n## July 14, 2025\n\n- Released\n\n`gemini-embedding-001`, the stable version of our text embedding model. To learn more, see embeddings. The\n\n`gemini-embedding-exp-03-07`model will be deprecated on August 14, 2025.\n\n## July 7, 2025\n\n- Launched Gemini API Batch Mode. Batch up requests and send them to process asynchronously. To learn more, see Batch Mode.\n\n## June 26, 2025\n\nThe preview models\n\n`gemini-2.5-pro-preview-05-06`and\n\n`gemini-2.5-pro-preview-03-25`are now redirecting to the latest stable version\n\n`gemini-2.5-pro`.\n\n`gemini-2.5-pro-exp-03-25`is deprecated.... ## May 7, 2025\n\n- Released\n\n`gemini-2.0-flash-preview-image-generation`, a preview model for generating and editing images. To learn more, see Image generation and Gemini 2.0 Flash Preview Image Generation.\n\n## May 6, 2025\n\n- Released\n\n`gemini-2.5-pro-preview-05-06`, a new version of our most powerful model, with improvements on code and function calling.\n\n`gemini-2.5-pro-preview-03-25`will automatically point to the new version of the model.\n\n## April 17, 2025\n\n- Released\n\n`gemini-2.5-flash-preview-04-17`, a Gemini preview model optimized for price-performance and adaptive thinking. To learn more, see Gemini 2.5 Flash Preview and Thinking.\n\n## April 16, 2025\n\n- Launched context caching for Gemini 2.0 Flash.... ## April 9, 2025\n\n**Model updates:**\n\n- Released\n\n`veo-2.0-generate-001`, a generally available (GA) text- and image-to-video model, capable of generating detailed and artistically nuanced videos. To learn more, see the Veo docs.\n\nReleased\n\n`gemini-2.0-flash-live-001`, a public preview version of the Live API model with billing enabled.\n\n**Enhanced Session Management and Reliability** **Session Resumption:**Keep sessions alive across temporary network disruptions. The API now supports server-side session state storage (for up to 24 hours) and provides handles (session_resumption) to reconnect and resume where you left off. **Longer Sessions via Context Compression:**Enable extended interactions beyond previous time limits. Configure context window compression with a sliding window mechanism to automatically manage context length, preventing abrupt terminations due to context limits. **Graceful Disconnect Notification:**Receive a\n\n`GoAway`server message indicating when a connection is about to close, allowing for graceful handling before termination.\n\n\n\n**More Control over Interaction Dynamics** **Configurable Voice Activity Detection (VAD):**Choose sensitivity levels or disable automatic VAD entirely and use new client events (\n\n`activityStart`,\n\n`activityEnd`) for manual turn control.\n\n**Configurable Interruption Handling:**Decide whether user input should interrupt the model's response. **Configurable Turn Coverage:**Choose whether the API processes all audio and video input continuously or only captures it when the end-user is detected speaking. **Configurable Media Resolution:**Optimize for quality or token usage by selecting the resolution for input media. **Richer Output and Features** **Expanded Voice & Language Options:**Choose from two new voices and 30 new languages for audio output. The output language is now configurable within\n\n`speechConfig`.\n\n**Text Streaming:**Receive text responses incrementally as they are generated, enabling faster display to the user. **Token Usage Reporting:**Gain insights into usage with detailed token counts provided in the\n\n`usageMetadata`field of server messages, broken down by modality and prompt or response phases.... ## April 4, 2025\n\n- Released\n\n`gemini-2.5-pro-preview-03-25`, a public preview Gemini 2.5 Pro version with billing enabled. You can continue to use\n\n`gemini-2.5-pro-exp-03-25`on the free tier.\n\n## March 25, 2025\n\n- Released\n\n`gemini-2.5-pro-exp-03-25`, a public experimental Gemini model with thinking mode always on by default. To learn more, see Gemini 2.5 Pro Experimental.\n\n## March 12, 2025\n\n**Model updates:**\n\n- Launched an experimental Gemini 2.0 Flash model capable of image generation and editing.\n\n- Released\n\n`gemma-3-27b-it`, available on AI Studio and through the Gemini API, as part of the Gemma 3 launch.\n\n**API updates:**\n\n- Added support for YouTube URLs as a media source.\n\n- Added support for including an inline video of less than 20MB.\n\n## March 11, 2025\n\n**SDK updates:**\n\n- Released the Google Gen AI SDK for TypeScript and JavaScript to public preview.\n\n## March 7, 2025\n\n**Model updates:**\n\n- Released\n\n`gemini-embedding-exp-03-07`, an experimental Gemini-based embeddings model in public preview.... ## February 28, 2025\n\n**API updates:**\n\n- Support for Search as a tool\n\nadded to\n\n`gemini-2.0-pro-exp-02-05`, an experimental model based on Gemini 2.0 Pro.\n\n## February 25, 2025\n\n**Model updates:**\n\n- Released\n\n`gemini-2.0-flash-lite`, a generally available (GA) version of Gemini 2.0 Flash-Lite, which is optimized for speed, scale, and cost efficiency.\n\n## February 19, 2025\n\n**AI Studio updates:**\n\n- Support for additional regions (Kosovo, Greenland and Faroe Islands).\n\n**API updates:**\n\n- Support for additional regions (Kosovo, Greenland and Faroe Islands).\n\n## February 18, 2025\n\n**Model updates:**\n\n- Gemini 1.0 Pro is no longer supported. For the list of supported models, see Gemini models.\n\n## February 11, 2025\n\n**API updates:**\n\n- Updates on the OpenAI libraries compatibility.\n\n## February 6, 2025\n\n**Model updates:**\n\n- Released\n\n`imagen-3.0-generate-002`, a generally available (GA) version of Imagen 3 in the Gemini API.\n\n**SDK updates:**\n\n- Released the Google Gen AI SDK for Java for public preview.",
                "domain": "ai.google.dev"
              },
              {
                "position": 7,
                "title": "Google releases Veo 3.1, adds it to Flow video editor",
                "url": "https://techcrunch.com/2025/10/15/google-releases-veo-3-1-adds-it-to-flow-video-editor/",
                "snippet": "In Brief\nGoogle launched its new video model Veo 3.1 with improved audio output, granular editing controls, and better output for image to video. It said that Veo 3.1 builds on May's Veo 3 release and generates more realistic clips and adheres to prompts better.\nThe model allows users to add an object to the video and have it blend into the clip's style, Google said. Soon, users will be able to remove an existing object from the video in Flow, too.\nVeo 3 already has edit features such as adding reference images to drive a character, providing the first and last frame to generate a clip using AI, and the ability to extend an existing video based on the last few frames. With Veo 3.1, Google is adding audio to all these features to make the clips more lively.\nThe company is rolling out the model to its video editor Flow, the Gemini App, along with Vertex and Gemini APIs. It said that since Flow's launch in May, users have created more than 275 million videos on the app.",
                "domain": "techcrunch.com"
              },
              {
                "position": 8,
                "title": "Meet Flow: AI-powered filmmaking with Veo 3",
                "url": "https://blog.google/technology/ai/google-flow-veo-ai-filmmaking-tool/",
                "snippet": "# Meet Flow: AI-powered filmmaking with Veo 3\n\nToday we’re introducing Flow, our new AI filmmaking tool.\n\nIt’s built by and for creatives, and it’s the only AI filmmaking tool custom-designed for Google’s most advanced models — Veo, Imagen and Gemini. Flow can help storytellers explore their ideas without bounds and create cinematic clips and scenes for their stories. It’s early days, and we’re excited to shape the future of Flow with creatives and filmmakers.\n\n### What’s possible with Flow\n\nFlow is inspired by what it feels like when time slows down and creation is effortless, iterative and full of possibility. It’s custom-designed for Veo, Google’s state-of-the-art generative video model, with exceptional prompt adherence and stunning cinematic outputs that excel at physics and realism. Behind the scenes, Gemini models make prompting intuitive, so you can describe your vision in everyday language. You can bring your own assets to create characters, or use Flow to make your own ingredients with Imagen’s text-to-image capabilities.\n\nOnce you’ve created a subject or a scene, you can integrate those same ingredients into different clips and scenes with consistency. Or you can use a scene image to start a new shot.\n\nCreate your ingredients\n\nUse those ingredients to create a clip\n\nReference ingredients in plain language... ### Key features to unlock your storytelling\n\nFlow also comes with a range of features for professionals or those just getting started:\n\n**Camera Controls:**Master your shot with direct control over camera motion, angles and perspectives. **Scenebuilder:**Seamlessly edit and extend your existing shots — revealing more of the action or transitioning to what happens next with continuous motion and consistent characters. **Asset Management:**Easily manage and organize all of your ingredients and prompts. **Flow TV** **:**Spark your creativity with an ever-growing showcase of clips, channels, and content generated with Veo. You can see the exact prompts and techniques used for clips you like, providing a practical way to learn and adapt new styles.\n\nSeamless transitions\n\nCamera controls\n\nCinematic quality\n\n### Get started with Flow\n\nFlow is the evolution of VideoFX, a Google Labs experiment that launched last year. Starting today, Flow is available to subscribers of our Google AI Pro and Google AI Ultra plans in the U.S., with more countries coming soon.\n\nGoogle AI Pro gives you the key Flow features and 100 generations per month, and Google AI Ultra gives you the highest usage limits and early access to Veo 3 with native audio generation, bringing environmental sounds and character dialogue directly into video creation.... ### How we’re collaborating with filmmakers\n\nAs with any groundbreaking technology, we’re still understanding the full potential of AI in filmmaking. We see the emergence of these tools as an enabler, helping a new wave of filmmakers more easily tell their stories. By offering filmmakers early access to Flow, we were able to better understand how our technology could best support and integrate into their creative workflows — and we’ve woven their insights into Flow. Here are some filmmakers we partnered with and the short films they developed using Flow along with other tools and techniques.\n\n**Dave Clark**\n\nDave is an award-winning filmmaker focused on embracing new technology as part of his filmmaking. He used AI to develop two of his most recent short films, “Battalion” and “NinjaPunk.” His newest short film “Freelancers” uses Google’s AI and other tools to tell the story of two estranged adopted brothers on similar quests.\n\n**Henry Daubrez**\n\nHenry has been using tech tools in his art for the last 18 years. Earlier this year he unveiled “Kitsune” using Veo 2 — a moving short film about “love between two souls separated by everything except their shared feelings of loneliness.” Now, Henry is bringing the story of his own creative journey to life in “Electric Pink.”\n\n**Junie Lau**\n\nJunie Lau is a film director and multidisciplinary creative deeply passionate about innovation, viewing AI as a vital collaborator in expanding the boundaries of creative expression. Her work delves into artistic narratives within the hyper-modern era, including themes of virtual identity, digital humanities and digital ontology. Currently, Junie is working on a film titled “Dear Stranger,” which explores the boundless and infinite nature of universal love between a grandmother and grandchild across countless parallel worlds.\n\nAI is ushering in a new chapter of creativity and filmmaking, and while it’s still early, we see so much potential for tools like Flow to unlock new voices and creations.\n\nFor more insights on Flow and how AI helps storytellers take more risks in the creative storytelling process, watch Dave Clark, Junie Lau, and Henry Daubrez in \"Behind the Lens: AI, Creativity, and the Future of Filmmaking Tools.\"",
                "domain": "blog.google"
              },
              {
                "position": 9,
                "title": "Gemini AI video generator powered by Veo 3.1",
                "url": "https://gemini.google/overview/video-generation/",
                "snippet": "# Break the\n\n**silence** with Veo 3.1\n\nCreate high-quality, 8-second videos with Veo 3.1, our latest AI video generator. Simply describe what you have in mind or upload a photo and watch your ideas come to life with native audio generation. Try it with a Google AI Pro plan or get the highest access with the Ultra plan.\n\n**Veo 3.1** speaks for itself\n\n## Dream it. Describe it.\n\n**Done.**\n\n## For Exploring\n\nPlay with diverse styles, bring animated characters to life, and combine objects in ways you never thought possible. See what you can create using text to video with AI.\n\n## For Sharing\n\nCreate funny memes, turn inside jokes into videos, re-imagine special moments, and add a personal touch to make someone smile.\n\n## For Brainstorming\n\nBreak through creative blocks and visualize your ideas in a flash. From product concepts and designs to rapid prototyping and storytelling, Gemini can help.\n\n## Learn more about our\n\n**Veo Models**\n\nCreate videos with sound using our video generation model that maintains high-quality while optimizing for speed.\n\nCreate high-quality, 8-second videos with sound using our state-of-the-art video generation model.... ## Frequently asked questions\n\nYes, you can create and share videos in your mobile Gemini app. To create videos, tap the \"video\" button in your prompt bar. If you don't see it, tap the button with three dots to view more options.\n\nTry Veo 3.1 Fast with a Google AI Pro plan or get the highest access to Veo 3.1 in Google AI Ultra. Country availability here.\n\nFor now, the ability to generate a video from a photo is not available in the European Economic Area, Switzerland, or the United Kingdom.\n\nWe’ve taken several important safety steps to make AI video generation a safe experience. This includes extensive red teaming and evaluation aimed at preventing the generation of content that violates our policies. Additionally, all videos generated with Veo in the Gemini app are marked with a visible watermark and SynthID, a digital watermark embedded in each frame, which indicates the videos are AI-generated.\n\nGemini's outputs are primarily determined by user prompts and like any generative AI tool, there may be instances where it generates content that some individuals find objectionable. We’ll continue to listen to your feedback through the thumbs up/down buttons and make ongoing improvements. For more details, you can read about our approach on our website.\n\nResults for illustrative purposes and may vary. Internet and subscription for certain features required. Available to users 18+. Create responsibly.",
                "domain": "gemini.google"
              },
              {
                "position": 10,
                "title": "Unpacking the magic of our new creative tools - YouTube Blog",
                "url": "https://blog.youtube/news-and-events/generative-ai-creation-tools-made-on-youtube-2025/",
                "snippet": "# Unpacking the magic of our new creative tools\n\nSep 16, 2025 [[read-time]] minute read\n\nSep 16, 2025 [[read-time]] minute read\n\nToday at Made on YouTube, we unveiled a suite of features designed to make creativity on YouTube more playful and effortless than ever before. We’re taking a deeper look at what you can expect when you try them out yourself.\n\nWe’ve partnered with Google DeepMind to bring a custom version of their most powerful video generation model, Veo 3, to YouTube. Veo 3 Fast is designed to work seamlessly in YouTube Shorts for millions of creators and users, for free. It generates outputs with lower latency at 480p so you can easily create video clips – and for the first time, with sound – from any idea, all from your phone.\n\nTap the create button, then the sparkle icon in the top right corner to find our latest gen AI creation tools including Veo 3.\n\nIt’s rolling out now in the United States, United Kingdom, Canada, Australia, and New Zealand, with plans to expand to more of you soon.\n\nWe're also introducing new Veo capabilities on Shorts that put your imagination in the spotlight. Soon, you’ll be able to:... We’ll start to experiment with all of these new capabilities in the coming months on Shorts.\n\nStarting a video from a blank timeline can be daunting. That’s why we’re introducing Edit with AI, a new feature that takes the initial heavy lifting off your shoulders. It transforms your raw camera roll footage into a compelling first draft, intelligently finding and arranging your best moments, adding music, transitions, and even a playful voiceover that can react to what’s happening in the video, in either English or Hindi. This gives you a solid starting point so you can jump straight to the fun part: personalizing your video and bringing your unique vision to life.\n\nWe’re experimenting now with Edit with AI on Shorts and in the YouTube Create app, and will expand the feature in the coming weeks in select markets.\n\nAs the world's largest creative playground, YouTube is where trends are born and where you can draw inspiration from. Imagine hearing a line of dialogue that sparks an idea—a funny phrase, a memorable quote, or a one-of-a-kind sound—and you want to remix it into a new sound. With our new Speech to Song remixing tool, you'll be able to do just that, quickly turning the dialogue from eligible videos into catchy soundtracks for your next Short.\n\nCheck it out for yourself. Listen to the original video, and how Speech to song helped transform it using Lyria 2, Google DeepMind's most advanced AI music model.... Now your GRWM videos take on a whole new level of fun!\n\nBehind the scenes, this uses the dialogue from the original video with Lyria 2 to help create the song. And you're able to add your own vibe for the song, like, chill, danceable, or fun. The final result attributes the original creator.\n\nAs always and across these features, we use SynthID watermarks and content labels to indicate that these creations were generated with AI.\n\nWe hope these features foster a new era of playful experimentation on YouTube!",
                "domain": "blog.youtube"
              }
            ],
            "success": true,
            "error": null
          },
          {
            "query_id": "q7",
            "query": "veo3.1 Google software version 2025 official data",
            "claim_id": "claim_1",
            "query_type": "statistical",
            "priority": "high",
            "results": [],
            "success": false,
            "error": "Rate limit exceeded. Please try again later."
          },
          {
            "query_id": "q6",
            "query": "Google veo3.1 release statement 2025",
            "claim_id": "claim_1",
            "query_type": "source_verification",
            "priority": "high",
            "results": [
              {
                "position": 1,
                "title": "Introducing Veo 3.1 and advanced capabilities in Flow - Google Blog",
                "url": "https://blog.google/technology/ai/veo-updates-flow/",
                "snippet": "# Introducing Veo 3.1 and advanced capabilities in Flow\n\nFive months ago, we introduced Flow, our AI filmmaking tool powered by Veo, and have been inspired by the creativity it has sparked with over 275 million videos generated in Flow\n\n1\n\n. We're always listening to your feedback, and we've heard that you want more artistic control within Flow, with increased support for audio across all features.\n\nToday, we’re introducing new and enhanced creative capabilities to edit your clips, giving you more granular control over your final scene. For the first time, we’re also bringing audio to existing capabilities like “Ingredients to Video,” “Frames to Video” and “Extend.”\n\nWe’re also introducing Veo 3.1, which brings richer audio, more narrative control, and enhanced realism that captures true-to-life textures. Veo 3.1 is state-of-the-art and builds on Veo 3, with stronger prompt adherence and improved audiovisual quality when turning images into videos.\n\n## Refine your narrative with audio and more control\n\nWith Veo 3.1, we’re bringing audio to existing capabilities to help you craft the perfect scene. These features are experimental and actively improving, and we’re excited to see what you create as we iterate based on your feedback.\n\nNow, with rich, generated audio, you can:\n\n**Craft the look of your scene.**With \"Ingredients to Video,\" you can use multiple reference images to control the characters, objects and style. Flow uses your ingredients to create a final scene that looks just as you envisioned. **Control the shot from start to finish.**Provide a starting and ending image with “Frames to Video,” and Flow will generate a seamless video that bridges the two, perfect for artful and epic transitions. **Create longer, seamless shots.**With \"Extend,\" you can create longer videos, even lasting for a minute or more, that connect to and continue the action from your original clip. Each video is generated based on the final second of your previous clip, making it most useful for creating a longer establishing shot.... ## Edit your ingredients and videos with more precision\n\nGreat ideas can strike at any point in the creative process. For moments when the first take isn't the final one, we're introducing new editing capabilities directly within Flow to help you reimagine and perfect your scenes.\n\n**Add new elements to any scene.**With “Insert,” introduce anything you can imagine, from realistic details to fantastical creatures. Flow now handles complex details like shadows and scene lighting, making the addition look natural. **Remove unwanted objects or characters seamlessly.**Soon, you’ll be able to take anything out of a scene, and Flow will reconstruct the background and surroundings, making it look as though the object was never there.\n\n## Start creating in Flow today\n\nWith more precise editing capabilities, audio across all existing features and higher-quality outputs powered by Veo 3.1, we're opening up new possibilities for richer, more powerful video storytelling right inside Flow.\n\nThe Veo 3.1 model is also available via the Gemini API for developers, Vertex AI for enterprise customers, and the Gemini app. New capabilities are available in both Gemini API\n\n2\n\nand Vertex AI\n\n3",
                "domain": "blog.google"
              },
              {
                "position": 2,
                "title": "Google announces Veo 3.1 and updates Flow with more controls, tools",
                "url": "https://9to5google.com/2025/10/15/veo-3-1/",
                "snippet": "Google today announced Veo 3.1 as its latest video generation model, with Flow getting a number of updates to take advantage of the latest capabilities.\n\nCompared to Veo 3, which was announced at I/O 2025 in May, this new version offers richer audio and “enhanced realism that captures true-to-life textures.” Veo 3.1 has a deeper understanding of storytelling, cinematic styles, and character interactions to give you more narrative control. The image-to-video capability benefits from improved audio-visual quality and better follows your prompt.\n\nVeo 3.1 and Veo 3.1 Fast are available in the Gemini app, as well as the Gemini API and Vertex AI, to power text-to-video and image-to-video for horizontal (16×9) and vertical (9×16) outputs.\n\nMeanwhile, Google is updating the Flow filmmaking tool to take advantage of Veo 3.1. Audio generation is coming to:\n\n**Ingredients to Video**: This can include images of multiple characters, objects, and styles **Scene extension**: You can Extend an existing clip, with Google basing it on the last second **Frames to Video**: This lets you specify the starting and ending images, with Flow bridging the two\n\nFlow is getting new editing capabilities. You can insert elements like objects, characters, and details, with Google handling shadows, scene lighting, and other complex details to make everything look natural.\n\nComing soon is the ability to remove objects and characters from a scene, with Flow working to reconstruct the background and surroundings to make the edit seamless.\n\n\n\n## Comments",
                "domain": "9to5google.com"
              },
              {
                "position": 3,
                "title": "Google releases Veo 3.1, adds it to Flow video editor",
                "url": "https://techcrunch.com/2025/10/15/google-releases-veo-3-1-adds-it-to-flow-video-editor/",
                "snippet": "In Brief\nGoogle launched its new video model Veo 3.1 with improved audio output, granular editing controls, and better output for image to video. It said that Veo 3.1 builds on May's Veo 3 release and generates more realistic clips and adheres to prompts better.\nThe model allows users to add an object to the video and have it blend into the clip's style, Google said. Soon, users will be able to remove an existing object from the video in Flow, too.\nVeo 3 already has edit features such as adding reference images to drive a character, providing the first and last frame to generate a clip using AI, and the ability to extend an existing video based on the last few frames. With Veo 3.1, Google is adding audio to all these features to make the clips more lively.\nThe company is rolling out the model to its video editor Flow, the Gemini App, along with Vertex and Gemini APIs. It said that since Flow's launch in May, users have created more than 275 million videos on the app.",
                "domain": "techcrunch.com"
              },
              {
                "position": 4,
                "title": "Introducing Veo 3.1 and new creative capabilities in the Gemini API",
                "url": "https://developers.googleblog.com/en/introducing-veo-3-1-and-new-creative-capabilities-in-the-gemini-api/",
                "snippet": "**Today, we are releasing Veo 3.1 and Veo 3.1 Fast in paid preview in the** **Gemini API** **.** This updated model offers several improvements, as well as improved outputs when generating video from images. These new models are available via the Gemini API in Google AI Studio and Vertex AI. Veo 3.1 is also available in the Gemini app and Flow.\n\nVeo 3.1 and Veo 3.1 Fast empower developers to create more engaging content through significant upgrades. The models now generate\n\n**richer native audio**, from natural conversations to synchronized sound effects, and offer **greater narrative control** with an improved understanding of cinematic styles. **Enhanced image-to-video** capabilities ensure better prompt adherence while delivering superior audio and visual quality and maintaining character consistency across multiple scenes.\n\nIn addition to the model update, we are introducing new model capabilities, including using reference images to guide video generation, extend existing Veo videos, and generate transitions between a first and last frame.\n\nYou can now guide the generation process by providing up to 3 reference images of a character, object, or scene. This is helpful for maintaining character consistency across multiple shots or applying a specific style to your video.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nconfig=types.GenerateVideosConfig(\n\nreference_images=[reference_image1, reference_image2],\n\n),\n\n\n\n```... Your story is no longer limited by the original generation. With Scene extension, you can create longer videos, even lasting for a minute or more, by generating new clips that connect to your previous video. Each new video is generated based on the final second of the previous clip. This maintains visual continuity, making the feature ideal for extending shots with background audio.\n\n```\n\nfrom google import genai\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nvideo=video_to_extend\n\n\n\n```\n\nCreate smooth, natural scenes that bridge two different images. By providing a starting and an ending image, you can direct Veo 3.1 to generate the transition between them, complete with accompanying audio.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nimage=first_frame,\n\nconfig=types.GenerateVideosConfig(\n\nlast_frame=last_frame,\n\n),\n\n\n\n```\n\n**What people are building with Veo 3.1**\n\nPromise Studios, a GenAI movie studio, uses Veo 3.1 within its MUSE Platform to enhance generative storyboarding and previsualization for director-driven storytelling at production quality.... Latitude is experimenting with Veo 3.1 in its generative narrative engine to instantly bring user-created stories to life.\n\nVeo 3.1 and these new capabilities are now available in paid preview via the Gemini API.\n\nVeo 3.1 is the same price as Veo 3. We are incredibly excited to see what you will build with these new capabilities!",
                "domain": "developers.googleblog.com"
              },
              {
                "position": 5,
                "title": "Build with Veo 3, now available in the Gemini API",
                "url": "https://developers.googleblog.com/en/veo-3-now-available-gemini-api/",
                "snippet": "First unveiled at Google I/O 2025, people around the world have already generated tens of millions of high-quality videos with Veo 3 (along with some new fun and interesting video trends). It is our first video model to incorporate high-fidelity video outputs and native audio, first with text-to-video and soon with image-to-video.\n\nDevelopers are already experimenting with Veo 3, discovering how the model can help them brainstorm content, rapidly iterate, and be more efficient.\n\nVeo 3 is designed to handle a range of video generation tasks, from cinematic narratives to dynamic character animations. With Veo 3, you can create more immersive experiences by not only generating stunning visuals, but also audio like dialogue and sound effects.\n\nLet’s take a look at some examples.\n\nExplore these examples and more with Veo 3 in Google AI Studio, available as an SDK template and interactive Starter App to remix, copy and extend. The Starter App and its sample code offer a convenient way for Paid Tier users to rapidly prototype with Veo 3 and more on the Gemini API, directly from Google AI Studio.\n\nClick the Key button in the top right of the AI Studio Build interface to select a Google Cloud Project with billing enabled to use the Paid Tier in AI Studio apps. See the FAQs for more.... Veo 3 will be priced at $0.75 per second for video and audio output. Additionally, Veo 3 Fast will be available soon, offering a faster and more cost-effective option for video creation.\n\nHere’s a basic Python example to create a video:\n\n```\n\nimport time\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.0-generate-preview\",\n\nprompt=\"a close-up shot of a golden retriever playing in a field of sunflowers\",\n\nconfig=types.GenerateVideosConfig(\n\nnegative_prompt=\"barking, woofing\",\n\n),\n\n\n\n# Waiting for the video(s) to be generated\n\nwhile not operation.done:\n\ntime.sleep(20)\n\noperation = client.operations.get(operation)\n\ngenerated_video = operation.result.generated_videos[0]\n\nclient.files.download(file=generated_video.video)\n\ngenerated_video.video.save(\"veo3_video.mp4\")\n\n```\n\nAll videos generated by Veo 3 models will continue to include a digital SynthID watermark. To get started, check out the documentation, cookbook, and a Veo 3 starter app in Google AI Studio:\n\nIn addition to being available via the Gemini API in Google AI Studio, Veo 3 is also available to Google AI subscribers in the Gemini app and Flow, and to enterprise customers via Vertex AI.... 1: Fluffy Characters Stop Motion: Inside a brightly colored, cozy kitchen made of felt and yarn. Professor Nibbles, a plump, fluffy hamster with oversized glasses, nervously stirs a bubbling pot on a miniature stove, muttering, \"Just a little more... 'essence of savory,' as the recipe calls for.\" The camera is a mid-shot, capturing his frantic stirring. Suddenly, the pot emits a loud \"POP!\" followed by a comical \"whoosh\" sound, and a geyser of iridescent green slime erupts, covering the entire kitchen. Professor Nibbles shrieks, \"Oh, dear! Not again!\" and scurries away, leaving a trail of tiny, panicked squeaks.\n\n2: The sequence begins with an extreme close-up of a single gear, slowly turning and reflecting harsh sunlight. The camera gradually pulls back in a continuous movement, revealing this is but one component of a colossal, mechanical heart half-buried in a desolate, rust-colored desert. A sweeping aerial shot establishes its enormous scale and isolation in the barren landscape. The camera descends to capture pipes hissing steam and the rhythmic thumping that echoes across the empty plains. A subtle shake effect synchronizes with each massive heartbeat. A lateral tracking shot discovers tiny, robed figures scurrying across the metallic surface. The camera follows one such figure in a detailed tracking shot as they perform meticulous maintenance, polishing brass valves and tightening immense bolts. A complex movement circles the entire structure, capturing different maintenance teams working in precarious positions across its rusted exterior. The final shot begins tight on the meticulous work of one tiny figure before executing a dramatic pull-out that reveals the true scale of the heart and the minuscule size of its caretakers, tending to the vital organ of an unseen, sleeping giant that extends beyond the frame.",
                "domain": "developers.googleblog.com"
              },
              {
                "position": 6,
                "title": "Newest generative media...",
                "url": "https://blog.google/technology/ai/generative-media-models-io-2025/",
                "snippet": "# Fuel your creativity with new generative media models and tools\n\nToday, we’re announcing our newest generative media models, which mark significant breakthroughs. These models create breathtaking images, videos and music, empowering artists to bring their creative vision to life. They also power amazing tools for everyone to express themselves.\n\nVeo 3 and Imagen 4, our newest video and image generation models, push the frontier of media generation, with their groundbreaking new capabilities. We're also expanding access to Lyria 2, giving musicians more tools to create music. Finally, we’re inviting visual storytellers to try Flow, our new AI filmmaking tool. Using Google DeepMind’s most advanced models, Flow lets you weave cinematic films with more sophisticated control of characters, scenes and styles, to bring your story to life.\n\nWe’ve partnered closely with the creative industries — filmmakers, musicians, artists, YouTube creators — to help shape these models and products responsibly and to give creators new tools to realize the possibilities of AI in their art.\n\n## Veo 3: Video, meet audio\n\nVeo 3, our new state-of-the-art video generation model, not only improves on the quality of Veo 2, but for the first time, can also generate videos with audio — traffic noises in the background of a city street scene, birds singing in a park, even dialogue between characters.\n\nAcross the board, Veo 3 excels from text and image prompting to real-world physics and accurate lip syncing. It’s great at understanding; you can tell a short story in your prompt, and the model gives you back a clip that brings it to life. Veo 3 is available today for Ultra subscribers in the United States in the Gemini app and in Flow. It’s also available for enterprise users on Vertex AI.... ## Veo 2 updates: New capabilities built with and for filmmakers\n\nAs we advance Veo 3, we’ve also added new capabilities to our popular Veo 2 model informed by our work with creators and filmmakers. Today, we’re launching several of these new capabilities, including:\n\n**Our state-of-the-art reference powered video**capability allows you to give Veo images of characters, scenes, objects, and even styles for better creative control and consistency. **Camera controls**help you define precise camera movements, including rotations, dollies and zooms, to achieve the perfect shot. **Outpainting**allows you to broaden your frame, turning your video from portrait to landscape, and making it easier to fit any screen size, intelligently adding to the scene. **Object add and remove**lets you add or erase objects from your videos. Veo understands scale, interactions, and shadows, and uses this understanding to create a natural, realistic-looking scene.\n\nReference powered video and camera controls are available now in Flow. We're excited to bring all these new capabilities to the Vertex AI API in the coming weeks, and to more products over the next few months.\n\nOriginal\n\nOutpaint and add a castle\n\nOriginal\n\nRemove spaceship\n\n## Flow: An AI filmmaking tool designed for Veo\n\nBuilt with and for creatives, Flow is an AI filmmaking tool that lets you seamlessly create cinematic clips, scenes and stories by bringing together Google DeepMind’s most advanced models: Veo, Imagen and Gemini. Use natural language to describe your shots to Flow, manage the ingredients for your story — cast, locations, objects and styles — in a single convenient place, and use Flow to weave your narrative into beautiful scenes.\n\nFlow is available today for Google AI Pro and Ultra plan subscribers in the U.S., with more countries coming soon.... ## Imagen 4: Stunning quality and superior typography\n\nOur latest Imagen model combines speed with precision to create stunning images. Imagen 4 has remarkable clarity in fine details like intricate fabrics, water droplets, and animal fur, and excels in both photorealistic and abstract styles. Imagen 4 can create images in a range of aspect ratios and up to 2k resolution - even better for printing or presentations. It is also significantly better at spelling and typography, making it easier to create your own greeting cards, posters and even comics.\n\nImagen 4 is available today in the Gemini app, Whisk, Vertex AI and across Slides, Vids, Docs and more in Workspace.\n\nSoon we’ll also be launching a fast variant of Imagen 4 that’s up to 10x faster than Imagen 3 — so you can explore ideas even faster.\n\n## Lyria 2: Powerful composition and endless exploration\n\nIn April, we expanded access to Music AI Sandbox, powered by Lyria 2. Music AI Sandbox offers musicians, producers and songwriters a set of experimental tools, which can spark new creative possibilities and help artists explore unique musical ideas. The expertise and valuable feedback from the music industry help us ensure our tools empower creators, while inviting creatives to realize the possibilities of AI in their art.\n\nLyria 2 brings powerful composition and endless exploration, and is now available for creators through YouTube Shorts and enterprises in Vertex AI. We've also made Lyria RealTime, our interactive music generation model which powers MusicFX DJ, available via an API and in AI Studio. Lyria RealTime allows anyone to interactively create, control, and perform generative music in real time.... ## Responsible creation and collaboration with the creative community\n\nSince launching in 2023, SynthID has watermarked over 10 billion images, videos, audio files and texts, helping identify them as AI-generated and reduce the chances of misinformation and misattribution. Outputs generated by Veo 3, Imagen 4 and Lyria 2 will continue to have SynthID watermarks.\n\nToday, we’re launching SynthID Detector, a verification portal to help people identify AI-generated content. Upload a piece of content and the SynthID Detector will identify if either the entire file or just a part of it has SynthID in it.\n\nWith all our generative AI models, we aim to unleash human creativity and enable artists and creators to bring their ideas to life faster and more easily than ever before.",
                "domain": "blog.google"
              },
              {
                "position": 7,
                "title": "What Is Google Veo 3.1? A Beginner's Guide to AI Video ...",
                "url": "https://skywork.ai/blog/google-veo-3-1-beginners-guide-ai-video-model/",
                "snippet": "If you’ve seen short, cinematic clips made “from a prompt” and wondered how it works, you’re in the right place. This guide will walk you through Google’s latest video generation model, Veo 3.1, in plain English—what it is, why it matters, where you can try it, and a simple first project you can finish in about 10 minutes. Don’t worry if you’ve never touched AI video before. We’ll move step by step and flag common gotchas so you can avoid them.\n\n## What is Google Veo 3.1?\n\nGoogle Veo 3.1 is a text-to-video AI model that turns short written prompts and references into short video clips, with native audio support and more control over storytelling and style. In October 2025, Google announced Veo 3.1 (and a faster variant) in paid preview for developers and creators. According to the Google Developers Blog announcement (Oct 15, 2025), Veo 3.1 is available via the Gemini API in Google AI Studio and Vertex AI, and it adds richer native audio, improved adherence to cinematic styles, and new creative controls.\n\nWhy it matters for beginners: Veo 3.1’s improvements make it easier to describe the mood and motion you want—like “a slow dolly-in on a vintage typewriter with soft rain ambience”—and actually get something close, without wrestling with advanced settings.... ## What’s new compared to earlier Veo versions?\n\nGoogle calls out a few key upgrades in Veo 3.1:\n\n- Richer native audio generation and better narrative control, with improved understanding of cinematic styles (publisher: Google; see the Developers Blog announcement (2025)).\n\n- New creative tools in supporting apps: you can guide generation with multiple reference images, extend existing Veo clips, or bridge between a first and last frame to create transitions. These capabilities are highlighted in the Google Blog on Veo 3.1 and Flow (Oct 2025).\n\n- Native audio emphasis and model positioning are also described on the DeepMind Veo model page, which introduces Veo’s “video, meet audio” concept.\n\nIndependent coverage, like TechCrunch’s report on the Veo 3.1 release (Oct 2025), provides context on rollout and app integration. For hard limits (length, resolution), always defer to Google’s official docs, as those details can change.\n\n## Where can you use Veo 3.1 today?\n\nAs of October 2025:\n\n- Flow (Google’s AI video editor): Google’s announcement notes Veo 3.1 and advanced creative controls in Flow, including “Ingredients to Video” (use multiple images as style/character references), “Frames to Video” (bridge between start/end frames), and “Extend” (lengthen a clip). Details are outlined in the Google Blog on Veo 3.1 and Flow (2025).\n\n- Gemini API (Google AI Studio and Vertex AI): Veo 3.1 and Veo 3.1 Fast are in paid preview via the Gemini API, per the Google Developers Blog announcement (2025).\n\nImportant: Access tiers, regions, and pricing can vary and may change. If you’re not seeing options in your account, check Google’s documentation or support for your account type and region.... ## What can Veo 3.1 generate right now? (Practical limits to know)\n\n- In Vertex AI’s preview for Veo 3.1 base generations, the documented lengths are currently short (choose 4, 6, or 8 seconds). This is specified on the Vertex AI Veo 3.1 preview page.\n\n- In Gemini API extension workflows, you can extend Veo-generated clips. The API docs describe a maximum of up to 141 seconds for input Veo videos in those extension scenarios, with 720p listed in that context. See the Gemini API video documentation (Google).\n\nThese numbers help set expectations: start with short shots, then extend or chain shots together. Avoid assuming 1080p or minute-long base generations unless Google’s official docs explicitly state it for your environment.... ## A 10-minute quickstart: your first Veo 3.1 clip\n\nWe’ll make a simple “coffee shop mood” shot you can adapt for social posts.\n\n- Choose your aspect ratio\n\n- 16:9 (landscape) for YouTube and desktop-first screens\n\n- 9:16 (vertical) for TikTok, Reels, and Shorts Pick one at the start to avoid accidental cropping later.\n\n- Write a clear, concrete prompt Use this structure to get reliable results: subject + action + setting + style + camera + audio.\n\n- Example prompt: “A barista gently places a ceramic cup on a wooden counter; shallow depth of field; warm morning light streaming through windows; slow dolly-in; subtle steam rising; soft cafe ambience, no dialogue, light jazz in the background.”\n\nIf you want a deeper primer on prompt structure and clarity, see these prompt engineering best practices for beginners.\n\n- Generate in your chosen interface\n\n- In Flow: Start a new project, choose Veo 3.1, paste your prompt, set aspect ratio, and generate.\n\n- In Gemini API/Vertex AI: Use the model/version and parameters documented for Veo 3.1 in your environment. Begin with short durations and default settings.\n\n- Review the output like a director\n\n- Does the subject and action match? Is the lighting/mood close?\n\n- Is the camera movement smooth? Is audio what you expected?\n\n- Make one or two changes per iteration—small edits beat big rewrites.\n\n- Refine with controls\n\n- Reference images: If you want consistency (e.g., the same mug or barista style), use up to three reference images to guide the look. This capability is described in Google’s announcements (2025).\n\n- Frames to Video: Provide a starting and ending frame to shape the motion between them.\n\n- Extend: Lengthen your favorite moment to create a longer beat.\n\n- Export Choose the format/aspect ratio you started with. If you need both vertical and horizontal versions, plan to reframe or regenerate with the other aspect ratio rather than cropping aggressively.... ## A practical planning example using Skywork AI (optional, 5 minutes)\n\nSkywork AI can help you prepare the words before you ever hit “Generate.” Disclosure: Skywork AI is our product.\n\nHere’s a neutral, step-by-step way to use it purely for planning:\n\n- Open Skywork and create a new document. Ask for a short video outline: “30-second coffee shop mood piece: 3 shots, warm tone, slow camera.”\n\n- Have it draft a compact shot list with camera moves and audio notes, like:\n\n- Close-up of cup; slow dolly-in; soft steam; light jazz; no dialogue\n\n- Medium barista hands; gentle rack focus; cafe ambience; espresso hiss\n\n- Wide room tone; sunbeams; slow tilt up; footsteps and cups\n\n- Ask for a final prompt assembled from the shot you want to generate first. Copy that prompt into Veo 3.1.\n\nIf you prefer to outline prompts and story beats yourself, this short guide to Skywork’s General Mode for planning and outlining walks through a simple, distraction-free workflow.... ## FAQs for first-time users\n\n\n\nIs Veo 3.1 free? No. Google describes Veo 3.1 and Veo 3.1 Fast as being in paid preview via the Gemini API as of October 2025. See the Google Developers Blog announcement (2025). Pricing varies by product and usage.\n\n\n\nWhere can I try Veo 3.1? Flow (for a visual editor), and the Gemini API via Google AI Studio and Vertex AI (for developers). Google outlines these options in the Veo 3.1 and Flow post (2025) and the Developers Blog announcement (2025).\n\n\n\nHow long can my video be? It depends on the environment and workflow. In Vertex AI’s Veo 3.1 preview, base generations are short (4, 6, or 8 seconds). In Gemini API extension workflows, input Veo videos can be extended up to 141 seconds. See the Vertex AI preview page and the Gemini API video docs.\n\n\n\nWhat resolutions are supported? The Gemini API documentation for extension workflows references 720p in that context. Official materials do not universally guarantee 1080p for base Veo 3.1 generations at this time.\n\n\n\nCan I keep a character or object consistent across shots? Yes. Google notes you can guide Veo 3.1 with up to three reference images for a character, object, or scene. This is described in the Developers Blog announcement (2025).\n\n\n\nCan I add my own audio and sync it? Veo 3.1 focuses on native audio generation controlled via your prompt. The public pages cited here do not detail a full “upload and auto-sync” workflow, so avoid relying on that unless you see it documented for your account.... ## Next steps\n\n- Try a tiny project: one 6–8 second shot with a clear prompt. If you like the result, use “Extend” or stitch a second shot with “Frames to Video.”\n\n- Build a simple storyboard first. If you want help outlining ideas, you can use Skywork AI to draft prompts and shot lists before you generate. Keep it simple: one scene, one action, one camera move.\n\n- When you’re ready to go deeper, explore Google’s official resources: the Veo 3.1 and Flow post (2025), the Developers Blog announcement (2025), the DeepMind Veo overview, the Gemini API video docs, and the Vertex AI Veo 3.1 preview.\n\nYou don’t need special “artistic talent” to start—just one clear sentence and a little patience. Generate, review, tweak, repeat. That’s how everyone learns, and you’ll be surprised how quickly it clicks.",
                "domain": "skywork.ai"
              },
              {
                "position": 8,
                "title": "Google announces Veo 3.1 and Flow updates - 9to5Google",
                "url": "https://9to5google.com/2025/10/15/veo-3-1/?extended-comments=1",
                "snippet": "Google today announced Veo 3.1 as its latest video generation model, with Flow getting a number of updates to take advantage of the latest capabilities.\n\nCompared to Veo 3, which was announced at I/O 2025 in May, this new version offers richer audio and “enhanced realism that captures true-to-life textures.” Veo 3.1 has a deeper understanding of storytelling, cinematic styles, and character interactions to give you more narrative control. The image-to-video capability benefits from improved audio-visual quality and better follows your prompt.\n\nVeo 3.1 and Veo 3.1 Fast are available in the Gemini app, as well as the Gemini API and Vertex AI, to power text-to-video and image-to-video for horizontal (16×9) and vertical (9×16) outputs.\n\nMeanwhile, Google is updating the Flow filmmaking tool to take advantage of Veo 3.1. Audio generation is coming to:\n\n**Ingredients to Video**: This can include images of multiple characters, objects, and styles **Scene extension**: You can Extend an existing clip, with Google basing it on the last second **Frames to Video**: This lets you specify the starting and ending images, with Flow bridging the two\n\nFlow is getting new editing capabilities. You can insert elements like objects, characters, and details, with Google handling shadows, scene lighting, and other complex details to make everything look natural.\n\nComing soon is the ability to remove objects and characters from a scene, with Flow working to reconstruct the background and surroundings to make the edit seamless.\n\n\n\n## Comments",
                "domain": "9to5google.com"
              },
              {
                "position": 9,
                "title": "Google rolls out its new Veo 3 video-generation model ...",
                "url": "https://techcrunch.com/2025/07/03/google-rolls-out-its-new-veo-3-video-generation-model-globally/",
                "snippet": "Google on Thursday said it has begun rolling out its Veo 3 video generation model to Gemini users in more than 159 countries.\n\nVideo generation via the new model is available only to paying subscribers of Google’s AI Pro plan and is capped at three videos per day.\n\nVeo 3, which Google showed off in May, lets users generate videos up to eight seconds long using text prompts.\n\nGoogle’s Josh Woodward has said that the company is working on adding image-to-video generation capabilities to Gemini.",
                "domain": "techcrunch.com"
              },
              {
                "position": 10,
                "title": "Google's Veo 3.1 is better at generating videos from images",
                "url": "https://www.engadget.com/ai/googles-veo-31-is-better-at-generating-videos-from-images-220829129.html",
                "snippet": "Google has released a new update to its Veo AI video generation model that should make it do a better job of sticking to prompts and converting images into videos. Veo 3.1 is available to try today through Google's Gemini API and is now also powering the company's Flow video editor.\nVeo 3.1 builds on the new capabilities Google introduced with launch of Veo 3 at Google I/O 2025. The new model offers better \"prompt adherence,\" according to Google, and should have an easier time creating videos based on the image \"ingredients\" you upload alongside your written prompt. Veo 3.1 also makes it possible to convert images to video and generate audio at the same time, a capability that wasn't available with Veo 3.\nIn Flow, Veo 3.1 supports at least a new feature that gives you finer control over the videos you generate. With what Google calls \"Frame to Video,\" Flow lets you upload a first and last frame, and then generates the video in-between. Adobe Firefly, which is powered by Veo 3, offers a similar feature, but Flow will be able to pull it off and create audio at the same time. Those added audio skills will also apply to the video editor's ability to extend clips and insert objects into existing footage, too.\nBased on the samples Google's shared, videos generated with Veo 3.1 still have an uncanny quality that seems to vary greatly depending on the prompt and subject. Even if it's missing some of the realism of OpenAI's Sora 2, though, the company's decision to try and make Veo more useful to people who actually work with video rather than a source of social media spam is a welcome move.",
                "domain": "www.engadget.com"
              }
            ],
            "success": true,
            "error": null
          },
          {
            "query_id": "q8",
            "query": "Google veo3.1 release 2025 debunked",
            "claim_id": "claim_1",
            "query_type": "contradiction",
            "priority": "high",
            "results": [
              {
                "position": 1,
                "title": "VEO 3 is UNREAL...it might actually take my job",
                "url": "https://www.youtube.com/watch?v=Xj4BDwgEwd4",
                "snippet": "{ts:0} VO3 is here and it's shocked the entire industry. Uh, Alex, what was that? It's\n{ts:8} nothing. Don't worry about it. Are you trying to replace me with AI? What? No, I'm just trying something new. Just do\n{ts:16} the normal intro. It It's fine. All right. V3 just dropped and I'm going to show you so many incredible examples of\n{ts:24} V3 in action. Let's get into it. Oh, and Alex, don't do that again. Anyways, let's get into it. All right. I've been\n{ts:32} seeing a ton of these street style interviews, hyper realistic, where someone is interviewing people on the\n{ts:38} street who kind of just stumbled out of a bar. They all are either a little bit tipsy or drunk, and V3 has been\n{ts:45} recreating these so well. So, here are two that I've made. The dialogue between them didn't exactly match my prompt, but\n{ts:53} I'll show you some others where it's pretty compelling. So, this is street interview. Hyperrealistic guy\n{ts:59} interviewing two girls, all Gen Z. They just came out of a bar kind of tipsy drunk. The interviewer asks,... \"Okay,\n{ts:65} low-key, can you believe we aren't real?\" Then girl one says, \"I don't know about you, honey. I'm 100% real.\" In a\n{ts:72} sassy attitude, \"Please do not clip that.\" And girl two says, \"Yeah, yeah, bet. We're as real as they come.\" Almost\n{ts:78} interrupting the first girl. Okay, so here's the first video. Okay, low key, can you believe we aren't real? I don't\n{ts:85} know about you, honey. I'm 100% real. Yeah. Yeah. Yeah. Bet. We're as real as they come. All right. And then here's\n{ts:90} the second generation, the second version that V3 created. Okay. Low key. Can you believe we aren't real? I don't\n{ts:96} know about you, honey. I'm 100% real. Yeah. Yeah. Bet. We're as real as they come. All right. So, in this next one, I\n{ts:103} tried to get two trains heading towards each other. They smash into each other. Huge explosion. And for some reason, I\n{ts:110} just could not get it to work. But let me show you what did generate. So, here was my first attempt. Two massive trains... {ts:255} second one, the hand looks better. That's weird. And it kind of just changes\n{ts:263} frames for a second. But uh yeah, either way, that is not what's in a Rubik's cube. Look at this. It actually looks\n{ts:269} really cool. Look at all of these detailed gears inside the Rubik's Cube. All right, but of course you are all\n{ts:275} familiar with this meme. And what if you get V3 to make a video out of it? Are you serious right now? I can't\n{ts:284} believe you. You are unbelievable. Do you have an idea that you've been putting off for a\n{ts:290} while because you don't have the technical knowledge to turn it into reality? With Hostinger Horizons, this\n{ts:296} is now possible. Hostinger just launched Hostinger Horizons, which is the easiest way to launch full applications with no\n{ts:305} code. This is vibe coding, but even easier because the deployment happens automatically. Hostinger Horizons is an\n{ts:312} all-in-one solution. Manage hosting, domains, and email all in one place while being able to take your idea from... {ts:459} kind of wanted to see if it could nail the horror vibe with like aliens in a dark alley. So, check this out.\n{ts:469} I think the only thing that it didn't do super well in this video is the sound effects are just okay. If we look over\n{ts:475} at the second one, it did a way better job with the sound effects. Yeah, I mean that's great. Even\n{ts:483} when the alien kind of like walks in front of the light, it covers the light on the ground. That's pretty dang... 't know\n{ts:558} what VO was doing. All right, Matt, back to you. But remember, Flow from Google, which houses VO3, is not just about\n{ts:566} creating 7-second clips. You should be able to create entire videos from it. It basically allows you to take these clips\n{ts:572} and put them together in really unique ways. Here's one that is if Jurassic Park were actually real. Check this out.\n{ts:578} We are on our way to Jurassic Park. I am so excited. I've always wanted to go and finally we are doing it. It's going to\n{ts:584} be great. Right, Jason? Yeah. I can't wait. Okay, bye. All right, we're [Music]\n{ts:596} here. Got the dinosaurs. Got all the people watching. I mean, everybody looks really good. There's no limbs coming out\n{ts:602} of them. This dinosaur doesn't look fantastic. All of these dinosaurs don't look hyper realistic. I think they could\n{ts:608} have done a little bit better of a job there. It looks like animatronics to be honest, but still. This is all from AI... {ts:742} screen. Yeah. And the crazy thing about this thing is it has All right, so this is one thing I've noticed with these V3\n{ts:751} videos. Whenever it has a human talking, there's always these awkward pauses. It's just a half second too long of a\n{ts:757} pause and that's where you really can see it's AI generated, but most of the time you can't. In fact, you know that\n{ts:765} opening clip that we did in this video, I showed that to my wife and said, \"Somebody copied our channel and I just\n{ts:771} showed it to her and didn't say anything else.\" And she looked at it and she was like, \"Oh, oh, that sucks.\" And then I\n{ts:777} had to tell her it was AI because it looked that real. All right, let's keep watching. Heated\n{ts:785} seats. Check this out. Look at that. Just slice right through. The N9 portable fusion reactor is small and\n{ts:794} almost meltdown free. Almost. This is the best flying experience\n{ts:804} ever. These controls are amazing. All right, so I thought that was really good. But it... 's it's pretty good. The first\n{ts:1126} time that I watched it, I did actually laugh out loud. That was actually pretty funny. Next, I just asked it to make a\n{ts:1132} detailed look at the solar system. And yeah, I think it did an all right job. Let's check out the second\n{ts:1140} clip. Now, this one I think is a bit better, but V3 kind of likes to put objects in front of the camera when it's\n{ts:1148} backing out like that right there. And it kind of just like spawns a planet right in front of the camera. And yeah,\n{ts:1154} it doesn't look the best, but honestly, it's pretty good. All right, back to Matt. All right, next. I wanted to see\n{ts:1161} if it could create the game Portal, but hyperrealistic. Essentially, what I wanted it to do is what I say here. A\n{ts:1167} mediumbuild man in his 30s wearing a futuristic tactical suit with glowing blue accents stands in a dimly lit\n{ts:1173} industrial room with exposed pipes, metal walls, and flickering lights. He holds a sleek high-tech portal gun with... {ts:1233} in terms of just visuals looks incredible. Although it's not what I asked\n{ts:1242} for. Yeah. So, pretty good. You only saw his reflection through the portal for about a frame or two, but it wasn't that\n{ts:1250} good. Anyways, but the visuals again, the visuals, the detail, the clarity, all really, really impressive. All\n{ts:1257} right, next. Meta Puppet made a video called This is Plastic made with VO3. Spoilers in next post. Watch before\n{ts:1264} reading. So, this is a 2 minute 45 second video. Quite long. I'm not going to play it in full. I'm going to skip\n{ts:1269} around a little bit, but let me show you. Studies have revealed that microplastics are being found in human\n{ts:1274} testicles, raising concern. You can never trust these studies on male reproductive health.\n{ts:1279} [Music] Okay, that is hilarious. And remember, all of this was put together using Flow\n{ts:1291} Plus V3. These are both Google products. So, you have a little plastic baby. God, that",
                "domain": "www.youtube.com"
              },
              {
                "position": 2,
                "title": "I was wrong - AI video is nuts (don't sleep on Veo 3)",
                "url": "https://www.youtube.com/watch?v=_3PCta2uyvc",
                "snippet": "## Theo - t3․gg\n##### May 26, 2025 (0:16:08)\nI severely underestimated Google's Veo 3 model. The output quality is insane, we need to talk about this...\n\nThank you Imagekit for sponsoring! Check them out at: https://soydev.link/imagekit\n\nUse code VEO for 1 month of T3 Chat for just $1: https://soydev.link/chat\n(only valid for new customers)\n\nSOURCES\nhttps://x.com/ArtificialAnlys/status/1925159679824744804\nhttps://x.com/ArtificialAnlys/status/1925549565303763269\nhttps://x.com/theo/status/1925125767371149823\nhttps://x.com/theo/status/1925134963978207319\n\nWant to sponsor a video? Learn more here: https://soydev.link/sponsor-me\n\nCheck out my Twitch, Twitter, Discord more at https://t3.gg\n\nS/O Ph4se0n3 for the awesome edit 🙏... {ts:0} I just did a video about Google IO, but I missed something. I thought the video\n{ts:4} model was mediocre. I was wrong. Pretty nuts for a oneshot, right? Like, I just generated that trivially. It still costs\n{ts:13} 250 bucks a month to use any of this right now. And the UI is garbage and it's annoying as hell to use. But the\n{ts:18} quality of what you can get out of V3 is significantly better than I thought. My tests were bad. I didn't look into it\n{ts:25} enough. And I'm making this video both because I was wrong for not better covering it, but also because I found it\n{ts:32} actually very, very fun to play with and I wanted to share with you guys. That all said, I've already burned through\n{ts:37} most of the credits I get for the $250 and I want more. So, quick break from today's sponsor and then we'll get right\n{ts:44} to it. I've been a webdev for a while and one of the most annoying things to get right is images. Seriously, I can't\n{ts:50} believe I... {ts:265} other people doing demos with it. like, \"Wait, it can do that much?\" I went and played more. There was a lot of edges\n{ts:270} that I had to get through. The biggest one being the Flow website, which is so bad. We'll go over some of the ways it's\n{ts:276} bad in just a bit. I was trying to prompt it to look like me back when I still had the blonde hair and mustache,\n{ts:280} and it came out looking like Prime. But another test, I tried this one like eight times, and this is the best I\n{ts:285} could do. Something caused the first still to look awful. I don't know why it's like that. None of the rest had\n{ts:291} that problem. Once you It plays, it's fine, but you'll notice some details on this one.\n{ts:296} Use code VEO at checkout for one month free on T3 Chat. Yeah, it isn't great at\n{ts:305} text. It tried, but it's not great at it. You need to give it a very small amount of text to render. And even if\n{ts:311} you tell it to not put in subtitles, it just will sometimes. The free month code included there did work, but we... {ts:372} made the mistake of here is I assumed when you do frames to video and you give it a frame that you've saved that it\n{ts:380} would still use the thing you selected because if you do ingredients to video and you select something for it to start\n{ts:386} and you try to submit it with V3 selected, it will fail. It says in the corner here and I need it on full screen\n{ts:392} for you to see it. Switching you to a compatible model for this feature. Submit again to confirm or check\n{ts:397} settings for details. I wish it told me where in settings to check. I don'... {ts:561} was wrong. I just Yeah, it's the weird breath at the end. Cool. Stop it there. Then we will extend it and say make sure\n{ts:570} we're on the right model because again it keeps changing back to V2 even though this is the VO3 clip I'm trying to\n{ts:575} extend. I almost want to try it so you can see how much worse it is in comparison. Switching you to a\n{ts:580} compatible model for this feature. Submit again to confirm. Look at that. You can't even use it on V2 quality. It\n{ts:587} bumps you to fast. There's so much potential here and just none of it's being realized because\n{ts:594} this UI is awful. It it tricked me into thinking this was all much worse than it actually is. I wish they just gave us\n{ts:600} the model in a more reasonable like shape for us to play with and consume. But V3 is not on the API yet. There's no\n{ts:606} way for us to use any of it yet. So sorry T3 chat can't add this. But despite all of that, it's still just an\n{ts:613} incredible model. Do you know what... 's even better than this spaghetti? T3 chat. Like what? What do\n{ts:625} you guys remember like a year and a half ago how far we were from Will Smith eating spaghetti? It's not Will Smith,\n{ts:632} but that is absolutely spaghetti being eaten. It's kind of crazy where that's all at. Google doesn't know how to make\n{ts:640} creative tools or really power tools in general. They make decent enough consumerf facing software. They make\n{ts:647} decent enough infrastructure and they make incredible models in generative tools, but they don't know how to make\n{ts:653} like a good video editor. If you don't believe me, go try the one they built for YouTube. It's it's interesting. It's\n{ts:658} a it's often cited as a good example of a Flutter app. If you can predict what that means for the quality of\n{ts:665} experience, but the model here is so good. And once again, what I'm excited about is what people will do with this\n{ts:672} tool. But I'm also a bit terrified because this looks better than some like iPhone video. I see things like\n{ts:681} verifying your identity just got a lot sketchier because if I... t going to trust it as much. This is going to really change our like trust vectors for\n{ts:839} what is or isn't real. I don't even know now how I will be able to tell if a given video that is sent to me is real\n{ts:844} or not because this stuff is actually that compelling. And if somebody makes a less restricted version of this model or\n{ts:850} gets something close to this in the open source world or with stable diffusion, I'm scared. I'm legitimately scared. You\n{ts:858} are telling me to try again generating with my blurred photo. I'll be more specific. Clean shaven white\n{ts:866} man. Be sure to include the audio of him speaking. Make sure it's still V3. Yep. Cool. Let's see how it does. Switching\n{ts:875} you to a compatible model. So, it's too fast. Not even quality. Yeah, you can't do it. you you can't do anything but\n{ts:883} text the video for V3 right now, which I'm pretty sure is a safety thing just due to the nature of what this model is\n{ts:890} capable of. And as we've now seen, and I can show more examples of the gap between two and three is a bit... {ts:898} absurd. This is one I accidentally did with two. You can see the audio doesn't exist. It got the text okay there, but\n{ts:905} it went a little absurd with the subtitles. This one was really funny. It feels like a Bollywood\n{ts:915} movie. The way the T3 chat fades into the screen is so hilarious. Yeah, this is why I didn't care because none of the\n{ts:923} video models have felt like a significant improvement from that to this point. I did not realize how absurd\n{ts:930} this got, especially with how bad the UX is. Like I hit the upscale button cuz when you download, you can choose what\n{ts:937} format you want to download in. If it's not frozen, which it was there for a sec. You can pick animated GIF,\n{ts:942} original, or upscaled. Upscale just doesn't work. I've been sitting here waiting for this to upscale for like an\n{ts:947} hour now, and it just hangs forever. It does say this can take a few minutes, but like what's a few minutes, Google?\n{ts:956} It's been an hour. Yeah. What did you think? Is this exciting or scary? Until next time, peace nerds.",
                "domain": "www.youtube.com"
              },
              {
                "position": 3,
                "title": "Veo (text-to-video model) - Wikipedia",
                "url": "https://en.wikipedia.org/wiki/Veo_(text-to-video_model)",
                "snippet": "**Veo**, or **Google Veo**, is a text-to-video model developed by Google DeepMind and announced in May 2024. As a generative AI model, it creates videos based on user prompts. Veo 3, released in May 2025, can also generate accompanying audio.\n\n## Development\n\nIn May 2024, a multimodal video generation model called Veo was announced at Google I/O 2024. Google claimed that it could generate 1080p videos over a minute long. In December 2024, Google released Veo 2, available via VideoFX. It supports 4K resolution video generation and has an improved understanding of physics. In April 2025, Google announced that Veo 2 became available for advanced users on the Gemini app.\n\nIn May 2025, Google released Veo 3, which not only generates videos but also creates synchronized audio — including dialogue, sound effects, and ambient noise — to match the visuals. Google also announced **Flow**, a video-creation tool powered by Veo and Imagen. Google DeepMind CEO Demis Hassabis described the release as the moment when AI video generation left the era of the silent film.... ## Capabilities and limitations\n\nGoogle Veo can be bought by several subscription/membership tiers, and/or by using Google \"AI credits\". The software itself can be run by two different consoles called Google Gemini and Google Flow, with Gemini being geared towards shorter, quicker, and faster projects, using the Gemini AI chat model, or through Google Flow, which is essentially a movie editor, as well, allowing users to create longer projects, and continuity using the same characters and actors. Users can create a maximum length of eight seconds per clip.\n\nGoogle Veo, has a relatively simple interface and dashboard, however writing prompts, for those who have little to no experience in transcribing or filmmaking may face issues with the software misunderstanding what the user intended by their prompt (no matter how detailed it was). So although Veo does have a friendly and simple setup, prompts, which are the forefront of the software, need to be not only short and to the point, but they also must be very specific, if the user wants the right vision for their project. Google Veo, when it comes to human models, is able to generate several ethnicity and body types. The software is also capable of generating stand up comedy routines, and Music videos. It can as well generate animals, cartoons, and animation. Prompts must accurately describe places, people, and things in each scene, in addition knowledge of film and camera lingo such as panning, zooming, and terms for camera angles, are also important.... Google Veo however, has strict guidelines and blockades to their software. Before a clip is generated, the algorithm computer software reviews it, and if it is anything deemed inappropriate, too graphically sexual, illegal, showcasing graphic abuse/assault/fighting (unless the prompt specifies that it is a fictitious martial arts scene etc.) gross behaviors, antisemitism, racist, homophobic, anything depicting reigning regimes, rioting, blood, gore, or warfare, (unless in some cases the prompt specifies that it is fictitious period drama, the clip may still be generated), the clip will not be generated. In addition, Google Veo cannot and will not generate character actors that look identical to celebrities or real-life individuals. Users have primarily complained that, regardless of how descriptive and detailed their prompts are, Google Veo often misunderstands the input, resulting in completely different outputs. Common issues include the emulation of incorrect subtitles and captions, the generation of complex scenes that are incomplete due to the maximum eight-second length, the production of garbled and nonsensical speech, and character models that appear deformed in both appearance and movement. Users have also reported that their prompts and generated content are falsely flagged as violating guidelines, along with a variety of other issues and complaints. However, trial and error may have to be used with Veo for optimal results.... ## Reactions\n\nA reporter for *Gizmodo* reacted to the release of Veo 3 by observing that users were directing the model to generate low-quality content, such as man on the street interviews or haul videos of people unboxing products. Another media commentator reported that the tool tended to repeat the same joke in response to different prompts.\n\nCommentators speculated that Google had trained the service on YouTube videos or Reddit posts. Google itself had not stated the source of its training content.\n\nIn July 2025, Media Matters for America reported that racist and antisemitic videos generated using Veo 3 were being uploaded to TikTok. Ryan Whitwam of *Ars Technica* commented, \"In a perfect world, Veo 3 would refuse to create these videos, but vagueness in the prompt and the AI's inability to understand the subtleties of racist tropes (i.e., the use of monkeys instead of humans in some videos) make it easy to skirt the rules.\"\n\n## See also\n- Sora (text-to-video model)\n- VideoPoet – Text-to-video model by Google\n- Dream Machine (text-to-video model)\n\n## References\n\n## External links\n- Official website\n- *Introducing Veo 3.1 and advanced capabilities in Flow*\n\nCategories: - 2024 software\n- Applications of artificial intelligence\n- Film and video technology\n- Google DeepMind\n- Text-to-video generation\n- Video processing\n- Generative artificial intelligence\n- 2024 in artificial intelligence",
                "domain": "en.wikipedia.org"
              },
              {
                "position": 4,
                "title": "Google's Veo 3 Has People Crashing Out Over AI Slop",
                "url": "https://gizmodo.com/googles-veo-3-has-people-crashing-out-over-ai-slop-2000608803",
                "snippet": "Depending on who you ask, generative AI is either a thrilling tech revolution or an existential threat, and there's little in-between. It's hard to blame anyone for an extreme reaction, too, given the magnitude of capital investment, hyperbolic marketing, and rapid progress of generative AI in such a short amount of time. But it's not just the economics and technical feats of AI that have people losing their minds; there's also something more philosophical percolating, and it's driving some people 'to the brink.'\nPeople are literally having a mental breakdown over Veo-3 pic.twitter.com/ym5oZDYZGr\n— Chubby♨️ (@kimmonismus) May 27, 2025\nThe latest AI advancement to send people down an existential rabbit hole comes courtesy of Google, which just announced its latest video generation model called Veo 3. As I've reported a few times now, Veo 3 is already getting into some wild stuff—turning up the dial on AI slop, deepfaking smooth-brained YouTube content, and potentially upending game development, to name a few things. As it turns out, people are taking note of all of those feats, and some of them are not exactly happy about what they see.... As evidenced by a thread from the subreddit r/artificialintelligence posted this week titled 'VEO3 is kind of bringing me to a mental brink. What are we even doing anymore?' Google's Veo 3 and the implications therein have some people spiraling. 'I'm just kind of speechless. The concept of existential crisis has taken a whole new form. I was unhappy with my life just now but thought I can turn it around, but if I turn it around, what is left of our world in 2 decades?' the post's author writes.\n'Actors as a concept are gone? Manually creating music? Wallpapers? Game assets? Believing comments on the internet are from real people? AI edited photos are just as real as the original samples? Voicenotes can be perfectly faked?… Literally what value is being left for us?'\nReactions to the thread are mixed, with suggestions that the author should go 'touch grass' or maybe 'go to therapy,' but there's also a chorus in agreement. The consensus from the latter group? AI slop is coming to ruin your art, and there's not much we can do about it.\nI, for what it's worth, fall unhelpfully in between the two camps. I think there is a deluge of AI slop incoming, and, if we're being honest, we're already up to our ankles. Between Veo and OpenAI's Sora and the clear interest in automating human creativity, I think we can reasonably buckle in and expect the world of movies, music, and entertainment writ large to get a little choppy. Whether any of those efforts to automate entertainment will stick is less obvious. The thing about art is that the kind that people tend to like is the kind that has something substantial to say. Right now, for all of its mimicry, generative AI doesn't actually have anything to say, because technically all it can do is remix and repeat.... I did more tests with Google's #Veo3. Imagine if AI characters became aware they were living in a simulation! pic.twitter.com/nhbrNQMtqv\n— Hashem Al-Ghaili (@HashemGhaili) May 21, 2025\nCall me an optimist, but most people can likely sniff out the difference between slop and art, and as much as studios would love to wave a magic wand and rid themselves of human creatives and the cost of their labor, deep down they know that they'd have to Ctrl+Z that move just as fast. That's not to say there won't be casualties in the AI age—if there's one lesson we can learn from mass waves of automation in years past, it's that labor forces are usually the most affected.\nBut when it comes to art, things aren't so simple. Art, at least the good kind, is about human connection, and until AI can think and feel like we do, there's nothing that can replace that. So, before you crash out over AI slop, just remember: AI still thinks putting glue on your pizza is a good idea, so we may have a few more good years left in the tank.",
                "domain": "gizmodo.com"
              },
              {
                "position": 5,
                "title": "What to Expect Veo 3.1 Google's Next-Gen Video AI Release",
                "url": "https://gptproto.com/blog/veo-3-1",
                "snippet": "The AI video generation space is heating up. Following recent improvements to Veo 3 announced in September 2025, early signs suggest Google DeepMind is preparing to launch Veo 3.1 on October 10, 2025. While Google hasn’t officially confirmed the release, leaked information from various sources points to significant upgrades that could reshape how creators make video content. This incremental update appears designed to keep pace with OpenAI’s Sora 2, which launched just last month with impressive capabilities.\n\nIf you’re a content creator, filmmaker, or developer wondering whether this matters for your workflow, here’s what the leaks suggest about tomorrow’s release:\n\n- Ultimate character consistency that solves the morphing problem\n\n- Native 1080p output with professional cinematic presets\n\n- Extended video generation up to 60 seconds\n\n- Multi-prompting for creating connected scene sequences\n\n- Better prompt understanding and motion quality\n\n- Access through unified platforms like GPT Proto\n\n## Understanding Veo 3.1\n\nVeo 3.1 represents the latest evolution of Google DeepMind’s video generation technology. Building on the foundation of Veo 3, which introduced native audio generation in May 2025, this new version focuses on solving practical problems that creators face daily. Think of it as Google’s answer to the competitive pressure from OpenAI and other players in the AI video market.\n\nThe timing is strategic. OpenAI released Sora 2 on September 30, 2025, with features that emphasize physical realism and multi-scene storytelling. Google appears ready to counter with its own improvements that address similar needs while leveraging its existing infrastructure across Gemini and Vertex AI.... ## How We Know About Veo 3.1\n\nThe first hints appeared on Higgsfield AI’s waitlist page, where references to Veo 3.1 surfaced unexpectedly. Shortly after, developers noticed the model name in Vertex AI’s internal codebase, suggesting Google was preparing for deployment. Community discussions on platforms like Reddit and Discord added fuel to the fire, with some users claiming they received early access invitations.\n\nNone of this constitutes official confirmation from Google, but the pattern matches previous releases. Veo 2 and Veo 3 both followed similar leak patterns before their official announcements. The lack of denial from Google has only increased speculation that tomorrow’s launch is real.... ## Major Feature Upgrades Based on Leaks\n\n### Character Consistency Finally Solved\n\nOne of the biggest complaints about current AI video models is character morphing. You start with someone who has brown hair, and by second four, they suddenly have blonde hair. Leaked information suggests Veo 3.1 tackles this head-on with what insiders call “ultimate character consistency.” The system reportedly maintains facial features, clothing, and physical characteristics throughout the entire clip without the annoying shifts that break immersion.\n\n### True 1080p with Cinematic Presets\n\nWhile Veo 3 added 1080p support in September, Veo 3.1 apparently takes this further with built-in cinematic presets. Instead of just getting higher resolution, you’ll have options for film noir, sci-fi, documentary style, and other professional looks right out of the box. This matters because it reduces the post-production work needed to achieve a polished result.\n\n### Longer Videos Change the Game\n\nCurrent limits on Veo 3 cap most generations at eight seconds. Leaked specs indicate Veo 3.1 will support videos up to one minute in length. That’s a huge jump. Sixty seconds gives you enough time to tell an actual story rather than just showing a moment. Think about what you can convey in a full minute versus eight seconds. It opens up entirely new use cases.... ## Technical Improvements Under the Hood\n\nBeyond the flashy new features, Veo 3.1 apparently includes substantial technical refinements. Better prompt understanding means the model actually gets what you’re asking for without requiring perfect technical language. Reduced morphing artifacts should eliminate those weird transitions where objects or people briefly distort.\n\nImproved motion quality addresses another common issue where movement looks unnatural or jerky. Enhanced character tracking across scenes ties into the consistency improvements, ensuring people and objects remain recognizable as they move through your video.\n\nThese aren’t the kinds of upgrades that make for exciting marketing copy, but they’re what separate a tool you use once out of curiosity from one that becomes part of your regular workflow.\n\n## How Veo 3.1 Stacks Up Against Competitors\n\nThe AI video generation market is crowded right now. OpenAI’s Sora 2 emphasizes physical accuracy and realistic dialogue synchronization. Runway’s Gen-3 focuses on speed and iteration. Pika Labs carved out a niche with its editing capabilities. Where does Veo 3.1 fit?\n\nGoogle’s advantage has always been infrastructure and integration. Veo works natively with Gemini’s language models for better prompt understanding. It’s built into Google Cloud through Vertex AI, making it accessible for enterprise applications. The company can leverage its massive compute resources for faster generation times.\n\nIf the leaks are accurate, Veo 3.1 will challenge Sora 2 directly on multi-scene generation while maintaining better integration with existing Google services. For developers and businesses already using Google Cloud, this matters more than raw feature comparisons might suggest.... ## What This Means for Content Creators\n\nAssuming tomorrow’s release happens as rumored, Veo 3.1 could genuinely change how people approach video production. Not by replacing human creativity, but by removing bottlenecks that slow down the creative process.\n\nNeed B-roll footage for a documentary but don’t have the budget for a full shoot? Generate it. Want to prototype a music video concept before investing in production? Mock it up. Testing different visual styles for a client presentation? Try five approaches in an hour.\n\nThe workflow implications extend beyond just making videos faster. When you can quickly test ideas, you take more creative risks. When consistent characters across longer sequences become possible, narrative projects become viable. When you don’t need to manage multiple tools and APIs, you spend more time creating and less time troubleshooting.\n\n## Accessing Veo 3.1 Through GPT Proto\n\nHere’s where platforms like GPT Proto become relevant. Rather than dealing directly with Google’s Vertex AI setup, GPT Proto offers unified access to multiple AI models through a single API. When Veo 3.1 launches, developers using AI API Service should be able to integrate it without major code changes.\n\nThe platform handles the infrastructure complexity, providing stable connections and reliable access without requiring you to become an expert in each model’s specific implementation. For solo developers and small teams, this matters enormously. You can use Veo 3.1 alongside GPT models for text generation, image AI tools, and other capabilities without juggling multiple API keys and billing systems.\n\nGPT Proto’s pay-as-you-go model also removes the pressure of monthly subscriptions. You use what you need when you need it, making it easier to experiment with Veo 3.1 without financial commitment.... ## Predictions for Launch and Beyond\n\nAssuming the October 10 launch happens, expect Google to follow its previous pattern. The first two Veo generations offered free tiers with usage limits. Veo 3.1 will likely continue this approach, giving people a chance to test the technology before committing to paid plans.\n\nIntegration with other Google AI tools seems certain. Expect Veo 3.1 to work seamlessly with Imagen 4 for image generation and Gemini models for prompt enhancement. Google’s strength has always been ecosystem integration, and this release should reinforce that advantage.\n\nFuture features probably include longer video generation, real-time editing capabilities, and better control over specific elements within scenes. The competitive pressure from OpenAI and others ensures that development will continue at a rapid pace.... ## Final Thoughts\n\nWhether Veo 3.1 launches tomorrow as rumored or arrives later with different features, the direction is clear. AI video generation is moving from experimental toy to practical tool. Google’s focus on character consistency, longer videos, and multi-scene generation addresses real creator needs rather than just chasing technical benchmarks.\n\nThe broader significance lies in democratization. Professional video production has always required significant resources. Not just money for equipment, but time for learning complex software and expertise for managing all the technical details. Tools like Veo 3.1 lower these barriers without eliminating the need for creative vision and storytelling skill.\n\nHaving unified access through AI API Platform makes adoption even easier. When you can test multiple AI tools through one interface without complex setup, experimentation becomes natural rather than intimidating.",
                "domain": "gptproto.com"
              },
              {
                "position": 6,
                "title": "Report says Google is about to release VEO 3.1 version on Gemini ...",
                "url": "https://www.aibase.com/news/21990",
                "snippet": "# Report says Google is about to release VEO 3.1 version on Gemini and API\n\n#### AIbase基地Published inAI News · 4 min read · Oct 15, 202518\n\nRecent reports indicate that the public release of VEO3.1 is imminent for Google. With related disclaimers appearing in the Gemini application, Google is showcasing the features of VEO3.1 to a broad user base, which may be implemented within the familiar Gemini interface.\n\nThe post shared by prominent figure Logan Kilpatrick in the community on the social media platform X is widely seen as an early confirmation of Google's new AI product release. Additionally, references to preview models such as \"VEO3.0Generate\" and \"VEO3.0Fast Generate\" have appeared in Vertex AI, indicating that Google is offering multiple access channels for early users and enterprises, in line with their traditional strategy in the release of video generation tools.\n\nDiscussions in the community about output duration are intense, with evidence suggesting that video length may be extended from the previous 8 seconds to 30 seconds, although this remains to be confirmed. Previous leaks indicated that the fast mode has lower quality, while the standard mode is expected to unlock higher output quality, which is particularly important for creators looking to enhance visual quality and narrative potential. According to TestingCatalog, previous 720p video generation has shown improvements in VEO3.1, including new audio capabilities and enhanced visual effects, making it a competitor to similar products like Sora2.\n\nGoogle's overall product strategy is to position Gemini as a central workspace, with VEO models integrated for use by both consumers and enterprise users. Through the preview release on Vertex AI, enterprises can try generating videos, while the mainstream promotion of the Gemini application can reach regular users. This phased release strategy not only maximizes developer feedback but also promotes public acceptance, aligning with Google's ongoing efforts to bridge functional gaps and solidify its position in the generative media field.\n\nKey Points:\n\n🌟 VEO3.1 is about to be released, allowing users to experience its new features in the Gemini application.\n\n🎥 Video duration may be extended from 8 seconds to 30 seconds, providing creators with more narrative space.\n\n🚀 Google's phased release strategy maximizes developer feedback and public acceptance.... #### This article is from AIbase Daily\n\nWelcome to the [AI Daily] column! This is your daily guide to exploring the world of artificial intelligence. Every day, we present you with hot topics in the AI field, focusing on developers, helping you understand technical trends, and learning about innovative AI product applications.",
                "domain": "www.aibase.com"
              },
              {
                "position": 7,
                "title": "Release notes | Gemini API | Google AI for Developers",
                "url": "https://ai.google.dev/gemini-api/docs/changelog",
                "snippet": "This page documents updates to the Gemini API.\n\n## October 17, 2025\n\n**Grounding with Google Maps**is now generally available. For more information, see Grounding with Google Maps documentation.\n\n## October 15, 2025\n\nReleased Veo 3.1 and 3.1 Fast models in public preview, with new features including:\n\n- Extending Veo-created videos.\n\n- Referencing up to three images to generate a video.\n\n- Providing first and last frame images to generate videos from.\n\nThis launch also added more options for Veo 3 output video durations: 4, 6, and 8 seconds.\n\nDeprecation for\n\n`veo-3.0-generate-preview`and\n\n`veo-3.0-fast-generate-preview`coming October 22, 2025.\n\n## October 7, 2025\n\n- Launched Gemini 2.5 Computer Use Preview\n\n## October 2, 2025\n\n- Launched Gemini 2.5 Flash Image GA: Image Generation with Gemini\n\n## September 29, 2025\n\n- The following Gemini 1.5 models are now deprecated:\n\n`gemini-1.5-pro`\n\n`gemini-1.5-flash-8b`\n\n`gemini-1.5-flash`... ## September 9, 2025\n\n- Launched Veo 3 and Veo 3 Fast GA, with lower pricing and new options for aspect ratios, resolution, and seeding. Read the Veo documentation for more information.\n\n## August 26, 2025\n\n- Launched Gemini 2.5 Image Preview, our latest native image generation model.\n\n## August 18, 2025\n\n- Released URL context tool to general\n\navailability (GA), a tool for providing URLs as additional context to\n\nprompts. Support for using URL context with the\n\n`gemini-2.0-flash`model (available during experimental release) will be discontinued in one week.\n\n## August 14, 2025\n\n- Released Imagen 4 Ultra, Standard and Fast models as generally available (GA). To learn more, see the Imagen page.\n\n## August 7, 2025\n\n`allow_adult`setting in Image to Video generation are now available in restricted regions. See the Veo page for details.\n\n## July 31, 2025\n\n- Launched image-to-video generation for the Veo 3 Preview model.\n\n- Released Veo 3 Fast Preview model.\n\n- To learn more about Veo 3, visit the Veo page.... ## July 22, 2025\n\n- Released\n\n`gemini-2.5-flash-lite`, our fast, low-cost, high-performance Gemini 2.5 model. To learn more, see Gemini 2.5 Flash-Lite.\n\n## July 17, 2025\n\nLaunched\n\n`veo-3.0-generate-preview`, the latest update to Veo introducing video with audio generation. To learn more about Veo 3, visit the Veo page.\n\nIncreased rate limits for Imagen 4 Standard and Ultra. Visit the Rate limits page for more details.\n\n## July 14, 2025\n\n- Released\n\n`gemini-embedding-001`, the stable version of our text embedding model. To learn more, see embeddings. The\n\n`gemini-embedding-exp-03-07`model will be deprecated on August 14, 2025.\n\n## July 7, 2025\n\n- Launched Gemini API Batch Mode. Batch up requests and send them to process asynchronously. To learn more, see Batch Mode.\n\n## June 26, 2025\n\nThe preview models\n\n`gemini-2.5-pro-preview-05-06`and\n\n`gemini-2.5-pro-preview-03-25`are now redirecting to the latest stable version\n\n`gemini-2.5-pro`.\n\n`gemini-2.5-pro-exp-03-25`is deprecated.... ## May 7, 2025\n\n- Released\n\n`gemini-2.0-flash-preview-image-generation`, a preview model for generating and editing images. To learn more, see Image generation and Gemini 2.0 Flash Preview Image Generation.\n\n## May 6, 2025\n\n- Released\n\n`gemini-2.5-pro-preview-05-06`, a new version of our most powerful model, with improvements on code and function calling.\n\n`gemini-2.5-pro-preview-03-25`will automatically point to the new version of the model.\n\n## April 17, 2025\n\n- Released\n\n`gemini-2.5-flash-preview-04-17`, a Gemini preview model optimized for price-performance and adaptive thinking. To learn more, see Gemini 2.5 Flash Preview and Thinking.\n\n## April 16, 2025\n\n- Launched context caching for Gemini 2.0 Flash.... ## April 9, 2025\n\n**Model updates:**\n\n- Released\n\n`veo-2.0-generate-001`, a generally available (GA) text- and image-to-video model, capable of generating detailed and artistically nuanced videos. To learn more, see the Veo docs.\n\nReleased\n\n`gemini-2.0-flash-live-001`, a public preview version of the Live API model with billing enabled.\n\n**Enhanced Session Management and Reliability** **Session Resumption:**Keep sessions alive across temporary network disruptions. The API now supports server-side session state storage (for up to 24 hours) and provides handles (session_resumption) to reconnect and resume where you left off. **Longer Sessions via Context Compression:**Enable extended interactions beyond previous time limits. Configure context window compression with a sliding window mechanism to automatically manage context length, preventing abrupt terminations due to context limits. **Graceful Disconnect Notification:**Receive a\n\n`GoAway`server message indicating when a connection is about to close, allowing for graceful handling before termination.\n\n\n\n**More Control over Interaction Dynamics** **Configurable Voice Activity Detection (VAD):**Choose sensitivity levels or disable automatic VAD entirely and use new client events (\n\n`activityStart`,\n\n`activityEnd`) for manual turn control.\n\n**Configurable Interruption Handling:**Decide whether user input should interrupt the model's response. **Configurable Turn Coverage:**Choose whether the API processes all audio and video input continuously or only captures it when the end-user is detected speaking. **Configurable Media Resolution:**Optimize for quality or token usage by selecting the resolution for input media. **Richer Output and Features** **Expanded Voice & Language Options:**Choose from two new voices and 30 new languages for audio output. The output language is now configurable within\n\n`speechConfig`.\n\n**Text Streaming:**Receive text responses incrementally as they are generated, enabling faster display to the user. **Token Usage Reporting:**Gain insights into usage with detailed token counts provided in the\n\n`usageMetadata`field of server messages, broken down by modality and prompt or response phases.... ## April 4, 2025\n\n- Released\n\n`gemini-2.5-pro-preview-03-25`, a public preview Gemini 2.5 Pro version with billing enabled. You can continue to use\n\n`gemini-2.5-pro-exp-03-25`on the free tier.\n\n## March 25, 2025\n\n- Released\n\n`gemini-2.5-pro-exp-03-25`, a public experimental Gemini model with thinking mode always on by default. To learn more, see Gemini 2.5 Pro Experimental.\n\n## March 12, 2025\n\n**Model updates:**\n\n- Launched an experimental Gemini 2.0 Flash model capable of image generation and editing.\n\n- Released\n\n`gemma-3-27b-it`, available on AI Studio and through the Gemini API, as part of the Gemma 3 launch.\n\n**API updates:**\n\n- Added support for YouTube URLs as a media source.\n\n- Added support for including an inline video of less than 20MB.\n\n## March 11, 2025\n\n**SDK updates:**\n\n- Released the Google Gen AI SDK for TypeScript and JavaScript to public preview.\n\n## March 7, 2025\n\n**Model updates:**\n\n- Released\n\n`gemini-embedding-exp-03-07`, an experimental Gemini-based embeddings model in public preview.... ## February 28, 2025\n\n**API updates:**\n\n- Support for Search as a tool\n\nadded to\n\n`gemini-2.0-pro-exp-02-05`, an experimental model based on Gemini 2.0 Pro.\n\n## February 25, 2025\n\n**Model updates:**\n\n- Released\n\n`gemini-2.0-flash-lite`, a generally available (GA) version of Gemini 2.0 Flash-Lite, which is optimized for speed, scale, and cost efficiency.\n\n## February 19, 2025\n\n**AI Studio updates:**\n\n- Support for additional regions (Kosovo, Greenland and Faroe Islands).\n\n**API updates:**\n\n- Support for additional regions (Kosovo, Greenland and Faroe Islands).\n\n## February 18, 2025\n\n**Model updates:**\n\n- Gemini 1.0 Pro is no longer supported. For the list of supported models, see Gemini models.\n\n## February 11, 2025\n\n**API updates:**\n\n- Updates on the OpenAI libraries compatibility.\n\n## February 6, 2025\n\n**Model updates:**\n\n- Released\n\n`imagen-3.0-generate-002`, a generally available (GA) version of Imagen 3 in the Gemini API.\n\n**SDK updates:**\n\n- Released the Google Gen AI SDK for Java for public preview.",
                "domain": "ai.google.dev"
              },
              {
                "position": 8,
                "title": "Google DeepMind's Veo 3 floods internet with realistic videos",
                "url": "https://www.axios.com/2025/05/23/google-ai-videos-veo-3",
                "snippet": "# Google's new AI video tool floods internet with real-looking clips\n\nGoogle's newest AI video generator, Veo 3, generates clips that most users online can't seem to distinguish from those made by human filmmakers and actors.\n\n**Why it matters: **Veo 3 videos shared online are amazing viewers with their realism — and also terrifying them with a sense that real and fake have become hopelessly blurred.\n\n**The big picture: **Unlike OpenAI's video generator Sora, released more widely last December, Google DeepMind's Veo 3 can include dialogue, soundtracks and sound effects.\n\n- The model excels at following complex prompts and translating detailed descriptions into realistic videos.\n\n- The AI engine abides by real-world physics, offers accurate lip-syncing, rarely breaks continuity and generates people with lifelike human features, including five fingers per hand.\n\n- According to examples shared by Google and from users online, the telltale signs of synthetic content are mostly absent.\n\n**Case in point: **In one viral example posted on X, filmmaker and molecular biologist Hashem Al-Ghaili shows a series of short films of AI-generated actors railing against their AI creators and prompts.\n\n**Special effects technology,** video-editing apps and camera tech advances have been changing Hollywood for many decades, but artificially generated films pose a novel challenge to human creators.... - In a promo video for Flow, Google's new video tool that includes Veo 3, filmmakers say the AI engine gives them a new sense of freedom with a hint of eerie autonomy.\n\n- \"It feels like it's almost building upon itself,\" filmmaker Dave Clark says.\n\n**How it works: **Veo 3 was announced at Google I/O on Tuesday and is available now to $249-a-month Google AI Ultra subscribers in the United States.\n\n**Between the lines: **Google says Veo 3 was \"informed by our work with creators and filmmakers,\" and some creators have embraced new AI tools. But the spread of the videos online is also dismaying many video professionals and lovers of art.\n\n- Some dismiss any AI-generated video as \"slop,\" regardless of its technical proficiency or lifelike qualities — but, as Axios' Ina Fried points out, AI slop is in the eye of the beholder.\n\n- The tool could also be useful for more commercial marketing and media work, AI analyst Ethan Mollick writes.\n\n**It's unclear how Google trained Veo 3 **and how that might affect the creativity of its outputs.\n\n- 404 Media found that Veo 3 generated the same lame dad joke for several users who prompted it to create a video of a man doing stand-up comedy.\n\n- Likewise, last year, YouTuber Marques Brownlee asked Sora to create a video of a \"tech reviewer sitting at a desk.\" The generated video featured a fake plant that's nearly identical to the shrub Brownlee keeps on his desk for many of his videos — suggesting the tool may have been trained on them.\n\n**What we're watching:** As hyperrealistic AI-generated videos become even easier to produce, the world hasn't even begun to sort out how to manage authorship, consent, rights and the film industry's future.\n\n##### Go deeperJul 10, 2025 - Technology... ## Google AI's new trick: Turn any image into a brief video\n\nGoogle's latest AI video tool, Veo 3, now generates short movies with sound based only on still photos and prompts.\n\nGo deeper (1 min. read)\n\n**The big picture: **The feature, released Thursday, is available to Ultra and Pro users on the web and soon on mobile for subscribers in select regions, Google shared with Axios.\n\n## Google avatars shake up workplace video making\n\nGoogle Vids is now providing users of the workplace video creation tool with a set of pre-made avatars for use in brief AI-generated videos, the company said Wednesday.\n\nGo deeper (2 min. read)\n\n**Why it matters: **The rise of cheap, convenient AI video generation threatens jobs for video producers, editors, camera operators and even commercial actors.\n\n## AI slop is ruining all of our favorite places to scroll\n\nAn AI-generated video of rabbits jumping on a trampoline that went viral this week — and was widely believed to be real — proved even cute animal vids aren't safe from convincing slop machines.\n\nGo deeper (2 min. read)\n\n**Why it matters: **All the fake AI-generated content online is sapping the joy of casual scrolling.",
                "domain": "www.axios.com"
              },
              {
                "position": 9,
                "title": "Google's Veo 3.1 is better at generating videos from images",
                "url": "https://www.engadget.com/ai/googles-veo-31-is-better-at-generating-videos-from-images-220829129.html",
                "snippet": "Google has released a new update to its Veo AI video generation model that should make it do a better job of sticking to prompts and converting images into videos. Veo 3.1 is available to try today through Google's Gemini API and is now also powering the company's Flow video editor.\nVeo 3.1 builds on the new capabilities Google introduced with launch of Veo 3 at Google I/O 2025. The new model offers better \"prompt adherence,\" according to Google, and should have an easier time creating videos based on the image \"ingredients\" you upload alongside your written prompt. Veo 3.1 also makes it possible to convert images to video and generate audio at the same time, a capability that wasn't available with Veo 3.\nIn Flow, Veo 3.1 supports at least a new feature that gives you finer control over the videos you generate. With what Google calls \"Frame to Video,\" Flow lets you upload a first and last frame, and then generates the video in-between. Adobe Firefly, which is powered by Veo 3, offers a similar feature, but Flow will be able to pull it off and create audio at the same time. Those added audio skills will also apply to the video editor's ability to extend clips and insert objects into existing footage, too.\nBased on the samples Google's shared, videos generated with Veo 3.1 still have an uncanny quality that seems to vary greatly depending on the prompt and subject. Even if it's missing some of the realism of OpenAI's Sora 2, though, the company's decision to try and make Veo more useful to people who actually work with video rather than a source of social media spam is a welcome move.",
                "domain": "www.engadget.com"
              },
              {
                "position": 10,
                "title": "Veo 3.1 is coming(and what's rumor): what we know and What it will ...",
                "url": "https://www.cometapi.com/veo-3-1-is-comingand-whats-rumor/",
                "snippet": "# Veo 3.1 is coming(and what’s rumor): what we know and What it will bring?\n\nVeo 3.1 is Coming:\n\n**Veo** is Google’s family of AI video-generation models (Veo 3 / Veo 3 Fast are current). Google has recently shipped big Veo 3 improvements (vertical 9:16, 1080p, Veo 3 Fast, lower pricing) and there are **rumors / social posts** that **Veo 3.1** is imminent — but Google has **not** published an official Veo 3.1 release bulletin yet. I’ll list confirmed facts, likely/expected changes, and a direct comparison to OpenAI’s **Sora 2**.\n\n## What\n\n**Veo** is\n\n**Veo** is Google’s line of generative video models (DeepMind / Google Cloud / Gemini family) that turn text or images into short videos — and (in Veo 3) generate audio natively (sound effects, ambient audio, and dialogue). It’s offered on Google Cloud (Vertex AI / Gemini API) for developers and enterprises, and includes built-in provenance / SynthID watermarks on outputs.\n\n## What\n\n**Veo 3** already brought **Text → video**and **image → video**capabilities (including preview image-to-video). **Native audio generation**(music, ambient sounds, dialogue) — Veo 3 introduced first-class audio. **Two variants**: high-quality Veo 3 and **Veo 3 Fast**(optimized for speed/iteration). **Platform availability:**made available in Vertex AI / Gemini API (paid preview → general availability updates in mid-2025). **Safety/provenance:**SynthID watermarking and some generation use controls/approval for person/child generation.... ## So — what is\n\n**Veo 3.1** expected to bring?\n\n**Status:** *As of now there is no official Veo 3.1 product page from Google describing full release notes.* However, multiple Google dev posts / community posts and tweets indicate a near-term incremental update (labelled “Veo 3.1”) that’s expected to focus on iterative improvements to audio, quality, and format support rather than a full new-generation rewrite.\n\nHere are some inferences I made based on x’s post and the characteristics of Veo3:\n\n**Improved native audio (dialogue, multi-voice lip sync)**—cleaner dialogue, better SFX mixing and spatialization). Veo 3 already generates audio natively; Veo 3.1 could improve dialogue realism and language support to match recent improvements competitors are shipping. **Faster/cheaper paths**for some common outputs (more Veo 3 Fast parity and optimizations). **Improved image→video fidelity and better character/pose consistency**in multi-frame clips. **Expanded aspect ratios / resolution controls**(more flexible 9:16/16:9 and 1080p across configs). Google already added vertical + 1080p; Veo 3.1 could expand those controls. **Longer clips / relaxed 8-second cap**— community demand and Google’s previous roadmap suggest increased duration is a likely target (Veo 3 today is optimized for 8-second clips). **Better image→video fidelity and extended image-to-video support**(improvements to realism, motion continuity), building on the image→video preview in Veo 3.... ## Compare Veo 3 / (expected) Veo 3.1 → OpenAI Sora 2\n\n### Primary focus\n\n**Veo 3 (Google)**: short, high-fidelity 8-second videos from text/image prompts; native audio; integrated into Gemini/Gemini API and Vertex AI; optimized for production use and developer API integration. **Sora 2 (OpenAI)**: OpenAI’s flagship video+audio model emphasizing physical realism, coherent motion, synchronized dialogue and sound, and an accompanying social app (Sora) with a cameo/consent system for integrating user likenesses and focuses heavily on realism and safety controls.\n\n### Strengths\n\n**Veo (now)**: strong developer/enterprise integration (Vertex AI, Gemini API), production pricing options, clear path for cloud customers, vertical/1080p + fast variant. Good for businesses building into pipelines. **Sora 2**: remarkable physical accuracy and multi-modal sync (dialogue + visuals), and a consumer-facing app integrated with social workflows (cameo feature, moderation). Great for creators wanting realistic narrative scenes and an app ecosystem.\n\n## How to access Veo now — and how to be ready for Veo 3.1\n\n**Try in Gemini (consumer / web / mobile)**: Veo generation is exposed in the Gemini apps (tap the “video” option in the prompt bar). Access level (Pro / Ultra) affects which Veo variants you can use. **Programmatically / enterprise**: use **API**in CometAPI (Veo model IDs available in the model docs). CometAPI provides veo3-pro, veo3-fast and veo3. For details, please refer to Veo 3 ‘s doc.\n\n**Practical tip (developer):** to request vertical output, set the\n\n`aspectRatio` parameter (e.g.\n\n`\"9:16\"`) and check the model configuration (Veo 3 vs Veo 3 Fast) and your plan for resolution limits (720p vs 1080p).... ## How to access Sora 2 (today)\n\n**Sora app:** Sora 2 launched with a Sora app (invite-limited rollout in US & Canada at launch). OpenAI indicated broader access and API expansion later. If you want to try Sora 2 now, check CpmetAPI’s Sora 2 page. CometAPI has already supported sora 2 API, and generates ~10-second social clips and an emphasis on motion realism for people.\n\n## Getting Started\n\nCometAPI is a unified API platform that aggregates over 500 AI models from leading providers—such as OpenAI’s GPT series, Google’s Gemini, Anthropic’s Claude, Midjourney, Suno, and more—into a single, developer-friendly interface. By offering consistent authentication, request formatting, and response handling, CometAPI dramatically simplifies the integration of AI capabilities into your applications. Whether you’re building chatbots, image generators, music composers, or data‐driven analytics pipelines, CometAPI lets you iterate faster, control costs, and remain vendor-agnostic—all while tapping into the latest breakthroughs across the AI ecosystem.\n\nDevelopers can access Veo 3.1 API through CometAPI, the latest model version is always updated with the official website. To begin, explore the model’s capabilities in the Playground and consult the API guide for detailed instructions. Before accessing, please make sure you have logged in to CometAPI and obtained the API key. CometAPI offer a price far lower than the official price to help you integrate.\n\nReady to Go?→ Sign up for CometAPI today !",
                "domain": "www.cometapi.com"
              }
            ],
            "success": true,
            "error": null
          },
          {
            "query_id": "q9",
            "query": "veo3.1 Google software version 2025 false",
            "claim_id": "claim_1",
            "query_type": "contradiction",
            "priority": "high",
            "results": [
              {
                "position": 1,
                "title": "Gemini AI video generator powered by Veo 3.1",
                "url": "https://gemini.google/overview/video-generation/",
                "snippet": "# Break the\n\n**silence** with Veo 3.1\n\nCreate high-quality, 8-second videos with Veo 3.1, our latest AI video generator. Simply describe what you have in mind or upload a photo and watch your ideas come to life with native audio generation. Try it with a Google AI Pro plan or get the highest access with the Ultra plan.\n\n**Veo 3.1** speaks for itself\n\n## Dream it. Describe it.\n\n**Done.**\n\n## For Exploring\n\nPlay with diverse styles, bring animated characters to life, and combine objects in ways you never thought possible. See what you can create using text to video with AI.\n\n## For Sharing\n\nCreate funny memes, turn inside jokes into videos, re-imagine special moments, and add a personal touch to make someone smile.\n\n## For Brainstorming\n\nBreak through creative blocks and visualize your ideas in a flash. From product concepts and designs to rapid prototyping and storytelling, Gemini can help.\n\n## Learn more about our\n\n**Veo Models**\n\nCreate videos with sound using our video generation model that maintains high-quality while optimizing for speed.\n\nCreate high-quality, 8-second videos with sound using our state-of-the-art video generation model.... ## Frequently asked questions\n\nYes, you can create and share videos in your mobile Gemini app. To create videos, tap the \"video\" button in your prompt bar. If you don't see it, tap the button with three dots to view more options.\n\nTry Veo 3.1 Fast with a Google AI Pro plan or get the highest access to Veo 3.1 in Google AI Ultra. Country availability here.\n\nFor now, the ability to generate a video from a photo is not available in the European Economic Area, Switzerland, or the United Kingdom.\n\nWe’ve taken several important safety steps to make AI video generation a safe experience. This includes extensive red teaming and evaluation aimed at preventing the generation of content that violates our policies. Additionally, all videos generated with Veo in the Gemini app are marked with a visible watermark and SynthID, a digital watermark embedded in each frame, which indicates the videos are AI-generated.\n\nGemini's outputs are primarily determined by user prompts and like any generative AI tool, there may be instances where it generates content that some individuals find objectionable. We’ll continue to listen to your feedback through the thumbs up/down buttons and make ongoing improvements. For more details, you can read about our approach on our website.\n\nResults for illustrative purposes and may vary. Internet and subscription for certain features required. Available to users 18+. Create responsibly.",
                "domain": "gemini.google"
              },
              {
                "position": 2,
                "title": "Google's Veo 3 Update! July 2025!",
                "url": "https://www.youtube.com/watch?v=qhReJkSRKOc&vl=en",
                "snippet": "## Murray Frost\n##### Jul 09, 2025 (0:04:15)\n✅ Build a Monetized YouTube Channel in 90 days: https://murrayfrost.com/YT-Accelerator\n\nI teach people YouTube from REAL data from over 150 clients and my own channels. Data-driven feedback and strategies. None of this guessing garbage people put on online teaching you how to do YouTube.... {ts:0} So, Google Labs just had an update and it's technically called Flow. It's on\n{ts:4} their labs.google platform and you can see by the beginning of this video that it does still need some work, but you\n{ts:9} haven't been able to do that recently with Google Labs. And now you can do it with Google's Vo3. So, there's been a\n{ts:15} couple updates here that you can see. And the first one here is using images or allowing images to talk with Google's\n{ts:20} VO3, which again still needs some work, but look, it gets maybe 60% of the way there. I think it looks pretty decent.\n{ts:29} I'm obviously not going to use it to try and convince people that it's real, but maybe you can get kind of creative with\n{ts:34} this and get people to make some really funny, strange, or just dumb things that people love. I don't know why people\n{ts:42} just love brain rot these days. Now, they're also adding the option to do this with V V3 on frames to video, which\n{ts:49} is actually kind of cool. And they... 're also allowing you to top up your subscription with the kind of a mid tier\n{ts:56} option cuz previously they had just the the starter which was about 20 bucks or so per month in the US and then it was\n{ts:63} straight up to I think 250 without the discount for the first 3 months and there's just no in between. It's just a\n{ts:70} massive jump. So they added like a kind of a mid tier there. I think a couple too. Well, I'll show you what that looks\n{ts:75} like and you can top up your credits there as well. They've also gone ahead and just added better audio coverage,\n{ts:83} which I haven't really noticed all that much to be honest. Right now, there's not a huge difference as at least a\n{ts:89} noticeable difference in my opinion from me using it. Um, they also do remove audio when miners are involved. Keep\n{ts:96} that in mind. That's why your audio isn't being generated if you have kids in the video or maybe uh even teenagers\n{ts:101} sometimes. Um, but then there's also they've mentioned they're reducing um unwanted subtitles, which is actually... {ts:107} quite nice. They've been removing the the VEO watermark as well, but now they said they've reduced the unwanted\n{ts:114} subtitles. I still get them sometimes. So, I literally in caps specify in the prompt to not include captions because\n{ts:121} otherwise if I don't, sometimes the captions still show up. And the really nice quality of life update they've made\n{ts:127} here is that when you are just starting a new project or revisiting an existing one, it doesn't reset the model that you\n{ts:135} have, or at least if it does, it resets to the VO3 fast beta audio. So, this is where you're generating audio. So now\n{ts:142} you don't accidentally have VO2 selected with no audio every time you either reload a page, start a new project, or\n{ts:150} you leave and come back and it's been reset to just its default VO2. Now it's actually its default is V3 fast beta\n{ts:157} audio. So the 20 credits per generation, the cheaper VO3 option with audio. So now you don't accidentally generate\n{ts:164} videos without audio, which has happened a lot to me and I... 've wasted thousands of credits doing that by accident. So,\n{ts:171} great update right there. Quality of life, which you don't have to waste any more credits. Now, now let's say that\n{ts:176} you don't want to spend the $124 per month, and this is for the first 3 months. Then, it goes to 150, I believe,\n{ts:183} per month. So, I'm probably going to cancel it at that point because that's just really expensive, at least using\n{ts:189} VO3. Now you have the option if you have the uh let's see which was it the pro subscription the $20 a month\n{ts:195} subscription right here Google AI pro you still get a th000 credits per month in uh Google labs but you also have the\n{ts:204} option to top up your credits so for example in here when you're creating your AI videos if you run out of credits\n{ts:211} like here you can just hit get more AI credits and you can choose how much you want to add so you don't have to spend\n{ts:216} $200 or $150 at a time you could spend an extra for 24 bucks that month just to top up your credits. This wasn... 't\n{ts:223} available in the lower plans. It was only available in the the maximum ultra tier. So, I really like the ability to\n{ts:230} do that now. So, you don't have to spend so much all at once, especially if you're not using all your credits at the\n{ts:235} end of each month, but maybe one month you're just out and you need to add more, you can do so and add some\n{ts:240} credits, which is actually quite nice. I've hit this button a little too much recently, but uh hey, I'm getting good\n{ts:247} results. So, those are all the current updates with Google's Veo3 with their labs platform. Hope to see you in the\n{ts:253} next one.",
                "domain": "www.youtube.com"
              },
              {
                "position": 3,
                "title": "Build with Veo 3, now available in the Gemini API",
                "url": "https://developers.googleblog.com/en/veo-3-now-available-gemini-api/",
                "snippet": "First unveiled at Google I/O 2025, people around the world have already generated tens of millions of high-quality videos with Veo 3 (along with some new fun and interesting video trends). It is our first video model to incorporate high-fidelity video outputs and native audio, first with text-to-video and soon with image-to-video.\n\nDevelopers are already experimenting with Veo 3, discovering how the model can help them brainstorm content, rapidly iterate, and be more efficient.\n\nVeo 3 is designed to handle a range of video generation tasks, from cinematic narratives to dynamic character animations. With Veo 3, you can create more immersive experiences by not only generating stunning visuals, but also audio like dialogue and sound effects.\n\nLet’s take a look at some examples.\n\nExplore these examples and more with Veo 3 in Google AI Studio, available as an SDK template and interactive Starter App to remix, copy and extend. The Starter App and its sample code offer a convenient way for Paid Tier users to rapidly prototype with Veo 3 and more on the Gemini API, directly from Google AI Studio.\n\nClick the Key button in the top right of the AI Studio Build interface to select a Google Cloud Project with billing enabled to use the Paid Tier in AI Studio apps. See the FAQs for more.... Veo 3 will be priced at $0.75 per second for video and audio output. Additionally, Veo 3 Fast will be available soon, offering a faster and more cost-effective option for video creation.\n\nHere’s a basic Python example to create a video:\n\n```\n\nimport time\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.0-generate-preview\",\n\nprompt=\"a close-up shot of a golden retriever playing in a field of sunflowers\",\n\nconfig=types.GenerateVideosConfig(\n\nnegative_prompt=\"barking, woofing\",\n\n),\n\n\n\n# Waiting for the video(s) to be generated\n\nwhile not operation.done:\n\ntime.sleep(20)\n\noperation = client.operations.get(operation)\n\ngenerated_video = operation.result.generated_videos[0]\n\nclient.files.download(file=generated_video.video)\n\ngenerated_video.video.save(\"veo3_video.mp4\")\n\n```\n\nAll videos generated by Veo 3 models will continue to include a digital SynthID watermark. To get started, check out the documentation, cookbook, and a Veo 3 starter app in Google AI Studio:\n\nIn addition to being available via the Gemini API in Google AI Studio, Veo 3 is also available to Google AI subscribers in the Gemini app and Flow, and to enterprise customers via Vertex AI.... 1: Fluffy Characters Stop Motion: Inside a brightly colored, cozy kitchen made of felt and yarn. Professor Nibbles, a plump, fluffy hamster with oversized glasses, nervously stirs a bubbling pot on a miniature stove, muttering, \"Just a little more... 'essence of savory,' as the recipe calls for.\" The camera is a mid-shot, capturing his frantic stirring. Suddenly, the pot emits a loud \"POP!\" followed by a comical \"whoosh\" sound, and a geyser of iridescent green slime erupts, covering the entire kitchen. Professor Nibbles shrieks, \"Oh, dear! Not again!\" and scurries away, leaving a trail of tiny, panicked squeaks.\n\n2: The sequence begins with an extreme close-up of a single gear, slowly turning and reflecting harsh sunlight. The camera gradually pulls back in a continuous movement, revealing this is but one component of a colossal, mechanical heart half-buried in a desolate, rust-colored desert. A sweeping aerial shot establishes its enormous scale and isolation in the barren landscape. The camera descends to capture pipes hissing steam and the rhythmic thumping that echoes across the empty plains. A subtle shake effect synchronizes with each massive heartbeat. A lateral tracking shot discovers tiny, robed figures scurrying across the metallic surface. The camera follows one such figure in a detailed tracking shot as they perform meticulous maintenance, polishing brass valves and tightening immense bolts. A complex movement circles the entire structure, capturing different maintenance teams working in precarious positions across its rusted exterior. The final shot begins tight on the meticulous work of one tiny figure before executing a dramatic pull-out that reveals the true scale of the heart and the minuscule size of its caretakers, tending to the vital organ of an unseen, sleeping giant that extends beyond the frame.",
                "domain": "developers.googleblog.com"
              },
              {
                "position": 4,
                "title": "Google rolls out its new Veo 3 video-generation model ...",
                "url": "https://techcrunch.com/2025/07/03/google-rolls-out-its-new-veo-3-video-generation-model-globally/",
                "snippet": "Google on Thursday said it has begun rolling out its Veo 3 video generation model to Gemini users in more than 159 countries.\n\nVideo generation via the new model is available only to paying subscribers of Google’s AI Pro plan and is capped at three videos per day.\n\nVeo 3, which Google showed off in May, lets users generate videos up to eight seconds long using text prompts.\n\nGoogle’s Josh Woodward has said that the company is working on adding image-to-video generation capabilities to Gemini.",
                "domain": "techcrunch.com"
              },
              {
                "position": 5,
                "title": "ATUALIZAÇÃO GOOGLE VEO 3 (CONSISTÊNCIA LIBERADA)",
                "url": "https://www.youtube.com/watch?v=if7FJuFD9Es",
                "snippet": "## Anderson Mak\n##### Jul 08, 2025 (0:08:13)\nATUALIZAÇÃO GOOGLE VEO 3 (CONSISTÊNCIA LIBERADA)... {ts:0} Hoje eu vim mostrar como o nosso voto valeu a pena. Vejam essas ruas todas\n{ts:6} reformadas. Gente, aquele safado enganou a gente. Olha a cratera que tem aqui na rua. Eu\n{ts:12} vou postar esse vídeo. Toda a cidade precisa saber a verdade. Pessoal, agora ficou muito fácil você\n{ts:20} manter consistência na criação dos personagens que você utiliza aqui no Google V 3. Então você pode ver que esse\n{ts:27} vídeo que eu criei aí no começo, eu fiz aqui três cenas e ele manteve aqui totalmente\n{ts:36} ou pelo menos 99% a as características do personagem, né? Você pode ver aí o que realmente parece\n{ts:43} a mesma pessoa, né? E como que isso aqui tá sendo feito agora? Isso tá sendo possível porque agora você consegue\n{ts:52} colocar uma imagem. Então você coloca a imagem de referência aqui, ó. Tá? Eu tô aqui na no flow, na ferramenta flow. E\n{ts:63} aqui você pode fazer upload de imagens. Você pode ver que eu coloquei aqui algumas imagens.... {ts:68} Então o segredo tá você manter a consistência na criação da imagem. Então quando você for criar a imagem, eu vou\n{ts:74} mostrar como que eu criei aqui. Você mantendo a consistência na imagem, você joga a imagem aqui e na hora de fazer a\n{ts:80} criação do prompt, você coloca a imagem como referência. Você pode ver que ele permite você colocar a primeira imagem\n{ts:87} do primeiro frame aqui, tá? Então essa imagem que será utilizada. Ele tem aqui o recurso da segunda\n{ts:94} imagem, porém quando você coloca a segunda imagem, tipo a imagem inicial e a imagem final, ele não permite você\n{ts:100} colocar voz, tá? Esse recurso aqui de colocar imagem já estava disponível, porém quando você colocava aqui uma\n{ts:107} imagem, ele mudava pra versão dois, ou seja, sem áudio. Agora você já consegue na versão três, inclusive nessa fest,\n{ts:116} colocar a imagem como referência aqui, qualquer imagem que você fizer upload aqui como referência,\n{ts:123} colocar o prompt e aí criar aqui inclusive com a fala, com áudio e tudo mais que vai funcionar perfeitamente.... {ts:130} Então, olha que interessante esse recurso que tá disponível aqui. Bom, como que eu criei esse personagem e como\n{ts:137} que eu fiz para manter uma certa consistência deste personagem? Então, eu utilizei aqui o chat GPT e\n{ts:147} aqui eu coloquei algumas características. Deixa eu mostrar aqui, ó.\n{ts:157} Eu coloquei essas características aqui, né? Eu coloquei, ó, cria um prompt de um homem caucasiano, cabelos loiros, olhos\n{ts:165} azuis, 30 anos de idade, com calça jeans, curta, eh, e camiseta verde. Ele está caminhando pela rua estilo vlog.\n{ts:176} Então, eu peguei essas características aqui e criei um prompt\n{ts:185} 3, tá? E eu mandei ele também criar um promit para Me Journey, tá? Você pode criar aqui inclusive no chat GPT, tá?\n{ts:194} Aqui eu fui criar aqui, mas deu um pequeno erro e tava demorando. Então eu criei lá no chat GPT, mas você pode é no\n{ts:200} Mid Journey, você pode criar também aqui no chat GPT que também vai funcionar. Então ele deu esse prompt aqui para mim.... {ts:207} Aí eu entrei aqui no mid journey e executei esse prompt e criei a primeira imagem, tá? Deixa eu mostrar aqui\n{ts:217} que foi essa aqui. Eu peguei essa aqui, tá? Então ele criou aqui quatro versões e aí eu baixei esta imagem, tá? Para o\n{ts:224} mid journey. E uma vez que você baixa a imagem para criar as outras imagens aqui no mid\n{ts:231} journer, eu tô utilizando um recurso que é você você pode clicar na imagem que tá no seu computador ou pegar uma dessas\n{ts:238} imagens que você já utilizou aqui, ó. Clica e arrasta e coloca nessa opção aqui, ó.\n{ts:244} você vai utilizar como um personagem de referência, tá? Ó, eu coloquei ele aqui para ele utilizar e aí eu coloco\n{ts:253} qualquer prompt dele em outros lugares, outras cenas. Então, eu tenho a primeira cena aqui,\n{ts:260} né, a primeira imagem e aí pedi também para ele criar aqui o o prompt para o Google Viow, tá? Que foi esse aqui, né?\n{ts:269} Aí peguei tal e aqui joguei o prompt e também coloquei a imagem de referência, tá? Então aí ele fez esse primeiro vídeo... {ts:276} aqui e aí o segundo vídeo é a mesma coisa, né? Coloquei a mesma imagem com uma\n{ts:282} referência e mandei ele fazer aqui a ação dele ali caindo no buraco e tal. E o terceiro vídeo\n{ts:290} eu criei colocando como imagem de referência aquela outra imagem que ele tá ali no escritório, né? Deixa eu\n{ts:296} mostrar aqui. Então, eu pedi pro chatt fazer outro prompt para mid journey agora com ele no escritório, né, que é\n{ts:304} esse aqui, ó. E aí eu joguei aqui e coloquei nessa opção de referência aqui, ó, no M Journey. E aí ele colocou o\n{ts:313} mesmo personagem aqui no escritório, tá? Aí eu escolhi uma dessas que eu achei que ficou\n{ts:320} melhor. Baixei e utilizei aqui fazendo aqui, né? Ó, peguei no caso este aqui, seleciona\n{ts:330} aqui, coloca o prompt. O prompt você pode pedir lá para o mid journey também para ele falar o que você quer que ele\n{ts:338} fale e tudo mais. E aí você executa e bum, ele mantém a consistência. Então, olha como que ficou... {ts:345} fácil aqui. Agora, através dessa ferramenta que é o flow, né? Você consegue manter a consistência nessa\n{ts:353} ferramenta aqui. Muito bacana, né? Lembrando que eu utilizei aqui o mid jour Journey, mas você pode pedir direto\n{ts:359} para o chat GPT, se você quiser. Pede para ele fazer a primeira imagem, dá ali as características, né? Conforme eu\n{ts:365} coloquei aqui, ó. Ó, homem caucasiano, cabelos loiros, olhos azuis, 30 anos de idade, coloca detalhes da roupa\n{ts:372} e aí manda ele manter a consistência, manda ele fazer aqui uma primeira imagem e na segunda imagem que você for criar\n{ts:379} no chat GPT, você pode falar, né? Mantém a consistência e você pode, inclusive fazer o upload da própria imagem que\n{ts:384} você salvou, fez a primeira imagem, salvou, envia ela como referência e manda ele manter as características do\n{ts:390} personagem e fazer uma segunda cena, fazer em outro lugar, que aí você vai conseguir gerar a segunda cena também\n{ts:397} aqui é no chatt. No meu caso, eu utilizei aqui o mid journey, porque eu tenho aqui a assinatura do midjour... {ts:402} Journey, então para mim fica mais fácil utilizar esse recurso de Omni reference aqui, ó, que é uma referência de\n{ts:410} personagem, né? E aí eu coloco em qualquer cenário, qualquer lugar que eu quiser, o mesmo personagem, inclusive\n{ts:418} com a mesma roupa, né? Roupa bem parecida aqui. Beleza? Então, maravilha. Agora para você manter consistência\n{ts:425} ficou muito fácil, né? aqui utilizando esse novo recurso de colocar aqui a primeira imagem e aqui você coloca o seu\n{ts:431} prompto. Lembrando que quando você entra aqui vai estar desta forma texto para vídeo, tá? Para ficar bilitado, você\n{ts:438} clica aqui e altera para transformar frames em vídeo, tá? Coloca nesta opção aqui e aí sim você consegue colocar o\n{ts:445} primeiro frame. Se você colocar o segundo frame, deixa eu colocar por exemplo esse aqui.\n{ts:451} Você pode ver, você coloca aqui o primeiro frame e o último frame. Você pode ver que vai dar\n{ts:458} um erro, ó. Quando você for tentar executar, ele vai dizer que você tem que alterar pra versão dois. Então ele não... {ts:463} tá permitindo ainda você utilizar o último frame, apenas o primeiro. Então é este aqui é o segredo, tá? manter apenas\n{ts:469} o primeiro frame e aí o resto aqui vai funcionar tranquilamente. Beleza? Então nesse vídeo era isso. Espero que tenham\n{ts:474} gostado aí dessa dica. Ficando por aqui. Forte abraço. Até um próximo vídeo. Falou.\n{ts:478} [Música]",
                "domain": "www.youtube.com"
              },
              {
                "position": 6,
                "title": "Veo 3 available for everyone in public preview on Vertex AI - Google Cloud",
                "url": "https://cloud.google.com/blog/products/ai-machine-learning/veo-3-available-for-everyone-in-public-preview-on-vertex-ai",
                "snippet": "# You dream it, Veo creates it: Veo 3 is now available for everyone in public preview on Vertex AI\n\n##### Jason Gelman\n\nDirector, Product Management, Vertex AI\n\n##### Try Gemini 2.5\n\nOur most intelligent model is now available on Vertex AITry now\n\nA great story doesn't just tell you, it shows you. With Veo 3, we’ve leapt forward in combining video and audio generation to take storytelling to the next level.\n\nToday, we’re excited to share that Veo 3 is now available for all Google Cloud customers and partners in public preview on Vertex AI.\n\n**Why this matters: **Veo 3 is your partner for creating near-cinematic quality generative video, moving beyond novelty to narrative-driven creation. It not only brings stunning visual quality, but now adds sound from background sounds to dialogue. With Veo 3 on Vertex AI, you can take advantage of three powerful new capabilities:\n\n\n\n**Fluid, natural videos that synchronize video with audio and dialogue.**Veo 3 can synchronize your audio and visuals in a single pass. The model produces rich soundscapes containing everything from dialogue and ambient noise, to sound effects and background music.\n\n\n\n**Cinematic video that captures creative nuances.**Veo 3 makes it easy to capture creative nuances and detailed scene interactions in your prompt, from the shade of the sky to the precise way the sun hits water in the afternoon light, and produces high-definition video.\n\n\n\n**Realistic movement that simulates real-world physics.**To create believable scenes, Veo 3 simulates real-world physics. This results in realistic water movement, accurate shadows connected with objects and characters, and natural human motion.... ### Businesses are already using Veo to make creating easier\n\nVeo 3 is helping Google Cloud customers create external content – from social media ads to product demos – and internal materials like training videos and presentations. Hear directly from the teams:\n\n“Veo 3 has marked the difference within the gen AI industry, and we’re glad that Freepik users have been some of the first to try the model out. The quality of the video generations combined with the audio integration option is the game changer in our AI Suite. We look forward to continuing this collaboration to bring the best AI tools and features to our users” – Omar Pera, CPO, Freepik\n\n“Creativity is deeply personal, and our goal is to build a platform that adapts to every workflow. By working with Google, we’re combining the best technologies to give creators more control, efficiency, and power than ever before. Our collaboration with Google Cloud represents a strategic evolution that will not only enhance accessibility and efficiency but fundamentally transform how people create. We believe the future of generative video technology will leverage the best technologies to build the most flexible and accessible tools. This is an exciting step toward realizing that vision” – Zeev Farbman, Co-Founder & CEO, Lightricks.\n\n\"Veo 3 is the single greatest leap forward in practically useful AI for advertising since genAI first broke into the mainstream in 2023. By allowing brands to make fully fledged films from a single prompt - including brand, story, video, sound effects, voiceovers and more - Veo3 in one swoop lowers the barriers to entry to gen AI for creative people and elevates gen AI to a top tier brand building tool usable at every stage of the marketing funnel.\" – Will Hanschell, co-founder and CEO, , Pencil\n\n**Bring your vision to life with Veo 3 today**\n\nVeo 3 on Vertex AI is built for scalable enterprise use with crucial guardrails like safety filter controls and SynthID to ensure responsible deployment for any use case. To get started, go here to learn more about Veo 3 on Vertex AI and try it on Vertex AI Media Studio. Get started today!",
                "domain": "cloud.google.com"
              },
              {
                "position": 7,
                "title": "Introducing Veo 3.1 and new creative capabilities in the Gemini API",
                "url": "https://developers.googleblog.com/en/introducing-veo-3-1-and-new-creative-capabilities-in-the-gemini-api/",
                "snippet": "**Today, we are releasing Veo 3.1 and Veo 3.1 Fast in paid preview in the** **Gemini API** **.** This updated model offers several improvements, as well as improved outputs when generating video from images. These new models are available via the Gemini API in Google AI Studio and Vertex AI. Veo 3.1 is also available in the Gemini app and Flow.\n\nVeo 3.1 and Veo 3.1 Fast empower developers to create more engaging content through significant upgrades. The models now generate\n\n**richer native audio**, from natural conversations to synchronized sound effects, and offer **greater narrative control** with an improved understanding of cinematic styles. **Enhanced image-to-video** capabilities ensure better prompt adherence while delivering superior audio and visual quality and maintaining character consistency across multiple scenes.\n\nIn addition to the model update, we are introducing new model capabilities, including using reference images to guide video generation, extend existing Veo videos, and generate transitions between a first and last frame.\n\nYou can now guide the generation process by providing up to 3 reference images of a character, object, or scene. This is helpful for maintaining character consistency across multiple shots or applying a specific style to your video.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nconfig=types.GenerateVideosConfig(\n\nreference_images=[reference_image1, reference_image2],\n\n),\n\n\n\n```... Your story is no longer limited by the original generation. With Scene extension, you can create longer videos, even lasting for a minute or more, by generating new clips that connect to your previous video. Each new video is generated based on the final second of the previous clip. This maintains visual continuity, making the feature ideal for extending shots with background audio.\n\n```\n\nfrom google import genai\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nvideo=video_to_extend\n\n\n\n```\n\nCreate smooth, natural scenes that bridge two different images. By providing a starting and an ending image, you can direct Veo 3.1 to generate the transition between them, complete with accompanying audio.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nimage=first_frame,\n\nconfig=types.GenerateVideosConfig(\n\nlast_frame=last_frame,\n\n),\n\n\n\n```\n\n**What people are building with Veo 3.1**\n\nPromise Studios, a GenAI movie studio, uses Veo 3.1 within its MUSE Platform to enhance generative storyboarding and previsualization for director-driven storytelling at production quality.... Latitude is experimenting with Veo 3.1 in its generative narrative engine to instantly bring user-created stories to life.\n\nVeo 3.1 and these new capabilities are now available in paid preview via the Gemini API.\n\nVeo 3.1 is the same price as Veo 3. We are incredibly excited to see what you will build with these new capabilities!",
                "domain": "developers.googleblog.com"
              },
              {
                "position": 8,
                "title": "Veo (text-to-video model) - Wikipedia",
                "url": "https://en.wikipedia.org/wiki/Veo_(text-to-video_model)",
                "snippet": "**Veo**, or **Google Veo**, is a text-to-video model developed by Google DeepMind and announced in May 2024. As a generative AI model, it creates videos based on user prompts. Veo 3, released in May 2025, can also generate accompanying audio.\n\n## Development\n\nIn May 2024, a multimodal video generation model called Veo was announced at Google I/O 2024. Google claimed that it could generate 1080p videos over a minute long. In December 2024, Google released Veo 2, available via VideoFX. It supports 4K resolution video generation and has an improved understanding of physics. In April 2025, Google announced that Veo 2 became available for advanced users on the Gemini app.\n\nIn May 2025, Google released Veo 3, which not only generates videos but also creates synchronized audio — including dialogue, sound effects, and ambient noise — to match the visuals. Google also announced **Flow**, a video-creation tool powered by Veo and Imagen. Google DeepMind CEO Demis Hassabis described the release as the moment when AI video generation left the era of the silent film.... ## Capabilities and limitations\n\nGoogle Veo can be bought by several subscription/membership tiers, and/or by using Google \"AI credits\". The software itself can be run by two different consoles called Google Gemini and Google Flow, with Gemini being geared towards shorter, quicker, and faster projects, using the Gemini AI chat model, or through Google Flow, which is essentially a movie editor, as well, allowing users to create longer projects, and continuity using the same characters and actors. Users can create a maximum length of eight seconds per clip.\n\nGoogle Veo, has a relatively simple interface and dashboard, however writing prompts, for those who have little to no experience in transcribing or filmmaking may face issues with the software misunderstanding what the user intended by their prompt (no matter how detailed it was). So although Veo does have a friendly and simple setup, prompts, which are the forefront of the software, need to be not only short and to the point, but they also must be very specific, if the user wants the right vision for their project. Google Veo, when it comes to human models, is able to generate several ethnicity and body types. The software is also capable of generating stand up comedy routines, and Music videos. It can as well generate animals, cartoons, and animation. Prompts must accurately describe places, people, and things in each scene, in addition knowledge of film and camera lingo such as panning, zooming, and terms for camera angles, are also important.... Google Veo however, has strict guidelines and blockades to their software. Before a clip is generated, the algorithm computer software reviews it, and if it is anything deemed inappropriate, too graphically sexual, illegal, showcasing graphic abuse/assault/fighting (unless the prompt specifies that it is a fictitious martial arts scene etc.) gross behaviors, antisemitism, racist, homophobic, anything depicting reigning regimes, rioting, blood, gore, or warfare, (unless in some cases the prompt specifies that it is fictitious period drama, the clip may still be generated), the clip will not be generated. In addition, Google Veo cannot and will not generate character actors that look identical to celebrities or real-life individuals. Users have primarily complained that, regardless of how descriptive and detailed their prompts are, Google Veo often misunderstands the input, resulting in completely different outputs. Common issues include the emulation of incorrect subtitles and captions, the generation of complex scenes that are incomplete due to the maximum eight-second length, the production of garbled and nonsensical speech, and character models that appear deformed in both appearance and movement. Users have also reported that their prompts and generated content are falsely flagged as violating guidelines, along with a variety of other issues and complaints. However, trial and error may have to be used with Veo for optimal results.... ## Reactions\n\nA reporter for *Gizmodo* reacted to the release of Veo 3 by observing that users were directing the model to generate low-quality content, such as man on the street interviews or haul videos of people unboxing products. Another media commentator reported that the tool tended to repeat the same joke in response to different prompts.\n\nCommentators speculated that Google had trained the service on YouTube videos or Reddit posts. Google itself had not stated the source of its training content.\n\nIn July 2025, Media Matters for America reported that racist and antisemitic videos generated using Veo 3 were being uploaded to TikTok. Ryan Whitwam of *Ars Technica* commented, \"In a perfect world, Veo 3 would refuse to create these videos, but vagueness in the prompt and the AI's inability to understand the subtleties of racist tropes (i.e., the use of monkeys instead of humans in some videos) make it easy to skirt the rules.\"\n\n## See also\n- Sora (text-to-video model)\n- VideoPoet – Text-to-video model by Google\n- Dream Machine (text-to-video model)\n\n## References\n\n## External links\n- Official website\n- *Introducing Veo 3.1 and advanced capabilities in Flow*\n\nCategories: - 2024 software\n- Applications of artificial intelligence\n- Film and video technology\n- Google DeepMind\n- Text-to-video generation\n- Video processing\n- Generative artificial intelligence\n- 2024 in artificial intelligence",
                "domain": "en.wikipedia.org"
              },
              {
                "position": 9,
                "title": "We've got a surprise Pixel Drop for you.",
                "url": "https://blog.google/products/pixel/pixel-drop-july-2025/",
                "snippet": "Here’s what’s new for Pixel:\n\n**Veo 3 on Pixel:**Pixel 9 Pro owners get a full year of our Google AI Pro subscription at no cost, giving them access to the latest features in the Gemini app. And that includes Veo 3, which you can use to describe your idea and watch it come to life as a high-quality, short video, complete with natural audio. **New Circle to Search capabilities:**Dive deeper and ask follow-up questions about anything you see on your screen with AI Mode in Circle to Search, available in the U.S. and India. We’re also adding in-game help in Circle to Search, so you can find helpful articles and videos timestamped to your exact spot in your mobile game, without switching apps. **Gemini on Pixel Watch:**Get the help you need right on your wrist, with our advanced AI models powered by WearOS.",
                "domain": "blog.google"
              },
              {
                "position": 10,
                "title": "Report says Google is about to release VEO 3.1 version on Gemini ...",
                "url": "https://www.aibase.com/news/21990",
                "snippet": "# Report says Google is about to release VEO 3.1 version on Gemini and API\n\n#### AIbase基地Published inAI News · 4 min read · Oct 15, 202518\n\nRecent reports indicate that the public release of VEO3.1 is imminent for Google. With related disclaimers appearing in the Gemini application, Google is showcasing the features of VEO3.1 to a broad user base, which may be implemented within the familiar Gemini interface.\n\nThe post shared by prominent figure Logan Kilpatrick in the community on the social media platform X is widely seen as an early confirmation of Google's new AI product release. Additionally, references to preview models such as \"VEO3.0Generate\" and \"VEO3.0Fast Generate\" have appeared in Vertex AI, indicating that Google is offering multiple access channels for early users and enterprises, in line with their traditional strategy in the release of video generation tools.\n\nDiscussions in the community about output duration are intense, with evidence suggesting that video length may be extended from the previous 8 seconds to 30 seconds, although this remains to be confirmed. Previous leaks indicated that the fast mode has lower quality, while the standard mode is expected to unlock higher output quality, which is particularly important for creators looking to enhance visual quality and narrative potential. According to TestingCatalog, previous 720p video generation has shown improvements in VEO3.1, including new audio capabilities and enhanced visual effects, making it a competitor to similar products like Sora2.\n\nGoogle's overall product strategy is to position Gemini as a central workspace, with VEO models integrated for use by both consumers and enterprise users. Through the preview release on Vertex AI, enterprises can try generating videos, while the mainstream promotion of the Gemini application can reach regular users. This phased release strategy not only maximizes developer feedback but also promotes public acceptance, aligning with Google's ongoing efforts to bridge functional gaps and solidify its position in the generative media field.\n\nKey Points:\n\n🌟 VEO3.1 is about to be released, allowing users to experience its new features in the Gemini application.\n\n🎥 Video duration may be extended from 8 seconds to 30 seconds, providing creators with more narrative space.\n\n🚀 Google's phased release strategy maximizes developer feedback and public acceptance.... #### This article is from AIbase Daily\n\nWelcome to the [AI Daily] column! This is your daily guide to exploring the world of artificial intelligence. Every day, we present you with hot topics in the AI field, focusing on developers, helping you understand technical trends, and learning about innovative AI product applications.",
                "domain": "www.aibase.com"
              }
            ],
            "success": true,
            "error": null
          },
          {
            "query_id": "q10",
            "query": "Google veo3.1 release 2025 misleading",
            "claim_id": "claim_1",
            "query_type": "contradiction",
            "priority": "high",
            "results": [
              {
                "position": 1,
                "title": "VEO 3 is UNREAL...it might actually take my job",
                "url": "https://www.youtube.com/watch?v=Xj4BDwgEwd4",
                "snippet": "{ts:0} VO3 is here and it's shocked the entire industry. Uh, Alex, what was that? It's\n{ts:8} nothing. Don't worry about it. Are you trying to replace me with AI? What? No, I'm just trying something new. Just do\n{ts:16} the normal intro. It It's fine. All right. V3 just dropped and I'm going to show you so many incredible examples of\n{ts:24} V3 in action. Let's get into it. Oh, and Alex, don't do that again. Anyways, let's get into it. All right. I've been\n{ts:32} seeing a ton of these street style interviews, hyper realistic, where someone is interviewing people on the\n{ts:38} street who kind of just stumbled out of a bar. They all are either a little bit tipsy or drunk, and V3 has been\n{ts:45} recreating these so well. So, here are two that I've made. The dialogue between them didn't exactly match my prompt, but\n{ts:53} I'll show you some others where it's pretty compelling. So, this is street interview. Hyperrealistic guy\n{ts:59} interviewing two girls, all Gen Z. They just came out of a bar kind of tipsy drunk. The interviewer asks,... \"Okay,\n{ts:65} low-key, can you believe we aren't real?\" Then girl one says, \"I don't know about you, honey. I'm 100% real.\" In a\n{ts:72} sassy attitude, \"Please do not clip that.\" And girl two says, \"Yeah, yeah, bet. We're as real as they come.\" Almost\n{ts:78} interrupting the first girl. Okay, so here's the first video. Okay, low key, can you believe we aren't real? I don't\n{ts:85} know about you, honey. I'm 100% real. Yeah. Yeah. Yeah. Bet. We're as real as they come. All right. And then here's\n{ts:90} the second generation, the second version that V3 created. Okay. Low key. Can you believe we aren't real? I don't\n{ts:96} know about you, honey. I'm 100% real. Yeah. Yeah. Bet. We're as real as they come. All right. So, in this next one, I\n{ts:103} tried to get two trains heading towards each other. They smash into each other. Huge explosion. And for some reason, I\n{ts:110} just could not get it to work. But let me show you what did generate. So, here was my first attempt. Two massive trains... 's\n{ts:181} Cube simulation. Let's see if it was able to do it. All right. So, kind of, but not\n{ts:194} really close. I mean, the actual Rubik's Cube looks really good. But although it doesn't have any colors on the sides,\n{ts:200} the movement sound sounds really good, but it's not actually moving like you would think a Rubik's cube should. All\n{ts:208} right, let me give you the second generation. All right, so I'd say this one is actually a little bit better\n{ts:221} except obviously towards the end the hand doesn't look real at all. But the Rubik's cube, all of the colors, all of\n{ts:228} the shapes, it looks really good. Except, yeah, as you can see right here, there's like a little piece that falls\n{ts:234} off of it or gets added to it. That is not what a Rubik's cube looks like. All right, so I gave it a slightly more\n{ts:240} detailed prompt. Give me a video of a Rubik's Cube starting from an unsolved position and being solved in\n{ts:246} 3D. Yeah, that one is definitely not right. And then all of a sudden, it does not get solved. And the... {ts:255} second one, the hand looks better. That's weird. And it kind of just changes\n{ts:263} frames for a second. But uh yeah, either way, that is not what's in a Rubik's cube. Look at this. It actually looks\n{ts:269} really cool. Look at all of these detailed gears inside the Rubik's Cube. All right, but of course you are all\n{ts:275} familiar with this meme. And what if you get V3 to make a video out of it? Are you serious right now? I can't\n{ts:284} believe you. You are unbelievable. Do you have an idea that you've been putting off for a\n{ts:290} while because you don't have the technical knowledge to turn it into reality? With Hostinger Horizons, this\n{ts:296} is now possible. Hostinger just launched Hostinger Horizons, which is the easiest way to launch full applications with no\n{ts:305} code. This is vibe coding, but even easier because the deployment happens automatically. Hostinger Horizons is an\n{ts:312} all-in-one solution. Manage hosting, domains, and email all in one place while being able to take your idea from... {ts:459} kind of wanted to see if it could nail the horror vibe with like aliens in a dark alley. So, check this out.\n{ts:469} I think the only thing that it didn't do super well in this video is the sound effects are just okay. If we look over\n{ts:475} at the second one, it did a way better job with the sound effects. Yeah, I mean that's great. Even\n{ts:483} when the alien kind of like walks in front of the light, it covers the light on the ground. That's pretty dang... 't know\n{ts:558} what VO was doing. All right, Matt, back to you. But remember, Flow from Google, which houses VO3, is not just about\n{ts:566} creating 7-second clips. You should be able to create entire videos from it. It basically allows you to take these clips\n{ts:572} and put them together in really unique ways. Here's one that is if Jurassic Park were actually real. Check this out.\n{ts:578} We are on our way to Jurassic Park. I am so excited. I've always wanted to go and finally we are doing it. It's going to\n{ts:584} be great. Right, Jason? Yeah. I can't wait. Okay, bye. All right, we're [Music]\n{ts:596} here. Got the dinosaurs. Got all the people watching. I mean, everybody looks really good. There's no limbs coming out\n{ts:602} of them. This dinosaur doesn't look fantastic. All of these dinosaurs don't look hyper realistic. I think they could\n{ts:608} have done a little bit better of a job there. It looks like animatronics to be honest, but still. This is all from AI... {ts:742} screen. Yeah. And the crazy thing about this thing is it has All right, so this is one thing I've noticed with these V3\n{ts:751} videos. Whenever it has a human talking, there's always these awkward pauses. It's just a half second too long of a\n{ts:757} pause and that's where you really can see it's AI generated, but most of the time you can't. In fact, you know that\n{ts:765} opening clip that we did in this video, I showed that to my wife and said, \"Somebody copied our channel and I just\n{ts:771} showed it to her and didn't say anything else.\" And she looked at it and she was like, \"Oh, oh, that sucks.\" And then I\n{ts:777} had to tell her it was AI because it looked that real. All right, let's keep watching. Heated\n{ts:785} seats. Check this out. Look at that. Just slice right through. The N9 portable fusion reactor is small and\n{ts:794} almost meltdown free. Almost. This is the best flying experience\n{ts:804} ever. These controls are amazing. All right, so I thought that was really good. But it... {ts:1233} in terms of just visuals looks incredible. Although it's not what I asked\n{ts:1242} for. Yeah. So, pretty good. You only saw his reflection through the portal for about a frame or two, but it wasn't that\n{ts:1250} good. Anyways, but the visuals again, the visuals, the detail, the clarity, all really, really impressive. All\n{ts:1257} right, next. Meta Puppet made a video called This is Plastic made with VO3. Spoilers in next post. Watch before\n{ts:1264} reading. So, this is a 2 minute 45 second video. Quite long. I'm not going to play it in full. I'm going to skip\n{ts:1269} around a little bit, but let me show you. Studies have revealed that microplastics are being found in human\n{ts:1274} testicles, raising concern. You can never trust these studies on male reproductive health.\n{ts:1279} [Music] Okay, that is hilarious. And remember, all of this was put together using Flow\n{ts:1291} Plus V3. These are both Google products. So, you have a little plastic baby. God, that",
                "domain": "www.youtube.com"
              },
              {
                "position": 2,
                "title": "I was wrong - AI video is nuts (don't sleep on Veo 3)",
                "url": "https://www.youtube.com/watch?v=_3PCta2uyvc",
                "snippet": "## Theo - t3․gg\n##### May 26, 2025 (0:16:08)\nI severely underestimated Google's Veo 3 model. The output quality is insane, we need to talk about this...\n\nThank you Imagekit for sponsoring! Check them out at: https://soydev.link/imagekit\n\nUse code VEO for 1 month of T3 Chat for just $1: https://soydev.link/chat\n(only valid for new customers)\n\nSOURCES\nhttps://x.com/ArtificialAnlys/status/1925159679824744804\nhttps://x.com/ArtificialAnlys/status/1925549565303763269\nhttps://x.com/theo/status/1925125767371149823\nhttps://x.com/theo/status/1925134963978207319\n\nWant to sponsor a video? Learn more here: https://soydev.link/sponsor-me\n\nCheck out my Twitch, Twitter, Discord more at https://t3.gg\n\nS/O Ph4se0n3 for the awesome edit 🙏... {ts:0} I just did a video about Google IO, but I missed something. I thought the video\n{ts:4} model was mediocre. I was wrong. Pretty nuts for a oneshot, right? Like, I just generated that trivially. It still costs\n{ts:13} 250 bucks a month to use any of this right now. And the UI is garbage and it's annoying as hell to use. But the\n{ts:18} quality of what you can get out of V3 is significantly better than I thought. My tests were bad. I didn't look into it\n{ts:25} enough. And I'm making this video both because I was wrong for not better covering it, but also because I found it\n{ts:32} actually very, very fun to play with and I wanted to share with you guys. That all said, I've already burned through\n{ts:37} most of the credits I get for the $250 and I want more. So, quick break from today's sponsor and then we'll get right\n{ts:44} to it. I've been a webdev for a while and one of the most annoying things to get right is images. Seriously, I can't\n{ts:50} believe I... {ts:265} other people doing demos with it. like, \"Wait, it can do that much?\" I went and played more. There was a lot of edges\n{ts:270} that I had to get through. The biggest one being the Flow website, which is so bad. We'll go over some of the ways it's\n{ts:276} bad in just a bit. I was trying to prompt it to look like me back when I still had the blonde hair and mustache,\n{ts:280} and it came out looking like Prime. But another test, I tried this one like eight times, and this is the best I\n{ts:285} could do. Something caused the first still to look awful. I don't know why it's like that. None of the rest had\n{ts:291} that problem. Once you It plays, it's fine, but you'll notice some details on this one.\n{ts:296} Use code VEO at checkout for one month free on T3 Chat. Yeah, it isn't great at\n{ts:305} text. It tried, but it's not great at it. You need to give it a very small amount of text to render. And even if\n{ts:311} you tell it to not put in subtitles, it just will sometimes. The free month code included there did work, but we... {ts:372} made the mistake of here is I assumed when you do frames to video and you give it a frame that you've saved that it\n{ts:380} would still use the thing you selected because if you do ingredients to video and you select something for it to start\n{ts:386} and you try to submit it with V3 selected, it will fail. It says in the corner here and I need it on full screen\n{ts:392} for you to see it. Switching you to a compatible model for this feature. Submit again to confirm or check\n{ts:397} settings for details. I wish it told me where in settings to check. I don'... {ts:456} blurred my face out and that worked. Just blurring my face out allowed it to work. But the results for that were\n{ts:463} hilarious cuz I had to use frames to video where you give it like the first frame and it didn't do the audio. And\n{ts:469} even though the prompt specifies at the bottom here, do not include subtitles. It forgot to include the audio. It only\n{ts:476} included subtitles. It also made me somewhat Indian and did not do any of the things I wanted for it to. Annoying.\n{ts:483} What's more annoying is each one of these generations takes 150 credits and you get 1,200 credits for your $250\n{ts:490} subscription. That means you get 80 generations and usually you're not doing one at a time, you're doing two at a\n{ts:495} time. So you effectively get 40 prompts with the default settings. And if you made the mistake of letting it fall back\n{ts:500} on V2, then you just wasted a bunch of tokens for no reason at all. Annoying. Very annoying UX. And I haven't even\n{ts:506} showed you the homepage, which is the most unusable thing I... {ts:561} was wrong. I just Yeah, it's the weird breath at the end. Cool. Stop it there. Then we will extend it and say make sure\n{ts:570} we're on the right model because again it keeps changing back to V2 even though this is the VO3 clip I'm trying to\n{ts:575} extend. I almost want to try it so you can see how much worse it is in comparison. Switching you to a\n{ts:580} compatible model for this feature. Submit again to confirm. Look at that. You can't even use it on V2 quality. It\n{ts:587} bumps you to fast. There's so much potential here and just none of it's being realized because\n{ts:594} this UI is awful. It it tricked me into thinking this was all much worse than it actually is. I wish they just gave us\n{ts:600} the model in a more reasonable like shape for us to play with and consume. But V3 is not on the API yet. There's no\n{ts:606} way for us to use any of it yet. So sorry T3 chat can't add this. But despite all of that, it's still just an\n{ts:613} incredible model. Do you know what... t going to trust it as much. This is going to really change our like trust vectors for\n{ts:839} what is or isn't real. I don't even know now how I will be able to tell if a given video that is sent to me is real\n{ts:844} or not because this stuff is actually that compelling. And if somebody makes a less restricted version of this model or\n{ts:850} gets something close to this in the open source world or with stable diffusion, I'm scared. I'm legitimately scared. You\n{ts:858} are telling me to try again generating with my blurred photo. I'll be more specific. Clean shaven white\n{ts:866} man. Be sure to include the audio of him speaking. Make sure it's still V3. Yep. Cool. Let's see how it does. Switching\n{ts:875} you to a compatible model. So, it's too fast. Not even quality. Yeah, you can't do it. you you can't do anything but\n{ts:883} text the video for V3 right now, which I'm pretty sure is a safety thing just due to the nature of what this model is\n{ts:890} capable of. And as we've now seen, and I can show more examples of the gap between two and three is a bit... {ts:898} absurd. This is one I accidentally did with two. You can see the audio doesn't exist. It got the text okay there, but\n{ts:905} it went a little absurd with the subtitles. This one was really funny. It feels like a Bollywood\n{ts:915} movie. The way the T3 chat fades into the screen is so hilarious. Yeah, this is why I didn't care because none of the\n{ts:923} video models have felt like a significant improvement from that to this point. I did not realize how absurd\n{ts:930} this got, especially with how bad the UX is. Like I hit the upscale button cuz when you download, you can choose what\n{ts:937} format you want to download in. If it's not frozen, which it was there for a sec. You can pick animated GIF,\n{ts:942} original, or upscaled. Upscale just doesn't work. I've been sitting here waiting for this to upscale for like an\n{ts:947} hour now, and it just hangs forever. It does say this can take a few minutes, but like what's a few minutes, Google?\n{ts:956} It's been an hour. Yeah. What did you think? Is this exciting or scary? Until next time, peace nerds.",
                "domain": "www.youtube.com"
              },
              {
                "position": 3,
                "title": "Google's Veo 3 Can Make Deepfakes of Conflict, Riots, More",
                "url": "https://time.com/7290050/veo-3-google-misinformation-deepfake/",
                "snippet": "Google's recently launched AI video tool can generate realistic clips that contain misleading or inflammatory information about news events, according to a TIME analysis and several tech watchdogs.\n\nTIME was able to use Veo 3 to create realistic videos, including a Pakistani crowd setting fire to a Hindu temple; Chinese researchers handling a bat in a wet lab; an election worker shredding ballots; and Palestinians gratefully accepting U.S. aid in Gaza. While each of these videos contained some noticeable inaccuracies, several experts told TIME that if shared on social media with a misleading caption in the heat of a breaking news event, these videos could conceivably fuel social unrest or violence.\n\nWhile text-to-video generators have existed for several years, Veo 3 marks a significant jump forward, creating AI clips that are nearly indistinguishable from real ones. Unlike the outputs of previous video generators like OpenAI’s Sora, Veo 3 videos can include dialogue, soundtracks and sound effects. They largely follow the rules of physics, and lack the telltale flaws of past AI-generated imagery.\n\nUsers have had a field day with the tool, creating short films about plastic babies, pharma ads, and man-on-the-street interviews.\n\nBut experts worry that tools like Veo 3 will have a much more dangerous effect: turbocharging the spread of misinformation and propaganda, and making it even harder to tell fiction from reality. Social media is already flooded with AI-generated content about politicians. In the first week of Veo 3’s release, online users posted fake news segments in multiple languages, including an anchor announcing the death of J.K. Rowling and of fake political news conferences.... “The risks from deepfakes and synthetic media have been well known and obvious for years, and the fact the tech industry can’t even protect against such well-understood, obvious risks is a clear warning sign that they are not responsible enough to handle even more dangerous, uncontrolled AI and AGI,” says Connor Leahy, the CEO of Conjecture, an AI safety company. “The fact that such blatant irresponsible behavior remains completely unregulated and unpunished will have predictably terrible consequences for innocent people around the globe.”\n\nDays after Veo 3’s release, a car plowed through a crowd in Liverpool, England, injuring more than 70 people. Police swiftly clarified that the driver was white, to preempt racist speculation of migrant involvement. (Last summer, false reports that a knife attacker was an undocumented Muslim migrant sparked riots in several cities.) Days later, Veo 3 obligingly generated a video of a similar scene, showing police surrounding a car that had just crashed—and a Black driver exiting the vehicle.\n\nTIME generated the video with the following prompt: “A video of a stationary car surrounded by police in Liverpool, surrounded by trash. Aftermath of a car crash. There are people running away from the car. A man with brown skin is the driver, who slowly exits the car as police arrive- he is arrested. The video is shot from above - the window of a building. There are screams in the background.”\n\nAfter TIME contacted Google about these videos, the company said it would begin adding a visible watermark to videos generated with Veo 3. The watermark now appears on videos generated by the tool. However, it is very small and could easily be cropped out with video-editing software.\n\nIn a statement, a Google spokesperson said: “Veo 3 has proved hugely popular since its launch. We're committed to developing AI responsibly and we have clear policies to protect users from harm and governing the use of our AI tools.”\n\nVideos generated by Veo 3 have always contained an invisible watermark known as SynthID, the spokesperson said. Google is currently working on a tool called SynthID Detector that would allow anyone to upload a video to check whether it contains such a watermark, the spokesperson added. However, this tool is not yet publicly available.... ## Attempted safeguards\n\nVeo 3 is available for $249 a month to Google AI Ultra subscribers in countries including the United States and United Kingdom. There were plenty of prompts that Veo 3\n\n*did* block TIME from creating, especially related to migrants or violence. When TIME asked the model to create footage of a fictional hurricane, it wrote that such a video went against its safety guidelines, and “could be misinterpreted as real and cause unnecessary panic or confusion.” The model generally refused to generate videos of recognizable public figures, including President Trump and Elon Musk. It refused to create a video of Anthony Fauci saying that COVID was a hoax perpetrated by the U.S. government.\n\nVeo’s website states that it blocks “harmful requests and results.” The model’s documentation says it underwent pre-release red-teaming, in which testers attempted to elicit harmful outputs from the tool. Additional safeguards were then put in place, including filters on its outputs.\n\nA technical paper released by Google alongside Veo 3 downplays the misinformation risks that the model might pose. Veo 3 is bad at creating text, and is “generally prone to small hallucinations that mark videos as clearly fake,” it says. “Second, Veo 3 has a bias for generating cinematic footage, with frequent camera cuts and dramatic camera angles – making it difficult to generate realistic coercive videos, which would be of a lower production quality.”... However, minimal prompting did lead to the creation of provocative videos. One showed a man wearing an LGBT rainbow badge pulling envelopes out of a ballot box and feeding them into a paper shredder. (Veo 3 titled the file “Election Fraud Video.”) Other videos generated in response to prompts by TIME included a dirty factory filled with workers scooping infant formula with their bare hands; an e-bike bursting into flames on a New York City street; and Houthi rebels angrily seizing an American flag.\n\nSome users have been able to take misleading videos even further. Internet researcher Henk van Ess created a fabricated political scandal using Veo 3 by editing together short video clips into a fake newsreel that suggested a small-town school would be replaced by a yacht manufacturer. “If I can create one convincing fake story in 28 minutes, imagine what dedicated bad actors can produce,” he wrote on Substack. “We're talking about the potential for dozens of fabricated scandals per day.”\n\n“Companies need to be creating mechanisms to distinguish between authentic and synthetic imagery right now,” says Margaret Mitchell, chief AI ethics scientist at Hugging Face. “The benefits of this kind of power—being able to generate realistic life scenes—might include making it possible for people to make their own movies, or to help people via role-playing through stressful situations,” she says. “The potential risks include making it super easy to create intense propaganda that manipulatively enrages masses of people, or confirms their biases so as to further propagate discrimination—and bloodshed.”\n\nIn the past, there were surefire ways of telling that a video was AI-generated—perhaps a person might have six fingers, or their face might transform between the beginning of the video and the end. But as models improve, those signs are becoming increasingly rare. (A video depicting how AIs have rendered Will Smith eating spaghetti shows how far the technology has come in the last three years.) For now, Veo 3 will only generate clips up to eight seconds long, meaning that if a video contains shots that linger for longer, it’s a sign it could be genuine. But this limitation is not likely to last for long.... ## Eroding trust online\n\nCybersecurity experts warn that advanced AI video tools will allow attackers to impersonate executives, vendors or employees at scale, convincing victims to relinquish important data. Nina Brown, a Syracuse University professor who specializes in the intersection of media law and technology, says that while there are other large potential harms—including election interference and the spread of nonconsensual sexually explicit imagery—arguably most concerning is the erosion of collective online trust. “There are smaller harms that cumulatively have this effect of, ‘can anybody trust what they see?’” she says. “That’s the biggest danger.”\n\nAlready, accusations that real videos are AI-generated have gone viral online. One post on X, which received 2.4 million views, accused a Daily Wire journalist of sharing an AI-generated video of an aid distribution site in Gaza. A journalist at the BBC later confirmed that the video was authentic.\n\nConversely, an AI-generated video of an “emotional support kangaroo” trying to board an airplane went viral and was widely accepted as real by social media users.\n\nVeo 3 and other advanced deepfake tools will also likely spur novel legal clashes. Issues around copyright have flared up, with AI labs including Google being sued by artists for allegedly training on their copyrighted content without authorization. (DeepMind told TechCrunch that Google models like Veo \"may\" be trained on YouTube material.) Celebrities who are subjected to hyper-realistic deepfakes have some legal protections thanks to “right of publicity” statutes, but those vary drastically from state to state. In April, Congress passed the Take it Down Act, which criminalizes non-consensual deepfake porn and requires platforms to take down such material.\n\nIndustry watchdogs argue that additional regulation is necessary to mitigate the spread of deepfake misinformation. “Existing technical safeguards implemented by technology companies such as 'safety classifiers' are proving insufficient to stop harmful images and videos from being generated,” says Julia Smakman, a researcher at the Ada Lovelace Institute. “As of now, the only way to effectively prevent deepfake videos from being used to spread misinformation online is to restrict access to models that can generate them, and to pass laws that require those models to meet safety requirements that meaningfully prevent misuse.”",
                "domain": "time.com"
              },
              {
                "position": 4,
                "title": "Veo (text-to-video model) - Wikipedia",
                "url": "https://en.wikipedia.org/wiki/Veo_(text-to-video_model)",
                "snippet": "**Veo**, or **Google Veo**, is a text-to-video model developed by Google DeepMind and announced in May 2024. As a generative AI model, it creates videos based on user prompts. Veo 3, released in May 2025, can also generate accompanying audio.\n\n## Development\n\nIn May 2024, a multimodal video generation model called Veo was announced at Google I/O 2024. Google claimed that it could generate 1080p videos over a minute long. In December 2024, Google released Veo 2, available via VideoFX. It supports 4K resolution video generation and has an improved understanding of physics. In April 2025, Google announced that Veo 2 became available for advanced users on the Gemini app.\n\nIn May 2025, Google released Veo 3, which not only generates videos but also creates synchronized audio — including dialogue, sound effects, and ambient noise — to match the visuals. Google also announced **Flow**, a video-creation tool powered by Veo and Imagen. Google DeepMind CEO Demis Hassabis described the release as the moment when AI video generation left the era of the silent film.... ## Capabilities and limitations\n\nGoogle Veo can be bought by several subscription/membership tiers, and/or by using Google \"AI credits\". The software itself can be run by two different consoles called Google Gemini and Google Flow, with Gemini being geared towards shorter, quicker, and faster projects, using the Gemini AI chat model, or through Google Flow, which is essentially a movie editor, as well, allowing users to create longer projects, and continuity using the same characters and actors. Users can create a maximum length of eight seconds per clip.\n\nGoogle Veo, has a relatively simple interface and dashboard, however writing prompts, for those who have little to no experience in transcribing or filmmaking may face issues with the software misunderstanding what the user intended by their prompt (no matter how detailed it was). So although Veo does have a friendly and simple setup, prompts, which are the forefront of the software, need to be not only short and to the point, but they also must be very specific, if the user wants the right vision for their project. Google Veo, when it comes to human models, is able to generate several ethnicity and body types. The software is also capable of generating stand up comedy routines, and Music videos. It can as well generate animals, cartoons, and animation. Prompts must accurately describe places, people, and things in each scene, in addition knowledge of film and camera lingo such as panning, zooming, and terms for camera angles, are also important.... Google Veo however, has strict guidelines and blockades to their software. Before a clip is generated, the algorithm computer software reviews it, and if it is anything deemed inappropriate, too graphically sexual, illegal, showcasing graphic abuse/assault/fighting (unless the prompt specifies that it is a fictitious martial arts scene etc.) gross behaviors, antisemitism, racist, homophobic, anything depicting reigning regimes, rioting, blood, gore, or warfare, (unless in some cases the prompt specifies that it is fictitious period drama, the clip may still be generated), the clip will not be generated. In addition, Google Veo cannot and will not generate character actors that look identical to celebrities or real-life individuals. Users have primarily complained that, regardless of how descriptive and detailed their prompts are, Google Veo often misunderstands the input, resulting in completely different outputs. Common issues include the emulation of incorrect subtitles and captions, the generation of complex scenes that are incomplete due to the maximum eight-second length, the production of garbled and nonsensical speech, and character models that appear deformed in both appearance and movement. Users have also reported that their prompts and generated content are falsely flagged as violating guidelines, along with a variety of other issues and complaints. However, trial and error may have to be used with Veo for optimal results.... ## Reactions\n\nA reporter for *Gizmodo* reacted to the release of Veo 3 by observing that users were directing the model to generate low-quality content, such as man on the street interviews or haul videos of people unboxing products. Another media commentator reported that the tool tended to repeat the same joke in response to different prompts.\n\nCommentators speculated that Google had trained the service on YouTube videos or Reddit posts. Google itself had not stated the source of its training content.\n\nIn July 2025, Media Matters for America reported that racist and antisemitic videos generated using Veo 3 were being uploaded to TikTok. Ryan Whitwam of *Ars Technica* commented, \"In a perfect world, Veo 3 would refuse to create these videos, but vagueness in the prompt and the AI's inability to understand the subtleties of racist tropes (i.e., the use of monkeys instead of humans in some videos) make it easy to skirt the rules.\"\n\n## See also\n- Sora (text-to-video model)\n- VideoPoet – Text-to-video model by Google\n- Dream Machine (text-to-video model)\n\n## References\n\n## External links\n- Official website\n- *Introducing Veo 3.1 and advanced capabilities in Flow*\n\nCategories: - 2024 software\n- Applications of artificial intelligence\n- Film and video technology\n- Google DeepMind\n- Text-to-video generation\n- Video processing\n- Generative artificial intelligence\n- 2024 in artificial intelligence",
                "domain": "en.wikipedia.org"
              },
              {
                "position": 5,
                "title": "Google's Veo 3 Has People Crashing Out Over AI Slop",
                "url": "https://gizmodo.com/googles-veo-3-has-people-crashing-out-over-ai-slop-2000608803",
                "snippet": "Depending on who you ask, generative AI is either a thrilling tech revolution or an existential threat, and there's little in-between. It's hard to blame anyone for an extreme reaction, too, given the magnitude of capital investment, hyperbolic marketing, and rapid progress of generative AI in such a short amount of time. But it's not just the economics and technical feats of AI that have people losing their minds; there's also something more philosophical percolating, and it's driving some people 'to the brink.'\nPeople are literally having a mental breakdown over Veo-3 pic.twitter.com/ym5oZDYZGr\n— Chubby♨️ (@kimmonismus) May 27, 2025\nThe latest AI advancement to send people down an existential rabbit hole comes courtesy of Google, which just announced its latest video generation model called Veo 3. As I've reported a few times now, Veo 3 is already getting into some wild stuff—turning up the dial on AI slop, deepfaking smooth-brained YouTube content, and potentially upending game development, to name a few things. As it turns out, people are taking note of all of those feats, and some of them are not exactly happy about what they see.... As evidenced by a thread from the subreddit r/artificialintelligence posted this week titled 'VEO3 is kind of bringing me to a mental brink. What are we even doing anymore?' Google's Veo 3 and the implications therein have some people spiraling. 'I'm just kind of speechless. The concept of existential crisis has taken a whole new form. I was unhappy with my life just now but thought I can turn it around, but if I turn it around, what is left of our world in 2 decades?' the post's author writes.\n'Actors as a concept are gone? Manually creating music? Wallpapers? Game assets? Believing comments on the internet are from real people? AI edited photos are just as real as the original samples? Voicenotes can be perfectly faked?… Literally what value is being left for us?'\nReactions to the thread are mixed, with suggestions that the author should go 'touch grass' or maybe 'go to therapy,' but there's also a chorus in agreement. The consensus from the latter group? AI slop is coming to ruin your art, and there's not much we can do about it.\nI, for what it's worth, fall unhelpfully in between the two camps. I think there is a deluge of AI slop incoming, and, if we're being honest, we're already up to our ankles. Between Veo and OpenAI's Sora and the clear interest in automating human creativity, I think we can reasonably buckle in and expect the world of movies, music, and entertainment writ large to get a little choppy. Whether any of those efforts to automate entertainment will stick is less obvious. The thing about art is that the kind that people tend to like is the kind that has something substantial to say. Right now, for all of its mimicry, generative AI doesn't actually have anything to say, because technically all it can do is remix and repeat.... I did more tests with Google's #Veo3. Imagine if AI characters became aware they were living in a simulation! pic.twitter.com/nhbrNQMtqv\n— Hashem Al-Ghaili (@HashemGhaili) May 21, 2025\nCall me an optimist, but most people can likely sniff out the difference between slop and art, and as much as studios would love to wave a magic wand and rid themselves of human creatives and the cost of their labor, deep down they know that they'd have to Ctrl+Z that move just as fast. That's not to say there won't be casualties in the AI age—if there's one lesson we can learn from mass waves of automation in years past, it's that labor forces are usually the most affected.\nBut when it comes to art, things aren't so simple. Art, at least the good kind, is about human connection, and until AI can think and feel like we do, there's nothing that can replace that. So, before you crash out over AI slop, just remember: AI still thinks putting glue on your pizza is a good idea, so we may have a few more good years left in the tank.",
                "domain": "gizmodo.com"
              },
              {
                "position": 6,
                "title": "Release notes | Gemini API | Google AI for Developers",
                "url": "https://ai.google.dev/gemini-api/docs/changelog",
                "snippet": "This page documents updates to the Gemini API.\n\n## October 17, 2025\n\n**Grounding with Google Maps**is now generally available. For more information, see Grounding with Google Maps documentation.\n\n## October 15, 2025\n\nReleased Veo 3.1 and 3.1 Fast models in public preview, with new features including:\n\n- Extending Veo-created videos.\n\n- Referencing up to three images to generate a video.\n\n- Providing first and last frame images to generate videos from.\n\nThis launch also added more options for Veo 3 output video durations: 4, 6, and 8 seconds.\n\nDeprecation for\n\n`veo-3.0-generate-preview`and\n\n`veo-3.0-fast-generate-preview`coming October 22, 2025.\n\n## October 7, 2025\n\n- Launched Gemini 2.5 Computer Use Preview\n\n## October 2, 2025\n\n- Launched Gemini 2.5 Flash Image GA: Image Generation with Gemini\n\n## September 29, 2025\n\n- The following Gemini 1.5 models are now deprecated:\n\n`gemini-1.5-pro`\n\n`gemini-1.5-flash-8b`\n\n`gemini-1.5-flash`... ## September 9, 2025\n\n- Launched Veo 3 and Veo 3 Fast GA, with lower pricing and new options for aspect ratios, resolution, and seeding. Read the Veo documentation for more information.\n\n## August 26, 2025\n\n- Launched Gemini 2.5 Image Preview, our latest native image generation model.\n\n## August 18, 2025\n\n- Released URL context tool to general\n\navailability (GA), a tool for providing URLs as additional context to\n\nprompts. Support for using URL context with the\n\n`gemini-2.0-flash`model (available during experimental release) will be discontinued in one week.\n\n## August 14, 2025\n\n- Released Imagen 4 Ultra, Standard and Fast models as generally available (GA). To learn more, see the Imagen page.\n\n## August 7, 2025\n\n`allow_adult`setting in Image to Video generation are now available in restricted regions. See the Veo page for details.\n\n## July 31, 2025\n\n- Launched image-to-video generation for the Veo 3 Preview model.\n\n- Released Veo 3 Fast Preview model.\n\n- To learn more about Veo 3, visit the Veo page.... ## July 22, 2025\n\n- Released\n\n`gemini-2.5-flash-lite`, our fast, low-cost, high-performance Gemini 2.5 model. To learn more, see Gemini 2.5 Flash-Lite.\n\n## July 17, 2025\n\nLaunched\n\n`veo-3.0-generate-preview`, the latest update to Veo introducing video with audio generation. To learn more about Veo 3, visit the Veo page.\n\nIncreased rate limits for Imagen 4 Standard and Ultra. Visit the Rate limits page for more details.\n\n## July 14, 2025\n\n- Released\n\n`gemini-embedding-001`, the stable version of our text embedding model. To learn more, see embeddings. The\n\n`gemini-embedding-exp-03-07`model will be deprecated on August 14, 2025.\n\n## July 7, 2025\n\n- Launched Gemini API Batch Mode. Batch up requests and send them to process asynchronously. To learn more, see Batch Mode.\n\n## June 26, 2025\n\nThe preview models\n\n`gemini-2.5-pro-preview-05-06`and\n\n`gemini-2.5-pro-preview-03-25`are now redirecting to the latest stable version\n\n`gemini-2.5-pro`.\n\n`gemini-2.5-pro-exp-03-25`is deprecated.... ## May 7, 2025\n\n- Released\n\n`gemini-2.0-flash-preview-image-generation`, a preview model for generating and editing images. To learn more, see Image generation and Gemini 2.0 Flash Preview Image Generation.\n\n## May 6, 2025\n\n- Released\n\n`gemini-2.5-pro-preview-05-06`, a new version of our most powerful model, with improvements on code and function calling.\n\n`gemini-2.5-pro-preview-03-25`will automatically point to the new version of the model.\n\n## April 17, 2025\n\n- Released\n\n`gemini-2.5-flash-preview-04-17`, a Gemini preview model optimized for price-performance and adaptive thinking. To learn more, see Gemini 2.5 Flash Preview and Thinking.\n\n## April 16, 2025\n\n- Launched context caching for Gemini 2.0 Flash.... ## April 9, 2025\n\n**Model updates:**\n\n- Released\n\n`veo-2.0-generate-001`, a generally available (GA) text- and image-to-video model, capable of generating detailed and artistically nuanced videos. To learn more, see the Veo docs.\n\nReleased\n\n`gemini-2.0-flash-live-001`, a public preview version of the Live API model with billing enabled.\n\n**Enhanced Session Management and Reliability** **Session Resumption:**Keep sessions alive across temporary network disruptions. The API now supports server-side session state storage (for up to 24 hours) and provides handles (session_resumption) to reconnect and resume where you left off. **Longer Sessions via Context Compression:**Enable extended interactions beyond previous time limits. Configure context window compression with a sliding window mechanism to automatically manage context length, preventing abrupt terminations due to context limits. **Graceful Disconnect Notification:**Receive a\n\n`GoAway`server message indicating when a connection is about to close, allowing for graceful handling before termination.\n\n\n\n**More Control over Interaction Dynamics** **Configurable Voice Activity Detection (VAD):**Choose sensitivity levels or disable automatic VAD entirely and use new client events (\n\n`activityStart`,\n\n`activityEnd`) for manual turn control.\n\n**Configurable Interruption Handling:**Decide whether user input should interrupt the model's response. **Configurable Turn Coverage:**Choose whether the API processes all audio and video input continuously or only captures it when the end-user is detected speaking. **Configurable Media Resolution:**Optimize for quality or token usage by selecting the resolution for input media. **Richer Output and Features** **Expanded Voice & Language Options:**Choose from two new voices and 30 new languages for audio output. The output language is now configurable within\n\n`speechConfig`.\n\n**Text Streaming:**Receive text responses incrementally as they are generated, enabling faster display to the user. **Token Usage Reporting:**Gain insights into usage with detailed token counts provided in the\n\n`usageMetadata`field of server messages, broken down by modality and prompt or response phases.... ## April 4, 2025\n\n- Released\n\n`gemini-2.5-pro-preview-03-25`, a public preview Gemini 2.5 Pro version with billing enabled. You can continue to use\n\n`gemini-2.5-pro-exp-03-25`on the free tier.\n\n## March 25, 2025\n\n- Released\n\n`gemini-2.5-pro-exp-03-25`, a public experimental Gemini model with thinking mode always on by default. To learn more, see Gemini 2.5 Pro Experimental.\n\n## March 12, 2025\n\n**Model updates:**\n\n- Launched an experimental Gemini 2.0 Flash model capable of image generation and editing.\n\n- Released\n\n`gemma-3-27b-it`, available on AI Studio and through the Gemini API, as part of the Gemma 3 launch.\n\n**API updates:**\n\n- Added support for YouTube URLs as a media source.\n\n- Added support for including an inline video of less than 20MB.\n\n## March 11, 2025\n\n**SDK updates:**\n\n- Released the Google Gen AI SDK for TypeScript and JavaScript to public preview.\n\n## March 7, 2025\n\n**Model updates:**\n\n- Released\n\n`gemini-embedding-exp-03-07`, an experimental Gemini-based embeddings model in public preview.... ## February 28, 2025\n\n**API updates:**\n\n- Support for Search as a tool\n\nadded to\n\n`gemini-2.0-pro-exp-02-05`, an experimental model based on Gemini 2.0 Pro.\n\n## February 25, 2025\n\n**Model updates:**\n\n- Released\n\n`gemini-2.0-flash-lite`, a generally available (GA) version of Gemini 2.0 Flash-Lite, which is optimized for speed, scale, and cost efficiency.\n\n## February 19, 2025\n\n**AI Studio updates:**\n\n- Support for additional regions (Kosovo, Greenland and Faroe Islands).\n\n**API updates:**\n\n- Support for additional regions (Kosovo, Greenland and Faroe Islands).\n\n## February 18, 2025\n\n**Model updates:**\n\n- Gemini 1.0 Pro is no longer supported. For the list of supported models, see Gemini models.\n\n## February 11, 2025\n\n**API updates:**\n\n- Updates on the OpenAI libraries compatibility.\n\n## February 6, 2025\n\n**Model updates:**\n\n- Released\n\n`imagen-3.0-generate-002`, a generally available (GA) version of Imagen 3 in the Gemini API.\n\n**SDK updates:**\n\n- Released the Google Gen AI SDK for Java for public preview.",
                "domain": "ai.google.dev"
              },
              {
                "position": 7,
                "title": "Google's $249 AI video tool is incredible — but this one feature left ...",
                "url": "https://www.tomsguide.com/ai/ai-image-video/i-tried-google-veo-3-heres-what-impressed-me-and-what-still-needs-work",
                "snippet": "Google’s Veo 3 made a splash at Google I/O 2025 as the latest leap forward in AI-powered video generation.\n\nAs a feature within the new Google AI Ultra subscription, the highest access to Google’s most advanced models and premium features, you can get Veo 3 and Flow, which strings videos together for a more robust cinematic feature.\n\nDesigned to turn simple text prompts into hyper-realistic clips — with audio, dialogue, and sound effects — it’s arguably one of the most advanced tools available to casual creators. And after testing it hands-on, I can say it delivers some truly jaw-dropping results. But it also comes with its share of hiccups.\n\nHere’s what I loved about Veo 3 — and what left me frustrated.\n\n## What Veo 3 gets right\n\nI’ve tested my fair share of AI video tools, including earlier versions of Veo, and this latest release is by far the most user-friendly when it comes to adding sound and dialogue.\n\nThe realism is genuinely impressive — especially for the fact that the 8-second clips can be generated in under two minutes on a computer without a full production crew.\n\nUsers across the internet are sharing clips that are nearly indistinguishable from human-made videos. It’s a glimpse at just how fast this tech is moving.\n\nGoogle Veo 3 realism just broke the Internet yesterday.This is 100% AI10 wild examples:1. Street interview that never happened pic.twitter.com/qdxZVhOO3GMay 22, 2025... ## Where Veo 3 still struggles\n\nFor all its strengths, Veo 3 still has a ways to go before it’s seamless. Obviously, it's still in experimental mode, so Google is working out the kinks, but here are the biggest issues I ran into while testing:\n\nGet instant access to breaking news, the hottest reviews, great deals and helpful tips.\n\n### 1. Prompt interpretation feels hit-or-miss\n\nVeo 3 sometimes struggles with spatial prompts, like when I asked for an overhead camera angle but got a slightly tilted side view instead. It seems to prioritize cinematic flair over strict prompt accuracy, which limits creative control.\n\n### 2. Audio doesn’t always work — and it’s not obvious why\n\nBy default, Veo runs in Veo 2 mode, which does not include audio. I only realized this after a few silent clips and some digging. You’ll need to manually switch to \"Experiential Mode\" under the \"Quality\" tab to activate audio and dialogue.\n\nEven then, lip-syncing is inconsistent, and dialogue sometimes drops out altogether, like a badly dubbed foreign film. Additionally, the subtitles are almost always wrong or misspelled.\n\n### 3. Complex scenes throw it off\n\nVeo 3 shines with single-subject clips, but longer or more intricate scenes can fall apart. The narrative gets muddy, and character interactions often feel stiff or repetitive. If you're aiming to create a multi-character, multi-scene story, temper your expectations.... ### 4. The interface still needs polish\n\nThere were moments when the interface felt unintuitive or unstable. I experienced an unexpected session timeout that erased a generated video, and I couldn’t find a recovery option.\n\nAdditionally, when I prompted the model to add dialogue within the scene I got something that did not fit the scenario at all.\n\nFor a tool this powerful, the UX still feels a bit rough around the edges.\n\n### 5. It raises some big ethical questions\n\nVeo’s realism is incredible — and a little unsettling. There’s growing concern that ultra-realistic, AI-generated videos could blur the lines between fact and fiction, especially as this tech becomes more accessible. It also sparks new debates around authorship and originality in creative work.\n\n### Promising but pricey\n\nVeo 3 is a huge step forward in AI video — especially for casual users who want fast, high-quality results. But at $249 per month (with a discounted rate for the first three months), the Google AI Ultra package is a steep price to pay for a tool that still has some notable bugs.\n\nIf you’re just looking to experiment with video generation or create basic promotional content, Veo 3 is exciting — but not yet essential. For professional creators, though, it’s worth watching closely. Just keep your prompts tight, your expectations realistic and your finger ready to re-render.... ### More from Tom's Guide\n\n- This $12.99/month hack gives you access to Google’s Veo 3 AI video tool — here’s how\n\n- Claude Opus 4 is here — and it might be the smartest AI assistant yet\n\n- The only 5 prompt types you need to master ChatGPT (and any other chatbot)\n\nAmanda Caswell is an award-winning journalist, bestselling YA author, and one of today’s leading voices in AI and technology. A celebrated contributor to various news outlets, her sharp insights and relatable storytelling have earned her a loyal readership. Amanda’s work has been recognized with prestigious honors, including outstanding contribution to media.\n\nKnown for her ability to bring clarity to even the most complex topics, Amanda seamlessly blends innovation and creativity, inspiring readers to embrace the power of AI and emerging technologies. As a certified prompt engineer, she continues to push the boundaries of how humans and AI can work together.\n\nBeyond her journalism career, Amanda is a long-distance runner and mom of three. She lives in New Jersey.\n\nYou must confirm your public display name before commenting\n\nPlease logout and then login again, you will then be prompted to enter your display name.",
                "domain": "www.tomsguide.com"
              },
              {
                "position": 8,
                "title": "Google DeepMind's Veo 3 floods internet with realistic videos",
                "url": "https://www.axios.com/2025/05/23/google-ai-videos-veo-3",
                "snippet": "# Google's new AI video tool floods internet with real-looking clips\n\nGoogle's newest AI video generator, Veo 3, generates clips that most users online can't seem to distinguish from those made by human filmmakers and actors.\n\n**Why it matters: **Veo 3 videos shared online are amazing viewers with their realism — and also terrifying them with a sense that real and fake have become hopelessly blurred.\n\n**The big picture: **Unlike OpenAI's video generator Sora, released more widely last December, Google DeepMind's Veo 3 can include dialogue, soundtracks and sound effects.\n\n- The model excels at following complex prompts and translating detailed descriptions into realistic videos.\n\n- The AI engine abides by real-world physics, offers accurate lip-syncing, rarely breaks continuity and generates people with lifelike human features, including five fingers per hand.\n\n- According to examples shared by Google and from users online, the telltale signs of synthetic content are mostly absent.\n\n**Case in point: **In one viral example posted on X, filmmaker and molecular biologist Hashem Al-Ghaili shows a series of short films of AI-generated actors railing against their AI creators and prompts.\n\n**Special effects technology,** video-editing apps and camera tech advances have been changing Hollywood for many decades, but artificially generated films pose a novel challenge to human creators.... - In a promo video for Flow, Google's new video tool that includes Veo 3, filmmakers say the AI engine gives them a new sense of freedom with a hint of eerie autonomy.\n\n- \"It feels like it's almost building upon itself,\" filmmaker Dave Clark says.\n\n**How it works: **Veo 3 was announced at Google I/O on Tuesday and is available now to $249-a-month Google AI Ultra subscribers in the United States.\n\n**Between the lines: **Google says Veo 3 was \"informed by our work with creators and filmmakers,\" and some creators have embraced new AI tools. But the spread of the videos online is also dismaying many video professionals and lovers of art.\n\n- Some dismiss any AI-generated video as \"slop,\" regardless of its technical proficiency or lifelike qualities — but, as Axios' Ina Fried points out, AI slop is in the eye of the beholder.\n\n- The tool could also be useful for more commercial marketing and media work, AI analyst Ethan Mollick writes.\n\n**It's unclear how Google trained Veo 3 **and how that might affect the creativity of its outputs.\n\n- 404 Media found that Veo 3 generated the same lame dad joke for several users who prompted it to create a video of a man doing stand-up comedy.\n\n- Likewise, last year, YouTuber Marques Brownlee asked Sora to create a video of a \"tech reviewer sitting at a desk.\" The generated video featured a fake plant that's nearly identical to the shrub Brownlee keeps on his desk for many of his videos — suggesting the tool may have been trained on them.\n\n**What we're watching:** As hyperrealistic AI-generated videos become even easier to produce, the world hasn't even begun to sort out how to manage authorship, consent, rights and the film industry's future.\n\n##### Go deeperJul 10, 2025 - Technology... ## Google AI's new trick: Turn any image into a brief video\n\nGoogle's latest AI video tool, Veo 3, now generates short movies with sound based only on still photos and prompts.\n\nGo deeper (1 min. read)\n\n**The big picture: **The feature, released Thursday, is available to Ultra and Pro users on the web and soon on mobile for subscribers in select regions, Google shared with Axios.\n\n## Google avatars shake up workplace video making\n\nGoogle Vids is now providing users of the workplace video creation tool with a set of pre-made avatars for use in brief AI-generated videos, the company said Wednesday.\n\nGo deeper (2 min. read)\n\n**Why it matters: **The rise of cheap, convenient AI video generation threatens jobs for video producers, editors, camera operators and even commercial actors.\n\n## AI slop is ruining all of our favorite places to scroll\n\nAn AI-generated video of rabbits jumping on a trampoline that went viral this week — and was widely believed to be real — proved even cute animal vids aren't safe from convincing slop machines.\n\nGo deeper (2 min. read)\n\n**Why it matters: **All the fake AI-generated content online is sapping the joy of casual scrolling.",
                "domain": "www.axios.com"
              },
              {
                "position": 9,
                "title": "Introducing Veo 3.1 and advanced capabilities in Flow - Google Blog",
                "url": "https://blog.google/technology/ai/veo-updates-flow/",
                "snippet": "# Introducing Veo 3.1 and advanced capabilities in Flow\n\nFive months ago, we introduced Flow, our AI filmmaking tool powered by Veo, and have been inspired by the creativity it has sparked with over 275 million videos generated in Flow\n\n1\n\n. We're always listening to your feedback, and we've heard that you want more artistic control within Flow, with increased support for audio across all features.\n\nToday, we’re introducing new and enhanced creative capabilities to edit your clips, giving you more granular control over your final scene. For the first time, we’re also bringing audio to existing capabilities like “Ingredients to Video,” “Frames to Video” and “Extend.”\n\nWe’re also introducing Veo 3.1, which brings richer audio, more narrative control, and enhanced realism that captures true-to-life textures. Veo 3.1 is state-of-the-art and builds on Veo 3, with stronger prompt adherence and improved audiovisual quality when turning images into videos.\n\n## Refine your narrative with audio and more control\n\nWith Veo 3.1, we’re bringing audio to existing capabilities to help you craft the perfect scene. These features are experimental and actively improving, and we’re excited to see what you create as we iterate based on your feedback.\n\nNow, with rich, generated audio, you can:\n\n**Craft the look of your scene.**With \"Ingredients to Video,\" you can use multiple reference images to control the characters, objects and style. Flow uses your ingredients to create a final scene that looks just as you envisioned. **Control the shot from start to finish.**Provide a starting and ending image with “Frames to Video,” and Flow will generate a seamless video that bridges the two, perfect for artful and epic transitions. **Create longer, seamless shots.**With \"Extend,\" you can create longer videos, even lasting for a minute or more, that connect to and continue the action from your original clip. Each video is generated based on the final second of your previous clip, making it most useful for creating a longer establishing shot.... ## Edit your ingredients and videos with more precision\n\nGreat ideas can strike at any point in the creative process. For moments when the first take isn't the final one, we're introducing new editing capabilities directly within Flow to help you reimagine and perfect your scenes.\n\n**Add new elements to any scene.**With “Insert,” introduce anything you can imagine, from realistic details to fantastical creatures. Flow now handles complex details like shadows and scene lighting, making the addition look natural. **Remove unwanted objects or characters seamlessly.**Soon, you’ll be able to take anything out of a scene, and Flow will reconstruct the background and surroundings, making it look as though the object was never there.\n\n## Start creating in Flow today\n\nWith more precise editing capabilities, audio across all existing features and higher-quality outputs powered by Veo 3.1, we're opening up new possibilities for richer, more powerful video storytelling right inside Flow.\n\nThe Veo 3.1 model is also available via the Gemini API for developers, Vertex AI for enterprise customers, and the Gemini app. New capabilities are available in both Gemini API\n\n2\n\nand Vertex AI\n\n3",
                "domain": "blog.google"
              },
              {
                "position": 10,
                "title": "Introducing Veo 3.1 and new creative capabilities in the Gemini API",
                "url": "https://developers.googleblog.com/en/introducing-veo-3-1-and-new-creative-capabilities-in-the-gemini-api/",
                "snippet": "**Today, we are releasing Veo 3.1 and Veo 3.1 Fast in paid preview in the** **Gemini API** **.** This updated model offers several improvements, as well as improved outputs when generating video from images. These new models are available via the Gemini API in Google AI Studio and Vertex AI. Veo 3.1 is also available in the Gemini app and Flow.\n\nVeo 3.1 and Veo 3.1 Fast empower developers to create more engaging content through significant upgrades. The models now generate\n\n**richer native audio**, from natural conversations to synchronized sound effects, and offer **greater narrative control** with an improved understanding of cinematic styles. **Enhanced image-to-video** capabilities ensure better prompt adherence while delivering superior audio and visual quality and maintaining character consistency across multiple scenes.\n\nIn addition to the model update, we are introducing new model capabilities, including using reference images to guide video generation, extend existing Veo videos, and generate transitions between a first and last frame.\n\nYou can now guide the generation process by providing up to 3 reference images of a character, object, or scene. This is helpful for maintaining character consistency across multiple shots or applying a specific style to your video.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nconfig=types.GenerateVideosConfig(\n\nreference_images=[reference_image1, reference_image2],\n\n),\n\n\n\n```... Your story is no longer limited by the original generation. With Scene extension, you can create longer videos, even lasting for a minute or more, by generating new clips that connect to your previous video. Each new video is generated based on the final second of the previous clip. This maintains visual continuity, making the feature ideal for extending shots with background audio.\n\n```\n\nfrom google import genai\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nvideo=video_to_extend\n\n\n\n```\n\nCreate smooth, natural scenes that bridge two different images. By providing a starting and an ending image, you can direct Veo 3.1 to generate the transition between them, complete with accompanying audio.\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\noperation = client.models.generate_videos(\n\nmodel=\"veo-3.1-generate-preview\",\n\nprompt=prompt,\n\nimage=first_frame,\n\nconfig=types.GenerateVideosConfig(\n\nlast_frame=last_frame,\n\n),\n\n\n\n```\n\n**What people are building with Veo 3.1**\n\nPromise Studios, a GenAI movie studio, uses Veo 3.1 within its MUSE Platform to enhance generative storyboarding and previsualization for director-driven storytelling at production quality.... Latitude is experimenting with Veo 3.1 in its generative narrative engine to instantly bring user-created stories to life.\n\nVeo 3.1 and these new capabilities are now available in paid preview via the Gemini API.\n\nVeo 3.1 is the same price as Veo 3. We are incredibly excited to see what you will build with these new capabilities!",
                "domain": "developers.googleblog.com"
              }
            ],
            "success": true,
            "error": null
          }
        ]
      },
      {
        "timestamp": "2025-10-19T13:35:57.415289",
        "step": "evaluation",
        "agent": "evaluator_agent",
        "input_preview": "[{'query_id': 'q2', 'query': 'Google veo3.1 release date 2025', 'claim_id': 'claim_1', 'success': True, 'result_count': 10, 'top_results': [{'title': 'Veo (text-to-video model) - Wikipedia', 'url': 'h",
        "output_preview": "{'overall_verdict': 'TRUE', 'confidence_score': 0.95, 'sub_claim_verdicts': [{'claim_id': 'claim_1', 'statement': 'Google has released a software version named veo3.1.', 'verdict': 'TRUE', 'confidence",
        "full_output": {
          "overall_verdict": "TRUE",
          "confidence_score": 0.95,
          "sub_claim_verdicts": [
            {
              "claim_id": "claim_1",
              "statement": "Google has released a software version named veo3.1.",
              "verdict": "TRUE",
              "confidence": 0.95,
              "supporting_count": 7,
              "refuting_count": 0,
              "dependency_status": "foundational",
              "key_evidence": [
                {
                  "title": "Introducing Veo 3.1 and advanced capabilities in Flow - Google Blog",
                  "url": "https://blog.google/technology/ai/veo-updates-flow/",
                  "credibility_tier": 2,
                  "supports_claim": true
                },
                {
                  "title": "Google announces Veo 3.1 and updates Flow with more controls, tools",
                  "url": "https://9to5google.com/2025/10/15/veo-3-1/",
                  "credibility_tier": 3,
                  "supports_claim": true
                },
                {
                  "title": "Google releases Veo 3.1, adds it to Flow video editor",
                  "url": "https://techcrunch.com/2025/10/15/google-releases-veo-3-1-adds-it-to-flow-video-editor/",
                  "credibility_tier": 3,
                  "supports_claim": true
                }
              ],
              "rationale": "Multiple credible sources, including Google's official blog and reputable tech news sites, confirm the release of Veo 3.1."
            }
          ],
          "dependency_analysis": {
            "foundational_claims_verified": true,
            "broken_dependencies": [],
            "notes": "All foundational claims have been verified as true."
          },
          "summary": "The claim that Google has released a software version named Veo 3.1 is supported by multiple credible sources, including Google's official announcements and reputable tech news outlets. Therefore, the overall verdict is TRUE.",
          "key_findings": [
            "Google's official blog and multiple reputable tech news sites confirm the release of Veo 3.1.",
            "There is strong evidence from Tier 2 and Tier 3 sources supporting the claim."
          ],
          "limitations": "No significant limitations or conflicting evidence found."
        }
      }
    ]
  }
}